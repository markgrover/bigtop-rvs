diff --git .gitignore .gitignore
index 1e2a1f6..84f6d75 100644
--- .gitignore
+++ .gitignore
@@ -16,12 +16,11 @@ target
 
 # Oozie build
 core/pig*
-build
-mem
+core/hive-*
 core/build
 core/mem
 examples/oozietests
 mkdistro-*.out
 distro/downloads
 SecurityAuth.audit
-
+build
diff --git client/pom.xml client/pom.xml
index 9e2d26d..f5bc042 100644
--- client/pom.xml
+++ client/pom.xml
@@ -7,25 +7,28 @@
   to you under the Apache License, Version 2.0 (the
   "License"); you may not use this file except in compliance
   with the License.  You may obtain a copy of the License at
-  
+
        http://www.apache.org/licenses/LICENSE-2.0
-  
+
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
     <modelVersion>4.0.0</modelVersion>
     <parent>
         <groupId>org.apache.oozie</groupId>
         <artifactId>oozie-main</artifactId>
         <version>3.1.3-incubating</version>
     </parent>
+    <groupId>org.apache.oozie</groupId>
     <artifactId>oozie-client</artifactId>
-    <description>Oozie Client</description>
-    <name>Oozie Client</name>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie Client</description>
+    <name>Apache Oozie Client</name>
     <packaging>jar</packaging>
 
     <dependencies>
@@ -44,6 +47,18 @@
             <artifactId>junit</artifactId>
             <scope>test</scope>
         </dependency>
+
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-auth</artifactId>
+            <scope>compile</scope>
+            <exclusions>
+                <exclusion>
+                    <groupId>org.slf4j</groupId>
+                    <artifactId>slf4j-api</artifactId>
+                </exclusion>
+            </exclusions>
+        </dependency>
     </dependencies>
 
     <build>
@@ -67,47 +82,47 @@
     </build>
 
     <profiles>
-	     <profile>
-	         <id>generateDocs</id>
-	         <activation>
-	             <activeByDefault>false</activeByDefault>
-	             <property>
-	                 <name>generateDocs</name>
-	             </property>
-	         </activation>
-	         <build>
-	             <plugins>
-	                 <plugin>
-	                     <groupId>org.apache.maven.plugins</groupId>
-	                     <artifactId>maven-javadoc-plugin</artifactId>
-	                     <configuration>
-	                         <linksource>true</linksource>
-			                 <quiet>true</quiet>
-			                 <verbose>false</verbose>
-			                 <source>${maven.compile.source}</source>
-			                 <charset>${maven.compile.encoding}</charset>
-			                 <groups>
-			                     <group>
-			                         <title>Client API</title>
-			                         <packages>
-			                             org.apache.oozie.client
-			                         </packages>
-			                     </group>
-			                 </groups>
-	                     </configuration>
-		                 <executions>
-	                         <execution>
-	                             <goals>
-	                                 <goal>javadoc</goal>
-	                             </goals>
-	                             <phase>package</phase>
-	                         </execution>
-		                 </executions>
-	                 </plugin>
-	             </plugins>
-	         </build>
-	     </profile>
+        <profile>
+            <id>generateDocs</id>
+            <activation>
+                <activeByDefault>false</activeByDefault>
+                <property>
+                    <name>generateDocs</name>
+                </property>
+            </activation>
+            <build>
+                <plugins>
+                    <plugin>
+                        <groupId>org.apache.maven.plugins</groupId>
+                        <artifactId>maven-javadoc-plugin</artifactId>
+                        <configuration>
+                            <linksource>true</linksource>
+                            <quiet>true</quiet>
+                            <verbose>false</verbose>
+                            <source>${maven.compile.source}</source>
+                            <charset>${maven.compile.encoding}</charset>
+                            <groups>
+                                <group>
+                                    <title>Client API</title>
+                                    <packages>
+                                        org.apache.oozie.client
+                                    </packages>
+                                </group>
+                            </groups>
+                        </configuration>
+                        <executions>
+                            <execution>
+                                <goals>
+                                    <goal>javadoc</goal>
+                                </goals>
+                                <phase>package</phase>
+                            </execution>
+                        </executions>
+                    </plugin>
+                </plugins>
+            </build>
+        </profile>
     </profiles>
-    
+
 </project>
 
diff --git client/src/main/bin/oozie client/src/main/bin/oozie
index 9393f9f..21802a6 100644
--- client/src/main/bin/oozie
+++ client/src/main/bin/oozie
@@ -7,9 +7,9 @@
 # to you under the Apache License, Version 2.0 (the
 # "License"); you may not use this file except in compliance
 # with the License.  You may obtain a copy of the License at
-# 
+#
 #      http://www.apache.org/licenses/LICENSE-2.0
-# 
+#
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -46,7 +46,6 @@ else
     JAVA_BIN=${JAVA_HOME}/bin/java
 fi
 
-JAVA_PROPERTIES=""
 while [[ ${1} =~ ^\-D ]]; do
   JAVA_PROPERTIES="${JAVA_PROPERTIES} ${1}"
   shift
diff --git client/src/main/java/org/apache/oozie/cli/OozieCLI.java client/src/main/java/org/apache/oozie/cli/OozieCLI.java
index bd35fdc..77fcdc1 100644
--- client/src/main/java/org/apache/oozie/cli/OozieCLI.java
+++ client/src/main/java/org/apache/oozie/cli/OozieCLI.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -31,6 +31,7 @@ import java.util.Locale;
 import java.util.Map;
 import java.util.Properties;
 import java.util.TimeZone;
+import java.util.concurrent.Callable;
 
 import javax.xml.XMLConstants;
 import javax.xml.parsers.DocumentBuilder;
@@ -48,6 +49,7 @@ import org.apache.commons.cli.OptionGroup;
 import org.apache.commons.cli.Options;
 import org.apache.commons.cli.ParseException;
 import org.apache.oozie.BuildInfo;
+import org.apache.oozie.client.AuthOozieClient;
 import org.apache.oozie.client.BundleJob;
 import org.apache.oozie.client.CoordinatorAction;
 import org.apache.oozie.client.CoordinatorJob;
@@ -102,6 +104,8 @@ public class OozieCLI {
     public static final String DEFINITION_OPTION = "definition";
     public static final String CONFIG_CONTENT_OPTION = "configcontent";
 
+    public static final String DO_AS_OPTION = "doas";
+
     public static final String LEN_OPTION = "len";
     public static final String FILTER_OPTION = "filter";
     public static final String JOBTYPE_OPTION = "jobtype";
@@ -145,6 +149,9 @@ public class OozieCLI {
      * @param args options and arguments for the Oozie CLI.
      */
     public static void main(String[] args) {
+        if (!System.getProperties().contains(AuthOozieClient.USE_AUTH_TOKEN_CACHE_SYS_PROP)) {
+            System.setProperty(AuthOozieClient.USE_AUTH_TOKEN_CACHE_SYS_PROP, "true");
+        }
         System.exit(new OozieCLI().run(args));
     }
 
@@ -171,8 +178,10 @@ public class OozieCLI {
         Option status = new Option(STATUS_OPTION, false, "show the current system status");
         Option version = new Option(VERSION_OPTION, false, "show Oozie server build version");
         Option queuedump = new Option(QUEUE_DUMP_OPTION, false, "show Oozie server queue elements");
+        Option doAs = new Option(DO_AS_OPTION, true, "doAs user, impersonates as the specified user");
         Options adminOptions = new Options();
         adminOptions.addOption(oozie);
+        adminOptions.addOption(doAs);
         OptionGroup group = new OptionGroup();
         group.addOption(system_mode);
         group.addOption(status);
@@ -218,6 +227,8 @@ public class OozieCLI {
         Option property = OptionBuilder.withArgName("property=value").hasArgs(2).withValueSeparator().withDescription(
                 "set/override value for given property").create("D");
 
+        Option doAs = new Option(DO_AS_OPTION, true, "doAs user, impersonates as the specified user");
+
         OptionGroup actions = new OptionGroup();
         actions.addOption(submit);
         actions.addOption(start);
@@ -235,6 +246,7 @@ public class OozieCLI {
         actions.setRequired(true);
         Options jobOptions = new Options();
         jobOptions.addOption(oozie);
+        jobOptions.addOption(doAs);
         jobOptions.addOption(config);
         jobOptions.addOption(property);
         jobOptions.addOption(changeValue);
@@ -257,13 +269,16 @@ public class OozieCLI {
         Option jobtype = new Option(JOBTYPE_OPTION, true,
                 "job type ('Supported in Oozie-2.0 or later versions ONLY - 'coordinator' or 'bundle' or 'wf'(default))");
         Option len = new Option(LEN_OPTION, true, "number of jobs (default '100')");
-        Option filter = new Option(FILTER_OPTION, true, "user=<U>;name=<N>;group=<G>;status=<S>;...");
+        Option filter = new Option(FILTER_OPTION, true, "user=<U>;name=<N>;group=<G>;status=<S>;frequency=<F>;unit=<M> " +
+                        "(Valid unit values are 'months', 'days', 'hours' or 'minutes'.)");
         Option localtime = new Option(LOCAL_TIME_OPTION, false, "use local time (default GMT)");
         Option verbose = new Option(VERBOSE_OPTION, false, "verbose mode");
+        Option doAs = new Option(DO_AS_OPTION, true, "doAs user, impersonates as the specified user");
         start.setType(Integer.class);
         len.setType(Integer.class);
         Options jobsOptions = new Options();
         jobsOptions.addOption(oozie);
+        jobsOptions.addOption(doAs);
         jobsOptions.addOption(localtime);
         jobsOptions.addOption(start);
         jobsOptions.addOption(len);
@@ -293,8 +308,10 @@ public class OozieCLI {
         Option pigFile = new Option(PIGFILE_OPTION, true, "Pig script");
         Option property = OptionBuilder.withArgName("property=value").hasArgs(2).withValueSeparator().withDescription(
                 "set/override value for given property").create("D");
+        Option doAs = new Option(DO_AS_OPTION, true, "doAs user, impersonates as the specified user");
         Options pigOptions = new Options();
         pigOptions.addOption(oozie);
+        pigOptions.addOption(doAs);
         pigOptions.addOption(config);
         pigOptions.addOption(property);
         pigOptions.addOption(pigFile);
@@ -317,7 +334,7 @@ public class OozieCLI {
         }
         used = true;
 
-        CLIParser parser = new CLIParser(OOZIE_OPTION, getCLIHelp());
+        final CLIParser parser = new CLIParser(OOZIE_OPTION, getCLIHelp());
         parser.addCommand(HELP_CMD, "", "display usage", new Options(), false);
         parser.addCommand(VERSION_CMD, "", "show client version", new Options(), false);
         parser.addCommand(JOB_CMD, "", "job operations", createJobOptions(), false);
@@ -329,30 +346,21 @@ public class OozieCLI {
                 createPigOptions(), true);
 
         try {
-            CLIParser.Command command = parser.parse(args);
-            if (command.getName().equals(HELP_CMD)) {
-                parser.showHelp();
-            }
-            else if (command.getName().equals(JOB_CMD)) {
-                jobCommand(command.getCommandLine());
-            }
-            else if (command.getName().equals(JOBS_CMD)) {
-                jobsCommand(command.getCommandLine());
-            }
-            else if (command.getName().equals(ADMIN_CMD)) {
-                adminCommand(command.getCommandLine());
-            }
-            else if (command.getName().equals(VERSION_CMD)) {
-                versionCommand();
-            }
-            else if (command.getName().equals(VALIDATE_CMD)) {
-                validateCommand(command.getCommandLine());
-            }
-            else if (command.getName().equals(SLA_CMD)) {
-                slaCommand(command.getCommandLine());
+            final CLIParser.Command command = parser.parse(args);
+
+            String doAsUser = command.getCommandLine().getOptionValue(DO_AS_OPTION);
+
+            if (doAsUser != null) {
+                OozieClient.doAs(doAsUser, new Callable<Void>() {
+                    @Override
+                    public Void call() throws Exception {
+                        processCommand(parser, command);
+                        return null;
+                    }
+                });
             }
-            else if (command.getName().equals(PIG_CMD)) {
-                pigCommand(command.getCommandLine());
+            else {
+                processCommand(parser, command);
             }
 
             return 0;
@@ -374,6 +382,32 @@ public class OozieCLI {
         }
     }
 
+    private void processCommand(CLIParser parser, CLIParser.Command command) throws Exception {
+        if (command.getName().equals(HELP_CMD)) {
+            parser.showHelp();
+        }
+        else if (command.getName().equals(JOB_CMD)) {
+            jobCommand(command.getCommandLine());
+        }
+        else if (command.getName().equals(JOBS_CMD)) {
+            jobsCommand(command.getCommandLine());
+        }
+        else if (command.getName().equals(ADMIN_CMD)) {
+            adminCommand(command.getCommandLine());
+        }
+        else if (command.getName().equals(VERSION_CMD)) {
+            versionCommand();
+        }
+        else if (command.getName().equals(VALIDATE_CMD)) {
+            validateCommand(command.getCommandLine());
+        }
+        else if (command.getName().equals(SLA_CMD)) {
+            slaCommand(command.getCommandLine());
+        }
+        else if (command.getName().equals(PIG_CMD)) {
+            pigCommand(command.getCommandLine());
+        }
+    }
     protected String getOozieUrl(CommandLine commandLine) {
         String url = commandLine.getOptionValue(OOZIE_OPTION);
         if (url == null) {
@@ -449,9 +483,8 @@ public class OozieCLI {
         }
     }
 
-    private Properties getConfiguration(CommandLine commandLine) throws IOException {
-        Properties conf = new Properties();
-        conf.setProperty("user.name", System.getProperty("user.name"));
+    private Properties getConfiguration(OozieClient wc, CommandLine commandLine) throws IOException {
+        Properties conf = wc.createConfiguration();
         String configFile = commandLine.getOptionValue(CONFIG_OPTION);
         if (configFile == null) {
             throw new IOException("configuration file not specified");
@@ -493,7 +526,7 @@ public class OozieCLI {
         return changeValue;
     }
 
-    private void addHeader(OozieClient wc) {
+    protected void addHeader(OozieClient wc) {
         for (Map.Entry entry : System.getProperties().entrySet()) {
             String key = (String) entry.getKey();
             if (key.startsWith(WS_HEADER_PREFIX)) {
@@ -513,10 +546,7 @@ public class OozieCLI {
      * @throws OozieCLIException thrown if the OozieClient could not be configured.
      */
     protected OozieClient createOozieClient(CommandLine commandLine) throws OozieCLIException {
-        OozieClient wc = new OozieClient(getOozieUrl(commandLine));
-        addHeader(wc);
-        setDebugMode(wc);
-        return wc;
+        return createXOozieClient(commandLine);
     }
 
     /**
@@ -529,7 +559,7 @@ public class OozieCLI {
      * @throws OozieCLIException thrown if the XOozieClient could not be configured.
      */
     protected XOozieClient createXOozieClient(CommandLine commandLine) throws OozieCLIException {
-        XOozieClient wc = new XOozieClient(getOozieUrl(commandLine));
+        XOozieClient wc = new AuthOozieClient(getOozieUrl(commandLine));
         addHeader(wc);
         setDebugMode(wc);
         return wc;
@@ -562,13 +592,13 @@ public class OozieCLI {
 
         try {
             if (options.contains(SUBMIT_OPTION)) {
-                System.out.println(JOB_ID_PREFIX + wc.submit(getConfiguration(commandLine)));
+                System.out.println(JOB_ID_PREFIX + wc.submit(getConfiguration(wc, commandLine)));
             }
             else if (options.contains(START_OPTION)) {
                 wc.start(commandLine.getOptionValue(START_OPTION));
             }
             else if (options.contains(DRYRUN_OPTION)) {
-                String[] dryrunStr = wc.dryrun(getConfiguration(commandLine)).split("action for new instance");
+                String[] dryrunStr = wc.dryrun(getConfiguration(wc, commandLine)).split("action for new instance");
                 int arraysize = dryrunStr.length;
                 System.out.println("***coordJob after parsing: ***");
                 System.out.println(dryrunStr[0]);
@@ -596,11 +626,11 @@ public class OozieCLI {
                 wc.change(commandLine.getOptionValue(CHANGE_OPTION), getChangeValue(commandLine));
             }
             else if (options.contains(RUN_OPTION)) {
-                System.out.println(JOB_ID_PREFIX + wc.run(getConfiguration(commandLine)));
+                System.out.println(JOB_ID_PREFIX + wc.run(getConfiguration(wc, commandLine)));
             }
             else if (options.contains(RERUN_OPTION)) {
                 if (commandLine.getOptionValue(RERUN_OPTION).contains("-W")) {
-                    wc.reRun(commandLine.getOptionValue(RERUN_OPTION), getConfiguration(commandLine));
+                    wc.reRun(commandLine.getOptionValue(RERUN_OPTION), getConfiguration(wc, commandLine));
                 }
                 else if (commandLine.getOptionValue(RERUN_OPTION).contains("-B")) {
                     String bundleJobId = commandLine.getOptionValue(RERUN_OPTION);
@@ -685,8 +715,8 @@ public class OozieCLI {
                             .contains(LOCAL_TIME_OPTION));
                 }
                 else if (commandLine.getOptionValue(INFO_OPTION).contains("-W@")) {
-                    printWorkflowAction(wc.getWorkflowActionInfo(commandLine.getOptionValue(INFO_OPTION)), options
-                            .contains(LOCAL_TIME_OPTION));
+                    printWorkflowAction(wc.getWorkflowActionInfo(commandLine.getOptionValue(INFO_OPTION)),
+                            options.contains(LOCAL_TIME_OPTION), options.contains(VERBOSE_OPTION));
                 }
                 else {
                     String s = commandLine.getOptionValue(OFFSET_OPTION);
@@ -865,23 +895,28 @@ public class OozieCLI {
         }
     }
 
-    private void printWorkflowAction(WorkflowAction action, boolean contains) {
+    private void printWorkflowAction(WorkflowAction action, boolean contains, boolean verbose) {
         System.out.println("ID : " + maskIfNull(action.getId()));
 
         System.out.println(RULER);
 
-        System.out.println("Console URL     : " + maskIfNull(action.getConsoleUrl()));
-        System.out.println("Error Code      : " + maskIfNull(action.getErrorCode()));
-        System.out.println("Error Message   : " + maskIfNull(action.getErrorMessage()));
-        System.out.println("External ID     : " + maskIfNull(action.getExternalId()));
-        System.out.println("External Status : " + maskIfNull(action.getExternalStatus()));
-        System.out.println("Name            : " + maskIfNull(action.getName()));
-        System.out.println("Retries         : " + action.getRetries());
-        System.out.println("Tracker URI     : " + maskIfNull(action.getTrackerUri()));
-        System.out.println("Type            : " + maskIfNull(action.getType()));
-        System.out.println("Started         : " + maskDate(action.getStartTime(), contains));
-        System.out.println("Status          : " + action.getStatus());
-        System.out.println("Ended           : " + maskDate(action.getEndTime(), contains));
+        System.out.println("Console URL       : " + maskIfNull(action.getConsoleUrl()));
+        System.out.println("Error Code        : " + maskIfNull(action.getErrorCode()));
+        System.out.println("Error Message     : " + maskIfNull(action.getErrorMessage()));
+        System.out.println("External ID       : " + maskIfNull(action.getExternalId()));
+        System.out.println("External Status   : " + maskIfNull(action.getExternalStatus()));
+        System.out.println("Name              : " + maskIfNull(action.getName()));
+        System.out.println("Retries           : " + action.getRetries());
+        System.out.println("Tracker URI       : " + maskIfNull(action.getTrackerUri()));
+        System.out.println("Type              : " + maskIfNull(action.getType()));
+        System.out.println("Started           : " + maskDate(action.getStartTime(), contains));
+        System.out.println("Status            : " + action.getStatus());
+        System.out.println("Ended             : " + maskDate(action.getEndTime(), contains));
+
+        if (verbose) {
+            System.out.println("External Stats    : " + action.getStats());
+            System.out.println("External ChildIDs : " + action.getExternalChildIDs());
+        }
 
         System.out.println(RULER);
     }
@@ -1220,6 +1255,8 @@ public class OozieCLI {
                 sources.add(new StreamSource(Thread.currentThread().getContextClassLoader().getResourceAsStream(
                         "oozie-workflow-0.1.xsd")));
                 sources.add(new StreamSource(Thread.currentThread().getContextClassLoader().getResourceAsStream(
+                        "shell-action-0.1.xsd")));
+                sources.add(new StreamSource(Thread.currentThread().getContextClassLoader().getResourceAsStream(
                         "email-action-0.1.xsd")));
                 sources.add(new StreamSource(Thread.currentThread().getContextClassLoader().getResourceAsStream(
                         "distcp-action-0.1.xsd")));
@@ -1239,6 +1276,12 @@ public class OozieCLI {
                         "oozie-bundle-0.1.xsd")));
                 sources.add(new StreamSource(Thread.currentThread().getContextClassLoader().getResourceAsStream(
                         "oozie-sla-0.1.xsd")));
+                sources.add(new StreamSource(Thread.currentThread().getContextClassLoader().getResourceAsStream(
+                        "hive-action-0.2.xsd")));
+                sources.add(new StreamSource(Thread.currentThread().getContextClassLoader().getResourceAsStream(
+                        "sqoop-action-0.2.xsd")));
+                sources.add(new StreamSource(Thread.currentThread().getContextClassLoader().getResourceAsStream(
+                        "ssh-action-0.1.xsd")));
                 SchemaFactory factory = SchemaFactory.newInstance(XMLConstants.W3C_XML_SCHEMA_NS_URI);
                 Schema schema = factory.newSchema(sources.toArray(new StreamSource[sources.size()]));
                 Validator validator = schema.newValidator();
@@ -1277,11 +1320,11 @@ public class OozieCLI {
             throw new OozieCLIException("Need to specify -config <configfile>");
         }
 
-        Properties conf = getConfiguration(commandLine);
-        String script = commandLine.getOptionValue(PIGFILE_OPTION);
 
         try {
             XOozieClient wc = createXOozieClient(commandLine);
+            Properties conf = getConfiguration(wc, commandLine);
+            String script = commandLine.getOptionValue(PIGFILE_OPTION);
             System.out.println(JOB_ID_PREFIX + wc.submitPig(conf, script, pigArgs.toArray(new String[pigArgs.size()])));
         }
         catch (OozieClientException ex) {
diff --git client/src/main/java/org/apache/oozie/client/AuthOozieClient.java client/src/main/java/org/apache/oozie/client/AuthOozieClient.java
new file mode 100644
index 0000000..0853c88
--- /dev/null
+++ client/src/main/java/org/apache/oozie/client/AuthOozieClient.java
@@ -0,0 +1,208 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.client;
+
+import org.apache.hadoop.security.authentication.client.AuthenticatedURL;
+import org.apache.hadoop.security.authentication.client.AuthenticationException;
+import org.apache.hadoop.security.authentication.client.Authenticator;
+import org.apache.hadoop.security.authentication.client.KerberosAuthenticator;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileReader;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.io.Writer;
+import java.net.HttpURLConnection;
+import java.net.URL;
+
+/**
+ * This subclass of {@link XOozieClient} supports Kerberos HTTP SPNEGO and simple authentication.
+ */
+public class AuthOozieClient extends XOozieClient {
+
+    /**
+     * Java system property to specify a custom Authenticator implementation.
+     */
+    public static final String AUTHENTICATOR_CLASS_SYS_PROP = "authenticator.class";
+
+    /**
+     * Java system property that, if set the authentication token will be cached in the user home directory in a hidden
+     * file <code>.oozie-auth-token</code> with user read/write permissions only.
+     */
+    public static final String USE_AUTH_TOKEN_CACHE_SYS_PROP = "oozie.auth.token.cache";
+
+    /**
+     * File constant that defines the location of the authentication token cache file.
+     * <p/>
+     * It resolves to <code>${user.home}/.oozie-auth-token</code>.
+     */
+    public static final File AUTH_TOKEN_CACHE_FILE = new File(System.getProperty("user.home"), ".oozie-auth-token");
+
+    /**
+     * Create an instance of the AuthOozieClient.
+     *
+     * @param oozieUrl the Oozie URL
+     */
+    public AuthOozieClient(String oozieUrl) {
+        super(oozieUrl);
+    }
+
+    /**
+     * Create an authenticated connection to the Oozie server.
+     * <p/>
+     * It uses Hadoop-auth client authentication which by default supports
+     * Kerberos HTTP SPNEGO, Pseudo/Simple and anonymous.
+     * <p/>
+     * if the Java system property {@link #USE_AUTH_TOKEN_CACHE_SYS_PROP} is set to true Hadoop-auth
+     * authentication token will be cached/used in/from the '.oozie-auth-token' file in the user
+     * home directory.
+     *
+     * @param url the URL to open a HTTP connection to.
+     * @param method the HTTP method for the HTTP connection.
+     * @return an authenticated connection to the Oozie server.
+     * @throws IOException if an IO error occurred.
+     * @throws OozieClientException if an oozie client error occurred.
+     */
+    @Override
+    protected HttpURLConnection createConnection(URL url, String method) throws IOException, OozieClientException {
+        boolean useAuthFile = System.getProperty(USE_AUTH_TOKEN_CACHE_SYS_PROP, "false").equalsIgnoreCase("true");
+        AuthenticatedURL.Token readToken = new AuthenticatedURL.Token();
+        AuthenticatedURL.Token currentToken = new AuthenticatedURL.Token();
+
+        if (useAuthFile) {
+            readToken = readAuthToken();
+            if (readToken != null) {
+                currentToken = new AuthenticatedURL.Token(readToken.toString());
+            }
+        }
+
+        if (currentToken.isSet()) {
+            HttpURLConnection conn = (HttpURLConnection) url.openConnection();
+            conn.setRequestMethod("OPTIONS");
+            AuthenticatedURL.injectToken(conn, currentToken);
+            if (conn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED) {
+                AUTH_TOKEN_CACHE_FILE.delete();
+                currentToken = new AuthenticatedURL.Token();
+            }
+        }
+
+        if (!currentToken.isSet()) {
+            Authenticator authenticator = getAuthenticator();
+            try {
+                new AuthenticatedURL(authenticator).openConnection(url, currentToken);
+            }
+            catch (AuthenticationException ex) {
+                AUTH_TOKEN_CACHE_FILE.delete();
+                throw new OozieClientException(OozieClientException.AUTHENTICATION,
+                                               "Could not authenticate, " + ex.getMessage(), ex);
+            }
+        }
+        if (useAuthFile && !currentToken.equals(readToken)) {
+            writeAuthToken(currentToken);
+        }
+        HttpURLConnection conn = super.createConnection(url, method);
+
+        AuthenticatedURL.injectToken(conn, currentToken);
+        return conn;
+    }
+
+
+    /**
+     * Read a authentication token cached in the user home directory.
+     * <p/>
+     *
+     * @return the authentication token cached in the user home directory, NULL if none.
+     */
+    protected AuthenticatedURL.Token readAuthToken() {
+        AuthenticatedURL.Token authToken = null;
+        if (AUTH_TOKEN_CACHE_FILE.exists()) {
+            try {
+                BufferedReader reader = new BufferedReader(new FileReader(AUTH_TOKEN_CACHE_FILE));
+                String line = reader.readLine();
+                reader.close();
+                if (line != null) {
+                    authToken = new AuthenticatedURL.Token(line);
+                }
+            }
+            catch (IOException ex) {
+                //NOP
+            }
+        }
+        return authToken;
+    }
+
+    /**
+     * Write the current authenthication token to the user home directory.
+     * <p/>
+     * The file is written with user only read/write permissions.
+     * <p/>
+     * If the file cannot be updated or the user only ready/write permissions cannot be set the file is deleted.
+     *
+     * @param authToken the authentication token to cache.
+     */
+    protected void writeAuthToken(AuthenticatedURL.Token authToken) {
+        try {
+            Writer writer = new FileWriter(AUTH_TOKEN_CACHE_FILE);
+            writer.write(authToken.toString());
+            writer.close();
+            // sets read-write permissions to owner only
+            AUTH_TOKEN_CACHE_FILE.setReadable(false, false);
+            AUTH_TOKEN_CACHE_FILE.setReadable(true, true);
+            AUTH_TOKEN_CACHE_FILE.setWritable(true, true);
+        }
+        catch (Exception ex) {
+            // if case of any error we just delete the cache, if user-only
+            // write permissions are not properly set a security exception
+            // is thrown and the file will be deleted.
+            AUTH_TOKEN_CACHE_FILE.delete();
+        }
+    }
+
+    /**
+     * Return the Hadoop-auth Authenticator to use.
+     * <p/>
+     * It looks for value of the {@link #AUTHENTICATOR_CLASS_SYS_PROP} Java system property, if not set it uses
+     * Hadoop-auth <code>KerberosAuthenticator</code> which supports both Kerberos HTTP SPNEGO and Pseudo/simple
+     * authentication.
+     *
+     * @return the Authenticator to use, <code>NULL</code> if none.
+     *
+     * @throws OozieClientException thrown if the authenticator could not be instatiated.
+     */
+    protected Authenticator getAuthenticator() throws OozieClientException {
+        String className = System.getProperty(AUTHENTICATOR_CLASS_SYS_PROP, KerberosAuthenticator.class.getName());
+        if (className != null) {
+            try {
+                ClassLoader cl = Thread.currentThread().getContextClassLoader();
+                Class klass = (cl != null) ? cl.loadClass(className) : getClass().getClassLoader().loadClass(className);
+                return (Authenticator) klass.newInstance();
+            }
+            catch (Exception ex) {
+                throw new OozieClientException(OozieClientException.AUTHENTICATION,
+                                               "Could not instantiate Authenticator [" + className + "], " +
+                                               ex.getMessage(), ex);
+            }
+        }
+        else {
+            throw new OozieClientException(OozieClientException.AUTHENTICATION,
+                                           "Authenticator class not found [" + className + "]");
+        }
+    }
+
+}
diff --git client/src/main/java/org/apache/oozie/client/OozieClient.java client/src/main/java/org/apache/oozie/client/OozieClient.java
index dd3b962..1272f75 100644
--- client/src/main/java/org/apache/oozie/client/OozieClient.java
+++ client/src/main/java/org/apache/oozie/client/OozieClient.java
@@ -32,6 +32,7 @@ import java.util.Collections;
 import java.util.Enumeration;
 import java.util.HashMap;
 import java.util.Iterator;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
@@ -120,6 +121,8 @@ public class OozieClient {
     public static final String FILTER_FREQUENCY = "frequency";
 
     public static final String FILTER_ID = "id";
+    
+    public static final String FILTER_UNIT = "unit";
 
     public static final String CHANGE_VALUE_ENDTIME = "endtime";
 
@@ -145,6 +148,32 @@ public class OozieClient {
     private boolean validatedVersion = false;
     private final Map<String, String> headers = new HashMap<String, String>();
 
+    private static ThreadLocal<String> USER_NAME_TL = new ThreadLocal<String>();
+
+    /**
+     * Allows to impersonate other users in the Oozie server. The current user
+     * must be configured as a proxyuser in Oozie.
+     * <p/>
+     * IMPORTANT: impersonation happens only with Oozie client requests done within
+     * doAs() calls.
+     *
+     * @param userName user to impersonate.
+     * @param callable callable with {@link OozieClient} calls impersonating the specified user.
+     * @return any response returned by the {@link Callable#call()} method.
+     * @throws Exception thrown by the {@link Callable#call()} method.
+     */
+    public static <T> T doAs(String userName, Callable<T> callable) throws Exception {
+        notEmpty(userName, "userName");
+        notNull(callable, "callable");
+        try {
+            USER_NAME_TL.set(userName);
+            return callable.call();
+        }
+        finally {
+            USER_NAME_TL.remove();
+        }
+    }
+
     protected OozieClient() {
     }
 
@@ -253,7 +282,11 @@ public class OozieClient {
      */
     public Properties createConfiguration() {
         Properties conf = new Properties();
-        conf.setProperty(USER_NAME, System.getProperty("user.name"));
+        String userName = USER_NAME_TL.get();
+        if (userName == null) {
+            userName = System.getProperty("user.name");
+        }
+        conf.setProperty(USER_NAME, userName);
         return conf;
     }
 
@@ -411,10 +444,14 @@ public class OozieClient {
     }
 
     static Map<String, String> prepareParams(String... params) {
-        Map<String, String> map = new HashMap<String, String>();
+        Map<String, String> map = new LinkedHashMap<String, String>();
         for (int i = 0; i < params.length; i = i + 2) {
             map.put(params[i], params[i + 1]);
         }
+        String doAsUserName = USER_NAME_TL.get();
+        if (doAsUserName != null) {
+            map.put(RestConstants.DO_AS_PARAM, doAsUserName);
+        }
         return map;
     }
 
diff --git client/src/main/java/org/apache/oozie/client/WorkflowAction.java client/src/main/java/org/apache/oozie/client/WorkflowAction.java
index 4fab81f..ed84063 100644
--- client/src/main/java/org/apache/oozie/client/WorkflowAction.java
+++ client/src/main/java/org/apache/oozie/client/WorkflowAction.java
@@ -141,6 +141,20 @@ public interface WorkflowAction {
     String getData();
 
     /**
+     * Return the action statistics.
+     *
+     * @return the action statistics.
+     */
+    String getStats();
+
+    /**
+     * Return the external child IDs of the action.
+     *
+     * @return the external child IDs of the action.
+     */
+    String getExternalChildIDs();
+
+    /**
      * Return the external ID of the action.
      *
      * @return the external ID of the action.
diff --git client/src/main/java/org/apache/oozie/client/rest/JsonTags.java client/src/main/java/org/apache/oozie/client/rest/JsonTags.java
index 249d1f1..b8ae77f 100644
--- client/src/main/java/org/apache/oozie/client/rest/JsonTags.java
+++ client/src/main/java/org/apache/oozie/client/rest/JsonTags.java
@@ -65,6 +65,8 @@ public interface JsonTags {
     public static final String WORKFLOW_ACTION_STATUS = "status";
     public static final String WORKFLOW_ACTION_TRANSITION = "transition";
     public static final String WORKFLOW_ACTION_DATA = "data";
+    public static final String WORKFLOW_ACTION_STATS = "stats";
+    public static final String WORKFLOW_ACTION_EXTERNAL_CHILD_IDS = "externalChildIDs";
     public static final String WORKFLOW_ACTION_EXTERNAL_ID = "externalId";
     public static final String WORKFLOW_ACTION_EXTERNAL_STATUS = "externalStatus";
     public static final String WORKFLOW_ACTION_TRACKER_URI = "trackerUri";
diff --git client/src/main/java/org/apache/oozie/client/rest/JsonToBean.java client/src/main/java/org/apache/oozie/client/rest/JsonToBean.java
index 5c688a7..1454a72 100644
--- client/src/main/java/org/apache/oozie/client/rest/JsonToBean.java
+++ client/src/main/java/org/apache/oozie/client/rest/JsonToBean.java
@@ -75,6 +75,8 @@ public class JsonToBean {
         WF_ACTION.put("getEndTime", new Property(JsonTags.WORKFLOW_ACTION_END_TIME, Date.class));
         WF_ACTION.put("getTransition", new Property(JsonTags.WORKFLOW_ACTION_TRANSITION, String.class));
         WF_ACTION.put("getData", new Property(JsonTags.WORKFLOW_ACTION_DATA, String.class));
+        WF_ACTION.put("getStats", new Property(JsonTags.WORKFLOW_ACTION_STATS, String.class));
+        WF_ACTION.put("getExternalChildIDs", new Property(JsonTags.WORKFLOW_ACTION_EXTERNAL_CHILD_IDS, String.class));
         WF_ACTION.put("getExternalId", new Property(JsonTags.WORKFLOW_ACTION_EXTERNAL_ID, String.class));
         WF_ACTION.put("getExternalStatus", new Property(JsonTags.WORKFLOW_ACTION_EXTERNAL_STATUS, String.class));
         WF_ACTION.put("getTrackerUri", new Property(JsonTags.WORKFLOW_ACTION_TRACKER_URI, String.class));
diff --git client/src/main/java/org/apache/oozie/client/rest/RestConstants.java client/src/main/java/org/apache/oozie/client/rest/RestConstants.java
index c899258..305eb6f 100644
--- client/src/main/java/org/apache/oozie/client/rest/RestConstants.java
+++ client/src/main/java/org/apache/oozie/client/rest/RestConstants.java
@@ -134,4 +134,6 @@ public interface RestConstants {
     public static final String MAX_EVENTS = "max-events";
 
     public static final String SLA = "sla";
+
+    public static final String DO_AS_PARAM = "doAs";
 }
diff --git client/src/main/resources/hive-action-0.2.xsd client/src/main/resources/hive-action-0.2.xsd
new file mode 100644
index 0000000..884bd5f
--- /dev/null
+++ client/src/main/resources/hive-action-0.2.xsd
@@ -0,0 +1,68 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema"
+           xmlns:hive="uri:oozie:hive-action:0.2" elementFormDefault="qualified"
+           targetNamespace="uri:oozie:hive-action:0.2">
+
+    <xs:element name="hive" type="hive:ACTION"/>
+
+    <xs:complexType name="ACTION">
+        <xs:sequence>
+            <xs:element name="job-tracker" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="name-node" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="prepare" type="hive:PREPARE" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="job-xml" type="xs:string" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="configuration" type="hive:CONFIGURATION" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="script" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="param" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="file" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="archive" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+        </xs:sequence>
+    </xs:complexType>
+
+    <xs:complexType name="CONFIGURATION">
+        <xs:sequence>
+            <xs:element name="property" minOccurs="1" maxOccurs="unbounded">
+                <xs:complexType>
+                    <xs:sequence>
+                        <xs:element name="name" minOccurs="1" maxOccurs="1" type="xs:string"/>
+                        <xs:element name="value" minOccurs="1" maxOccurs="1" type="xs:string"/>
+                        <xs:element name="description" minOccurs="0" maxOccurs="1" type="xs:string"/>
+                    </xs:sequence>
+                </xs:complexType>
+            </xs:element>
+        </xs:sequence>
+    </xs:complexType>
+
+    <xs:complexType name="PREPARE">
+        <xs:sequence>
+            <xs:element name="delete" type="hive:DELETE" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="mkdir" type="hive:MKDIR" minOccurs="0" maxOccurs="unbounded"/>
+        </xs:sequence>
+    </xs:complexType>
+
+    <xs:complexType name="DELETE">
+        <xs:attribute name="path" type="xs:string" use="required"/>
+    </xs:complexType>
+
+    <xs:complexType name="MKDIR">
+        <xs:attribute name="path" type="xs:string" use="required"/>
+    </xs:complexType>
+
+</xs:schema>
diff --git client/src/main/resources/shell-action-0.1.xsd client/src/main/resources/shell-action-0.1.xsd
new file mode 100644
index 0000000..4794c0d
--- /dev/null
+++ client/src/main/resources/shell-action-0.1.xsd
@@ -0,0 +1,73 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+<xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema"
+           xmlns:shell="uri:oozie:shell-action:0.1" elementFormDefault="qualified"
+           targetNamespace="uri:oozie:shell-action:0.1">
+
+    <xs:element name="shell" type="shell:ACTION"/>
+
+    <xs:complexType name="ACTION">
+      <xs:sequence>
+            <xs:element name="job-tracker" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="name-node" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="prepare" type="shell:PREPARE" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="job-xml" type="xs:string" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="configuration" type="shell:CONFIGURATION" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="exec" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="argument" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="env-var" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="file" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="archive" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="capture-output" type="shell:FLAG" minOccurs="0" maxOccurs="1"/>
+        </xs:sequence>
+    </xs:complexType>
+
+    <xs:complexType name="FLAG"/>
+
+    <xs:complexType name="CONFIGURATION">
+        <xs:sequence>
+            <xs:element name="property" minOccurs="1" maxOccurs="unbounded">
+                <xs:complexType>
+                    <xs:sequence>
+                        <xs:element name="name" minOccurs="1" maxOccurs="1" type="xs:string"/>
+                        <xs:element name="value" minOccurs="1" maxOccurs="1" type="xs:string"/>
+                        <xs:element name="description" minOccurs="0" maxOccurs="1" type="xs:string"/>
+                    </xs:sequence>
+                </xs:complexType>
+            </xs:element>
+        </xs:sequence>
+    </xs:complexType>
+
+    <xs:complexType name="PREPARE">
+        <xs:sequence>
+            <xs:element name="delete" type="shell:DELETE" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="mkdir" type="shell:MKDIR" minOccurs="0" maxOccurs="unbounded"/>
+        </xs:sequence>
+    </xs:complexType>
+
+    <xs:complexType name="DELETE">
+        <xs:attribute name="path" type="xs:string" use="required"/>
+    </xs:complexType>
+
+    <xs:complexType name="MKDIR">
+        <xs:attribute name="path" type="xs:string" use="required"/>
+    </xs:complexType>
+
+</xs:schema>
diff --git client/src/main/resources/sqoop-action-0.2.xsd client/src/main/resources/sqoop-action-0.2.xsd
new file mode 100644
index 0000000..8e2ebd3
--- /dev/null
+++ client/src/main/resources/sqoop-action-0.2.xsd
@@ -0,0 +1,70 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema"
+           xmlns:sqoop="uri:oozie:sqoop-action:0.2" elementFormDefault="qualified"
+           targetNamespace="uri:oozie:sqoop-action:0.2">
+
+    <xs:element name="sqoop" type="sqoop:ACTION"/>
+
+    <xs:complexType name="ACTION">
+        <xs:sequence>
+            <xs:element name="job-tracker" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="name-node" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="prepare" type="sqoop:PREPARE" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="job-xml" type="xs:string" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="configuration" type="sqoop:CONFIGURATION" minOccurs="0" maxOccurs="1"/>
+            <xs:choice>
+                <xs:element name="command" type="xs:string" minOccurs="1" maxOccurs="1"/>
+                <xs:element name="arg" type="xs:string" minOccurs="1" maxOccurs="unbounded"/>
+            </xs:choice>
+            <xs:element name="file" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="archive" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+        </xs:sequence>
+    </xs:complexType>
+
+    <xs:complexType name="CONFIGURATION">
+        <xs:sequence>
+            <xs:element name="property" minOccurs="1" maxOccurs="unbounded">
+                <xs:complexType>
+                    <xs:sequence>
+                        <xs:element name="name" minOccurs="1" maxOccurs="1" type="xs:string"/>
+                        <xs:element name="value" minOccurs="1" maxOccurs="1" type="xs:string"/>
+                        <xs:element name="description" minOccurs="0" maxOccurs="1" type="xs:string"/>
+                    </xs:sequence>
+                </xs:complexType>
+            </xs:element>
+        </xs:sequence>
+    </xs:complexType>
+
+    <xs:complexType name="PREPARE">
+        <xs:sequence>
+            <xs:element name="delete" type="sqoop:DELETE" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="mkdir" type="sqoop:MKDIR" minOccurs="0" maxOccurs="unbounded"/>
+        </xs:sequence>
+    </xs:complexType>
+
+    <xs:complexType name="DELETE">
+        <xs:attribute name="path" type="xs:string" use="required"/>
+    </xs:complexType>
+
+    <xs:complexType name="MKDIR">
+        <xs:attribute name="path" type="xs:string" use="required"/>
+    </xs:complexType>
+
+</xs:schema>
diff --git client/src/main/resources/ssh-action-0.1.xsd client/src/main/resources/ssh-action-0.1.xsd
new file mode 100644
index 0000000..e8a6f9e
--- /dev/null
+++ client/src/main/resources/ssh-action-0.1.xsd
@@ -0,0 +1,36 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema"
+           xmlns:ssh="uri:oozie:ssh-action:0.1" elementFormDefault="qualified"
+           targetNamespace="uri:oozie:ssh-action:0.1">
+
+    <xs:element name="ssh" type="ssh:ACTION"/>
+
+    <xs:complexType name="ACTION">
+        <xs:sequence>
+            <xs:element name="host" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="command" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="args" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="capture-output" type="ssh:FLAG" minOccurs="0" maxOccurs="1"/>
+        </xs:sequence>
+    </xs:complexType>
+
+    <xs:complexType name="FLAG"/>
+
+</xs:schema>
diff --git core/pom.xml core/pom.xml
index 106893d..a56227d 100644
--- core/pom.xml
+++ core/pom.xml
@@ -16,22 +16,37 @@
   See the License for the specific language governing permissions and
   limitations under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
     <modelVersion>4.0.0</modelVersion>
     <parent>
         <groupId>org.apache.oozie</groupId>
         <artifactId>oozie-main</artifactId>
         <version>3.1.3-incubating</version>
     </parent>
+    <groupId>org.apache.oozie</groupId>
     <artifactId>oozie-core</artifactId>
-    <description>Oozie Core</description>
-    <name>Oozie Core</name>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie Core</description>
+    <name>Apache Oozie Core</name>
     <packaging>jar</packaging>
 
     <dependencies>
 
         <dependency>
             <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-hadoop-test</artifactId>
+            <scope>test</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-hadoop</artifactId>
+            <scope>provided</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.oozie</groupId>
             <artifactId>oozie-client</artifactId>
             <scope>compile</scope>
         </dependency>
@@ -57,7 +72,18 @@
         <dependency>
             <groupId>javax.persistence</groupId>
             <artifactId>persistence-api</artifactId>
-            <scope>compile</scope>
+            <scope>provided</scope>
+            <exclusions>
+                <exclusion>
+                    <groupId>commons-lang</groupId>
+                    <artifactId>commons-lang</artifactId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+
+        <dependency>
+            <groupId>commons-lang</groupId>
+            <artifactId>commons-lang</artifactId>
         </dependency>
 
         <dependency>
@@ -73,34 +99,6 @@
         </dependency>
 
         <dependency>
-            <groupId>org.apache.hadoop</groupId>
-            <artifactId>hadoop-core</artifactId>
-            <scope>provided</scope>
-        </dependency>
-        <dependency>
-            <groupId>org.apache.hadoop</groupId>
-            <artifactId>hadoop-test</artifactId>
-            <scope>test</scope>
-        </dependency>
-        <dependency>
-            <groupId>com.sun.jersey</groupId>
-            <artifactId>jersey-server</artifactId>
-            <scope>test</scope>
-        </dependency>
-
-        <dependency>
-            <groupId>org.apache.oozie</groupId>
-            <artifactId>oozie-sharelib</artifactId>
-            <scope>provided</scope>
-        </dependency>
-
-        <dependency>
-           <groupId>org.slf4j</groupId>
-           <artifactId>slf4j-log4j12</artifactId>
-           <scope>test</scope>
-        </dependency>
-
-        <dependency>
             <groupId>com.googlecode.json-simple</groupId>
             <artifactId>json-simple</artifactId>
             <scope>compile</scope>
@@ -143,24 +141,6 @@
             <scope>compile</scope>
         </dependency>
 
-		<dependency>
-			<groupId>org.python</groupId>
-			<artifactId>jython</artifactId>
-			<scope>test</scope>
-		</dependency>
-
-		<dependency>
-			<groupId>org.antlr</groupId>
-			<artifactId>antlr-runtime</artifactId>
-			<scope>test</scope>
-		</dependency>
-
-		<dependency>
-			<groupId>com.google.guava</groupId>
-			<artifactId>guava</artifactId>
-			<scope>test</scope>
-		</dependency>
-
         <!--
         Oozie web-app module must exclude it.
          -->
@@ -190,6 +170,11 @@
         <dependency>
             <groupId>javax.mail</groupId>
             <artifactId>mail</artifactId>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-auth</artifactId>
             <scope>compile</scope>
         </dependency>
 
@@ -205,6 +190,54 @@
             <scope>compile</scope>
         </dependency>
 
+        <dependency>
+            <groupId>org.slf4j</groupId>
+            <artifactId>slf4j-api</artifactId>
+            <scope>compile</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.slf4j</groupId>
+            <artifactId>slf4j-log4j12</artifactId>
+            <scope>compile</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-sharelib-pig</artifactId>
+            <scope>provided</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-sharelib-streaming</artifactId>
+            <scope>provided</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-sharelib-hive</artifactId>
+            <scope>provided</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-sharelib-sqoop</artifactId>
+            <scope>provided</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-sharelib-oozie</artifactId>
+            <scope>provided</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.mockito</groupId>
+            <artifactId>mockito-all</artifactId>
+            <scope>test</scope>
+        </dependency>
+
     </dependencies>
 
     <build>
@@ -234,6 +267,23 @@
                 </configuration>
             </plugin>
             <plugin>
+              <artifactId>maven-dependency-plugin</artifactId>
+              <executions>
+                <execution>
+                  <id>create-mrapp-generated-classpath</id>
+                  <phase>generate-test-resources</phase>
+                  <goals>
+                    <goal>build-classpath</goal>
+                  </goals>
+                  <configuration>
+                    <!-- needed to run the unit test for DS to generate the required classpath
+                         that is required in the env of the launch container in the mini mr/yarn cluster -->
+                    <outputFile>${project.build.directory}/test-classes/mrapp-generated-classpath</outputFile>
+                  </configuration>
+                </execution>
+              </executions>
+            </plugin>
+            <plugin>
                 <groupId>org.apache.maven.plugins</groupId>
                 <artifactId>maven-antrun-plugin</artifactId>
                 <executions>
@@ -242,32 +292,32 @@
                         <configuration>
                             <target>
                                 <path id="cp">
-                                    <path refid="maven.test.classpath" />
-                                    <path refid="maven.compile.classpath" />
-                                    <path refid="maven.dependency.classpath" />
+                                    <path refid="maven.test.classpath"/>
+                                    <path refid="maven.compile.classpath"/>
+                                    <path refid="maven.dependency.classpath"/>
                                 </path>
                                 <taskdef name="openjpac" classname="org.apache.openjpa.ant.PCEnhancerTask">
-                                    <classpath refid="cp" />
+                                    <classpath refid="cp"/>
                                 </taskdef>
                                 <fileset id="enhance.path.ref" dir="../core/target/classes">
-                                    <include name="**/JsonWorkflowJob.class" />
-                                    <include name="**/JsonWorkflowAction.class" />
-                                    <include name="**/JsonCoordinatorJob.class" />
-                                    <include name="**/JsonCoordinatorAction.class" />
-                                    <include name="**/JsonSLAEvent.class" />
+                                    <include name="**/JsonWorkflowJob.class"/>
+                                    <include name="**/JsonWorkflowAction.class"/>
+                                    <include name="**/JsonCoordinatorJob.class"/>
+                                    <include name="**/JsonCoordinatorAction.class"/>
+                                    <include name="**/JsonSLAEvent.class"/>
                                     <include name="**/JsonBundleJob.class"/>
 
-                                    <include name="**/WorkflowJobBean.class" />
-                                    <include name="**/WorkflowActionBean.class" />
-                                    <include name="**/CoordinatorJobBean.class" />
-                                    <include name="**/CoordinatorActionBean.class" />
-                                    <include name="**/SLAEventBean.class" />
+                                    <include name="**/WorkflowJobBean.class"/>
+                                    <include name="**/WorkflowActionBean.class"/>
+                                    <include name="**/CoordinatorJobBean.class"/>
+                                    <include name="**/CoordinatorActionBean.class"/>
+                                    <include name="**/SLAEventBean.class"/>
                                     <include name="**/BundleJobBean.class"/>
                                     <include name="**/BundleActionBean.class"/>
                                 </fileset>
                                 <openjpac>
-                                    <classpath refid="cp" />
-                                    <fileset refid="enhance.path.ref" />
+                                    <classpath refid="cp"/>
+                                    <fileset refid="enhance.path.ref"/>
                                 </openjpac>
                             </target>
                         </configuration>
@@ -278,17 +328,17 @@
                 </executions>
             </plugin>
             <plugin>
-                 <groupId>org.apache.rat</groupId>
-                 <artifactId>apache-rat-plugin</artifactId>
-                 <configuration>
-                       <excludes>
-                       <exclude>src/test/resources/PigMain.txt</exclude>
-                       <exclude>src/test/resources/test-ioutils.txt</exclude>
-                       <exclude>.gitignore</exclude>
-                       <exclude>./SecurityAuth.audit</exclude>
-                       </excludes>
-                  </configuration>
-	     </plugin>
+                <groupId>org.apache.rat</groupId>
+                <artifactId>apache-rat-plugin</artifactId>
+                <configuration>
+                    <excludes>
+                        <exclude>src/test/resources/PigMain.txt</exclude>
+                        <exclude>src/test/resources/test-ioutils.txt</exclude>
+                        <exclude>.gitignore</exclude>
+                        <exclude>./SecurityAuth.audit</exclude>
+                    </excludes>
+                </configuration>
+            </plugin>
         </plugins>
     </build>
 
@@ -307,26 +357,26 @@
                         <groupId>org.apache.maven.plugins</groupId>
                         <artifactId>maven-javadoc-plugin</artifactId>
                         <configuration>
-		                    <linksource>true</linksource>
-		                    <quiet>true</quiet>
-		                    <verbose>false</verbose>
-		                    <source>${maven.compile.source}</source>
-		                    <charset>${maven.compile.encoding}</charset>
-		                    <groups>
-		                        <group>
-		                            <title>Action Executor API</title>
-		                            <packages>
-		                                org.apache.oozie.action
-		                            </packages>
-		                        </group>
-		                        <group>
-		                            <title>Local Oozie (for application development/testing)</title>
-		                            <packages>
-		                                org.apache.oozie.local
-		                            </packages>
-		                        </group>
-		                    </groups>
-		                </configuration>
+                            <linksource>true</linksource>
+                            <quiet>true</quiet>
+                            <verbose>false</verbose>
+                            <source>${maven.compile.source}</source>
+                            <charset>${maven.compile.encoding}</charset>
+                            <groups>
+                                <group>
+                                    <title>Action Executor API</title>
+                                    <packages>
+                                        org.apache.oozie.action
+                                    </packages>
+                                </group>
+                                <group>
+                                    <title>Local Oozie (for application development/testing)</title>
+                                    <packages>
+                                        org.apache.oozie.local
+                                    </packages>
+                                </group>
+                            </groups>
+                        </configuration>
                         <executions>
                             <execution>
                                 <goals>
@@ -341,22 +391,25 @@
         </profile>
 
         <profile>
-            <id>hadoop20</id>
+            <id>oozieci</id>
             <activation>
                 <activeByDefault>false</activeByDefault>
                 <property>
-                    <name>hadoop20</name>
-                    <value>true</value>
+                    <name>oozieci</name>
                 </property>
             </activation>
             <build>
                 <plugins>
                     <plugin>
                         <groupId>org.apache.maven.plugins</groupId>
-                        <artifactId>maven-compiler-plugin</artifactId>
+                        <artifactId>maven-surefire-plugin</artifactId>
                         <configuration>
                             <excludes>
-                                <exclude>**/Kerberos*.java</exclude>
+                                <exclude>**/TestRerun.java</exclude>
+                                <exclude>**/TestCallableQueueService.java</exclude>
+                                <exclude>**/TestPigActionExecutor.java</exclude>
+                                <exclude>**/TestCallableQueueService.java</exclude>
+                                <exclude>**/TestSshActionExecutor.java</exclude>
                             </excludes>
                         </configuration>
                     </plugin>
@@ -364,91 +417,64 @@
             </build>
         </profile>
 
-		<profile>
-			<id>oozieci</id>
-			<activation>
-				<activeByDefault>false</activeByDefault>
-				<property>
-					<name>oozieci</name>
-				</property>
-			</activation>
-			<build>
-				<plugins>
-					<plugin>
-						<groupId>org.apache.maven.plugins</groupId>
-						<artifactId>maven-surefire-plugin</artifactId>
-						<configuration>
-						    <excludes>
-						        <exclude>**/TestRerun.java</exclude>
-						        <exclude>**/TestCallableQueueService.java</exclude>
-						        <exclude>**/TestPigActionExecutor.java</exclude>
-						        <exclude>**/TestCallableQueueService.java</exclude>
-						        <exclude>**/TestSshActionExecutor.java</exclude>
-						    </excludes>
-						</configuration>
-					</plugin>
-				</plugins>
-			</build>
-		</profile>
-
-		<profile>
-			<id>test-jar-all</id>
-			<activation>
-				<activeByDefault>true</activeByDefault>
-				<property>
-					<name>testJarAll</name>
-				</property>
-			</activation>
-			<build>
-				<plugins>
-					<plugin>
-						<groupId>org.apache.maven.plugins</groupId>
-						<artifactId>maven-jar-plugin</artifactId>
-						<executions>
-							<execution>
-								<goals>
-									<goal>test-jar</goal>
-								</goals>
-							</execution>
-						</executions>
-					</plugin>
-				</plugins>
-			</build>
-		</profile>
-
-		<profile>
-			<id>test-jar-simple</id>
-			<activation>
-				<activeByDefault>false</activeByDefault>
-				<property>
-					<name>testJarSimple</name>
-				</property>
-			</activation>
-			<build>
-				<plugins>
-					<plugin>
-						<groupId>org.apache.maven.plugins</groupId>
-						<artifactId>maven-jar-plugin</artifactId>
-						<executions>
-							<execution>
-								<goals>
-									<goal>test-jar</goal>
-								</goals>
-								<configuration>
-									<includes>
-										<include>**/XTestCase.class</include>
-										<include>**/XTestCase$1.class</include>
-										<include>**/XFsTestCase.class</include>
-										<include>**/MiniOozieTestCase.class</include>
-										<include>**/XTestCase$Predicate.class</include>										
-									</includes>
-								</configuration>
-							</execution>
-						</executions>
-					</plugin>
-				</plugins>
-			</build>
-		</profile>
+        <profile>
+            <id>test-jar-all</id>
+            <activation>
+                <activeByDefault>true</activeByDefault>
+                <property>
+                    <name>testJarAll</name>
+                </property>
+            </activation>
+            <build>
+                <plugins>
+                    <plugin>
+                        <groupId>org.apache.maven.plugins</groupId>
+                        <artifactId>maven-jar-plugin</artifactId>
+                        <executions>
+                            <execution>
+                                <goals>
+                                    <goal>test-jar</goal>
+                                </goals>
+                            </execution>
+                        </executions>
+                    </plugin>
+                </plugins>
+            </build>
+        </profile>
+
+        <profile>
+            <id>test-jar-simple</id>
+            <activation>
+                <activeByDefault>false</activeByDefault>
+                <property>
+                    <name>testJarSimple</name>
+                </property>
+            </activation>
+            <build>
+                <plugins>
+                    <plugin>
+                        <groupId>org.apache.maven.plugins</groupId>
+                        <artifactId>maven-jar-plugin</artifactId>
+                        <executions>
+                            <execution>
+                                <goals>
+                                    <goal>test-jar</goal>
+                                </goals>
+                                <configuration>
+                                    <includes>
+                                        <include>**/XTestCase.class</include>
+                                        <include>**/XTestCase$1.class</include>
+                                        <include>**/XFsTestCase.class</include>
+                                        <include>**/MiniOozieTestCase.class</include>
+                                        <include>**/XTestCase$Predicate.class</include>
+                                    </includes>
+                                </configuration>
+                            </execution>
+                        </executions>
+                    </plugin>
+                </plugins>
+            </build>
+        </profile>
 
         <!-- Include MySQL JDBC driver -->
         <profile>
@@ -456,7 +482,8 @@
             <activation>
                 <activeByDefault>false</activeByDefault>
                 <property>
-                    <name>mysql</name>
+                    <name>oozie.test.db</name>
+                    <value>mysql</value>
                 </property>
             </activation>
             <dependencies>
@@ -476,7 +503,8 @@
             <activation>
                 <activeByDefault>false</activeByDefault>
                 <property>
-                    <name>oracle</name>
+                    <name>oozie.test.db</name>
+                    <value>oracle</value>
                 </property>
             </activation>
             <dependencies>
@@ -487,6 +515,54 @@
             </dependencies>
         </profile>
 
+        <!-- Forcing antlr-runtime 3.0.1 for Hive action testcases -->
+        <!-- This is required because Pig 0.9.0 requires 3.4  and  -->
+        <!-- Hive 0.9.0-SNAPSHOT requires 3.0.1                    -->
+        <profile>
+            <id>testHive</id>
+            <activation>
+                <activeByDefault>false</activeByDefault>
+                <property>
+                    <name>testHive</name>
+                </property>
+            </activation>
+            <dependencies>
+                <dependency>
+                    <groupId>org.antlr</groupId>
+                    <artifactId>antlr-runtime</artifactId>
+                    <version>3.0.1</version>
+                    <scope>test</scope>
+                </dependency>
+            </dependencies>
+        </profile>
+
+        <!-- Forcing Hadoop 0.20.2-cdh3u2 version because Sqoop    -->
+        <!-- uses Hadoop API that is avail 0.21 onwards and the    -->
+        <!-- Hadoop CDH version is the only stable version that    -->
+        <!-- currently has such API (SQOOP-384) and published      -->
+        <!-- in public Maven repos. This is a temporary fix        -->
+        <profile>
+            <id>testSqoop</id>
+            <activation>
+                <activeByDefault>false</activeByDefault>
+                <property>
+                    <name>testSqoop</name>
+                </property>
+            </activation>
+            <properties>
+                <hadoop.version>0.20.2-cdh3u2</hadoop.version>
+            </properties>
+            <repositories>
+                <repository>
+                    <id>cdh.releases.repo</id>
+                    <url>https://repository.cloudera.com/content/groups/cdh-releases-rcs</url>
+                    <name>CDH Releases Repository</name>
+                    <snapshots>
+                        <enabled>false</enabled>
+                    </snapshots>
+                </repository>
+            </repositories>
+        </profile>
     </profiles>
 
 </project>
diff --git core/src/main/conf/oozie-log4j.properties core/src/main/conf/oozie-log4j.properties
index 3db1419..a14ae89 100644
--- core/src/main/conf/oozie-log4j.properties
+++ core/src/main/conf/oozie-log4j.properties
@@ -6,9 +6,9 @@
 # to you under the Apache License, Version 2.0 (the
 # "License"); you may not use this file except in compliance
 # with the License.  You may obtain a copy of the License at
-# 
+#
 #      http://www.apache.org/licenses/LICENSE-2.0
-# 
+#
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -71,3 +71,4 @@ log4j.logger.org.apache.oozie=DEBUG, oozie
 log4j.logger.org.apache.hadoop=WARN, oozie
 log4j.logger.org.mortbay=WARN, oozie
 log4j.logger.org.hsqldb=WARN, oozie
+log4j.logger.org.apache.hadoop.security.authentication.server=DEBUG, oozie
diff --git core/src/main/conf/oozie-site.xml core/src/main/conf/oozie-site.xml
index e17ba7b..fd58686 100644
--- core/src/main/conf/oozie-site.xml
+++ core/src/main/conf/oozie-site.xml
@@ -18,24 +18,29 @@
 -->
 <configuration>
 
-<!--
-    Refer to the oozie-default.xml file for the complete list of
-    Oozie configuration properties and their default values.
--->
+    <!--
+        Refer to the oozie-default.xml file for the complete list of
+        Oozie configuration properties and their default values.
+    -->
+
+    <!-- Uncomment the following  properties to enable Email & Hive actions
 
-<!-- Uncomment to enable additional actions (email).
     <property>
         <name>oozie.service.ActionService.executor.ext.classes</name>
         <value>
-        org.apache.oozie.action.email.EmailActionExecutor
+            org.apache.oozie.action.email.EmailActionExecutor,
+            org.apache.oozie.action.hadoop.HiveActionExecutor,
+            org.apache.oozie.action.hadoop.ShellActionExecutor,
+            org.apache.oozie.action.hadoop.SqoopActionExecutor
         </value>
     </property>
 
     <property>
         <name>oozie.service.SchemaService.wf.ext.schemas</name>
-        <value>email-action-0.1.xsd</value>
+        <value>shell-action-0.1.xsd,email-action-0.1.xsd,hive-action-0.2.xsd,sqoop-action-0.2.xsd,ssh-action-0.1.xsd</value>
     </property>
--->
+
+    -->
 
     <property>
         <name>oozie.system.id</name>
@@ -80,7 +85,7 @@
 
     <property>
         <name>oozie.service.CallableQueueService.queue.size</name>
-        <value>1000</value>
+        <value>10000</value>
         <description>Max callable queue size</description>
     </property>
 
@@ -120,7 +125,7 @@
 
     <property>
         <name>oozie.service.StoreService.create.db.schema</name>
-        <value>true</value>
+        <value>false</value>
         <description>
             Creates Oozie DB.
 
@@ -244,5 +249,110 @@
         </description>
     </property>
 
-</configuration>
+    <property>
+        <name>oozie.authentication.type</name>
+        <value>simple</value>
+        <description>
+            Defines authentication used for Oozie HTTP endpoint.
+            Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#
+        </description>
+    </property>
+
+    <property>
+        <name>oozie.authentication.token.validity</name>
+        <value>36000</value>
+        <description>
+            Indicates how long (in seconds) an authentication token is valid before it has
+            to be renewed.
+        </description>
+    </property>
+
+    <property>
+        <name>oozie.authentication.signature.secret</name>
+        <value>oozie</value>
+        <description>
+            The signature secret for signing the authentication tokens.
+            If not set a random secret is generated at startup time.
+            In order to authentiation to work correctly across multiple hosts
+            the secret must be the same across al the hosts.
+        </description>
+    </property>
+
+    <property>
+      <name>oozie.authentication.cookie.domain</name>
+      <value></value>
+      <description>
+        The domain to use for the HTTP cookie that stores the authentication token.
+        In order to authentiation to work correctly across multiple hosts
+        the domain must be correctly set.
+      </description>
+    </property>
+
+    <property>
+        <name>oozie.authentication.simple.anonymous.allowed</name>
+        <value>true</value>
+        <description>
+            Indicates if anonymous requests are allowed.
+            This setting is meaningful only when using 'simple' authentication.
+        </description>
+    </property>
+
+    <property>
+        <name>oozie.authentication.kerberos.principal</name>
+        <value>HTTP/localhost@${local.realm}</value>
+        <description>
+            Indicates the Kerberos principal to be used for HTTP endpoint.
+            The principal MUST start with 'HTTP/' as per Kerberos HTTP SPNEGO specification.
+        </description>
+    </property>
+
+    <property>
+        <name>oozie.authentication.kerberos.keytab</name>
+        <value>${oozie.service.HadoopAccessorService.keytab.file}</value>
+        <description>
+            Location of the keytab file with the credentials for the principal.
+            Referring to the same keytab file Oozie uses for its Kerberos credentials for Hadoop.
+        </description>
+    </property>
+
+    <!-- Proxyuser Configuration -->
+
+    <!--
+
+    <property>
+        <name>oozie.service.ProxyUserService.proxyuser.#USER#.hosts</name>
+        <value>*</value>
+        <description>
+            List of hosts the '#USER#' user is allowed to perform 'doAs'
+            operations.
 
+            The '#USER#' must be replaced with the username o the user who is
+            allowed to perform 'doAs' operations.
+
+            The value can be the '*' wildcard or a list of hostnames.
+
+            For multiple users copy this property and replace the user name
+            in the property name.
+        </description>
+    </property>
+
+    <property>
+        <name>oozie.service.ProxyUserService.proxyuser.#USER#.groups</name>
+        <value>*</value>
+        <description>
+            List of groups the '#USER#' user is allowed to impersonate users
+            from to perform 'doAs' operations.
+
+            The '#USER#' must be replaced with the username o the user who is
+            allowed to perform 'doAs' operations.
+
+            The value can be the '*' wildcard or a list of groups.
+
+            For multiple users copy this property and replace the user name
+            in the property name.
+        </description>
+    </property>
+
+    -->
+
+</configuration>
diff --git core/src/main/java/org/apache/oozie/CoordinatorActionBean.java core/src/main/java/org/apache/oozie/CoordinatorActionBean.java
index 54f36cd..b058097 100644
--- core/src/main/java/org/apache/oozie/CoordinatorActionBean.java
+++ core/src/main/java/org/apache/oozie/CoordinatorActionBean.java
@@ -78,8 +78,10 @@ import org.apache.openjpa.persistence.jdbc.Index;
     @NamedQuery(name = "GET_COORD_ACTIONS_PENDING_FALSE_STATUS_COUNT", query = "select count(a) from CoordinatorActionBean a where a.jobId = :jobId AND a.pending = 0 AND a.status = :status"),
 
     @NamedQuery(name = "GET_ACTIONS_FOR_COORD_JOB", query = "select OBJECT(a) from CoordinatorActionBean a where a.jobId = :jobId"),
-
-    @NamedQuery(name = "GET_COORD_ACTION_FOR_COORD_JOB_BY_ACTION_NUMBER", query = "select OBJECT(a) from CoordinatorActionBean a where a.jobId = :jobId AND a.actionNumber = :actionNumber"),
+    // Query to retrieve Coordinator actions sorted by nominal time
+    @NamedQuery(name = "GET_ACTIONS_FOR_COORD_JOB_ORDER_BY_NOMINAL_TIME", query = "select OBJECT(a) from CoordinatorActionBean a where a.jobId = :jobId order by a.nominalTimestamp"),
+    // Query to retrieve action id, action status, pending status and external Id of not completed Coordinator actions
+    @NamedQuery(name = "GET_COORD_ACTIONS_NOT_COMPLETED", query = "select a.id, a.status, a.pending, a.externalId from CoordinatorActionBean a where a.jobId = :jobId AND a.status <> 'FAILED' AND a.status <> 'TIMEDOUT' AND a.status <> 'SUCCEEDED' AND a.status <> 'KILLED'"),
 
     @NamedQuery(name = "GET_COORD_ACTIONS_BY_LAST_MODIFIED_TIME", query = "select OBJECT(w) from CoordinatorActionBean w where w.lastModifiedTimestamp >= :lastModifiedTime"),
 
@@ -114,6 +116,7 @@ public class CoordinatorActionBean extends JsonCoordinatorAction implements
     private String status = null;
 
     @Basic
+    @Index
     @Column(name = "nominal_time")
     private java.sql.Timestamp nominalTimestamp = null;
 
diff --git core/src/main/java/org/apache/oozie/CoordinatorEngine.java core/src/main/java/org/apache/oozie/CoordinatorEngine.java
index 26b2f97..adbf893 100644
--- core/src/main/java/org/apache/oozie/CoordinatorEngine.java
+++ core/src/main/java/org/apache/oozie/CoordinatorEngine.java
@@ -505,6 +505,7 @@ public class CoordinatorEngine extends BaseEngine {
         FILTER_NAMES.add(OozieClient.FILTER_STATUS);
         FILTER_NAMES.add(OozieClient.FILTER_ID);
         FILTER_NAMES.add(OozieClient.FILTER_FREQUENCY);
+        FILTER_NAMES.add(OozieClient.FILTER_UNIT);
     }
 
     /**
@@ -537,6 +538,10 @@ public class CoordinatorEngine extends BaseEngine {
      */
     private Map<String, List<String>> parseFilter(String filter) throws CoordinatorEngineException {
         Map<String, List<String>> map = new HashMap<String, List<String>>();
+        boolean isTimeUnitSpecified = false;
+        String timeUnit = "MINUTE";
+        boolean isFrequencySpecified = false;
+        String frequency = "";
         if (filter != null) {
             StringTokenizer st = new StringTokenizer(filter, ";");
             while (st.hasMoreTokens()) {
@@ -547,10 +552,32 @@ public class CoordinatorEngine extends BaseEngine {
                         throw new CoordinatorEngineException(ErrorCode.E0420, filter,
                                 "elements must be name=value pairs");
                     }
-                    if (!FILTER_NAMES.contains(pair[0])) {
+                    if (!FILTER_NAMES.contains(pair[0].toLowerCase())) {
                         throw new CoordinatorEngineException(ErrorCode.E0420, filter, XLog.format("invalid name [{0}]",
                                 pair[0]));
                     }
+                    if (pair[0].equalsIgnoreCase("frequency")) {
+                        isFrequencySpecified = true;
+                        try {
+                            frequency = (int) Float.parseFloat(pair[1]) + "";
+                            continue;
+                        }
+                        catch (NumberFormatException NANException) {
+                            throw new CoordinatorEngineException(ErrorCode.E0420, filter, XLog.format(
+                                    "invalid value [{0}] for frequency. A numerical value is expected", pair[1]));
+                        }
+                    }
+                    if (pair[0].equalsIgnoreCase("unit")) {
+                        isTimeUnitSpecified = true;
+                        timeUnit = pair[1];
+                        if (!timeUnit.equalsIgnoreCase("months") && !timeUnit.equalsIgnoreCase("days")
+                                && !timeUnit.equalsIgnoreCase("hours") && !timeUnit.equalsIgnoreCase("minutes")) {
+                            throw new CoordinatorEngineException(ErrorCode.E0420, filter, XLog.format(
+                                    "invalid value [{0}] for time unit. "
+                                            + "Valid value is one of months, days, hours or minutes", pair[1]));
+                        }
+                        continue;
+                    }
                     if (pair[0].equals("status")) {
                         try {
                             CoordinatorJob.Status.valueOf(pair[1]);
@@ -566,11 +593,38 @@ public class CoordinatorEngine extends BaseEngine {
                         map.put(pair[0], list);
                     }
                     list.add(pair[1]);
-                }
-                else {
+                } else {
                     throw new CoordinatorEngineException(ErrorCode.E0420, filter, "elements must be name=value pairs");
                 }
             }
+            // Unit is specified and frequency is not specified
+            if (!isFrequencySpecified && isTimeUnitSpecified) {
+                throw new CoordinatorEngineException(ErrorCode.E0420, filter, "time unit should be added only when "
+                        + "frequency is specified. Either specify frequency also or else remove the time unit");
+            } else if (isFrequencySpecified) {
+                // Frequency value is specified
+                if (isTimeUnitSpecified) {
+                    if (timeUnit.equalsIgnoreCase("months")) {
+                        timeUnit = "MONTH";
+                    } else if (timeUnit.equalsIgnoreCase("days")) {
+                        timeUnit = "DAY";
+                    } else if (timeUnit.equalsIgnoreCase("hours")) {
+                        // When job details are persisted to database, frequency in hours are converted to minutes.
+                        // This conversion is to conform with that.
+                        frequency = Integer.parseInt(frequency) * 60 + "";
+                        timeUnit = "MINUTE";
+                    } else if (timeUnit.equalsIgnoreCase("minutes")) {
+                        timeUnit = "MINUTE";
+                    }
+                }
+                // Adding the frequency and time unit filters to the filter map
+                List<String> list = new ArrayList<String>();
+                list.add(timeUnit);
+                map.put("unit", list);
+                list = new ArrayList<String>();
+                list.add(frequency);
+                map.put("frequency", list);
+            }
         }
         return map;
     }
diff --git core/src/main/java/org/apache/oozie/DagELFunctions.java core/src/main/java/org/apache/oozie/DagELFunctions.java
index f7f7b69..4c15c36 100644
--- core/src/main/java/org/apache/oozie/DagELFunctions.java
+++ core/src/main/java/org/apache/oozie/DagELFunctions.java
@@ -18,6 +18,7 @@
 package org.apache.oozie;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.oozie.action.hadoop.MapReduceActionExecutor;
 import org.apache.oozie.client.WorkflowAction;
 import org.apache.oozie.service.CallbackService;
 import org.apache.oozie.workflow.WorkflowInstance;
@@ -38,6 +39,7 @@ import java.util.Map;
  */
 public class DagELFunctions {
 
+    public static final String HADOOP_JOBS_PREFIX = "hadoopJobs:";
     private static final String WORKFLOW = "oozie.el.workflow.bean";
     private static final String ACTION = "oozie.el.action.bean";
     private static final String ACTION_PROTO_CONF = "oozie.el.action.proto.conf";
@@ -111,6 +113,14 @@ public class DagELFunctions {
             workflowInstance
                     .setVar(action.getName() + WorkflowInstance.NODE_VAR_SEPARATOR + ACTION_DATA, action.getData());
         }
+        if (action.getExternalChildIDs() != null) {
+            workflowInstance.setVar(action.getName() + WorkflowInstance.NODE_VAR_SEPARATOR + ACTION_DATA,
+                    HADOOP_JOBS_PREFIX + action.getExternalChildIDs());
+        }
+        if (action.getStats() != null) {
+            workflowInstance.setVar(action.getName() + WorkflowInstance.NODE_VAR_SEPARATOR + MapReduceActionExecutor.HADOOP_COUNTERS,
+                    action.getStats());
+        }
         if (action.getErrorCode() != null) {
             workflowInstance.setVar(action.getName() + WorkflowInstance.NODE_VAR_SEPARATOR + ACTION_ERROR_CODE,
                                     action.getErrorCode());
diff --git core/src/main/java/org/apache/oozie/ErrorCode.java core/src/main/java/org/apache/oozie/ErrorCode.java
index b9497c6..656478e 100644
--- core/src/main/java/org/apache/oozie/ErrorCode.java
+++ core/src/main/java/org/apache/oozie/ErrorCode.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -39,6 +39,9 @@ public enum ErrorCode {
     E0026(XLog.OPS, "Missing required configuration property [{0}]"),
 
     E0100(XLog.OPS, "Could not initialize service [{0}], {1}"),
+    E0101(XLog.OPS, "Service [{0}] does not implement declared interface [{1}]"),
+    E0102(XLog.OPS, "Could not instanciate service class [{0}], {1}"),
+    E0103(XLog.OPS, "Could not load service classes, {0}"),
     E0110(XLog.OPS, "Could not parse or validate EL definition [{0}], {1}"),
     E0111(XLog.OPS, "class#method not found [{0}#{1}]"),
     E0112(XLog.OPS, "class#method does not have PUBLIC or STATIC modifier [{0}#{1}]"),
@@ -85,6 +88,9 @@ public enum ErrorCode {
     E0509(XLog.OPS, "User [{0}] not authorized for Coord job [{1}]"),
     E0510(XLog.OPS, "Unable to get Credential [{0}]"),
 
+    E0550(XLog.OPS, "Could not normalize host name [{0}], {1}"),
+    E0551(XLog.OPS, "Missing [{0}] property"),
+
     E0600(XLog.OPS, "Could not get connection, {0}"),
     E0601(XLog.OPS, "Could not close connection, {0}"),
     E0602(XLog.OPS, "Could not commit connection, {0}"),
@@ -127,6 +133,11 @@ public enum ErrorCode {
     E0727(XLog.STD, "Workflow Job can not be suspended as its not in running state, {0}, Status: {1}"),
     E0728(XLog.STD, "Coordinator Job can not be suspended as job finished or failed or killed, id : {0}, status : {1}"),
     E0729(XLog.OPS, "Kill node message [{0}]"),
+    E0730(XLog.STD, "Fork/Join not in pair"),
+    E0731(XLog.STD, "Fork node [{0}] cannot have less than two paths"),
+    E0732(XLog.STD, "Fork [{0}]/Join [{1}] not in pair"),
+    E0733(XLog.STD, "Fork [{0}] without a join"),
+    E0734(XLog.STD, "Invalid transition from node [{0}] to node [{1}] while using fork/join"),
 
     E0800(XLog.STD, "Action it is not running its in [{1}] state, action [{0}]"),
     E0801(XLog.STD, "Workflow already running, workflow [{0}]"),
@@ -153,6 +164,8 @@ public enum ErrorCode {
     E0900(XLog.OPS, "Jobtracker [{0}] not allowed, not in Oozie's whitelist"),
     E0901(XLog.OPS, "Namenode [{0}] not allowed, not in Oozie's whitelist"),
     E0902(XLog.OPS, "Exception occured: [{0}]"),
+    E0903(XLog.OPS, "Configuration does not have Jobtracker Kerberos name or rule"),
+    E0904(XLog.OPS, "Configuration does not have Namenode Kerberos name or rule"),
 
     E1001(XLog.STD, "Could not read the coordinator job definition, {0}"),
     E1002(XLog.STD, "Invalid coordinator application URI [{0}], {1}"),
@@ -203,6 +216,8 @@ public enum ErrorCode {
     E1318(XLog.STD, "No coord jobs for the bundle=[{0}], fail the bundle"),
     E1319(XLog.STD, "Invalid bundle coord job namespace, [{0}]"),
 
+    E1400(XLog.STD, "doAs (proxyuser) failure"),
+
     ETEST(XLog.STD, "THIS SHOULD HAPPEN ONLY IN TESTING, invalid job id [{0}]"),;
 
     private String template;
diff --git core/src/main/java/org/apache/oozie/LocalOozieClientCoord.java core/src/main/java/org/apache/oozie/LocalOozieClientCoord.java
index 5e0df32..6a3dc1d 100644
--- core/src/main/java/org/apache/oozie/LocalOozieClientCoord.java
+++ core/src/main/java/org/apache/oozie/LocalOozieClientCoord.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -30,6 +30,9 @@ import org.apache.oozie.client.OozieClientException;
 import org.apache.oozie.client.WorkflowJob;
 import org.apache.oozie.client.rest.JsonCoordinatorAction;
 import org.apache.oozie.client.rest.JsonCoordinatorJob;
+import org.apache.oozie.client.rest.RestConstants;
+import org.apache.oozie.command.CommandException;
+import org.apache.oozie.command.coord.CoordRerunXCommand;
 import org.apache.oozie.util.XConfiguration;
 
 /**
@@ -252,15 +255,28 @@ public class LocalOozieClientCoord extends OozieClient {
     public List<CoordinatorAction> reRunCoord(String jobId, String rerunType, String scope, boolean refresh,
             boolean noCleanup) throws OozieClientException {
         try {
+            if (!(rerunType.equals(RestConstants.JOB_COORD_RERUN_DATE) || rerunType
+                    .equals(RestConstants.JOB_COORD_RERUN_ACTION))) {
+                throw new CommandException(ErrorCode.E1018, "date or action expected.");
+            }
             CoordinatorActionInfo coordInfo = coordEngine.reRun(jobId, rerunType, scope, Boolean.valueOf(refresh),
                     Boolean.valueOf(noCleanup));
-            List<CoordinatorActionBean> actionBeans = coordInfo.getCoordActions();
+            List<CoordinatorActionBean> actionBeans;
+            if (coordInfo != null) {
+                actionBeans = coordInfo.getCoordActions();
+            }
+            else {
+                actionBeans = CoordRerunXCommand.getCoordActions(rerunType, jobId, scope);
+            }
             List<CoordinatorAction> actions = new ArrayList<CoordinatorAction>();
             for (CoordinatorActionBean actionBean : actionBeans) {
                 actions.add(actionBean);
             }
             return actions;
         }
+        catch(CommandException ce){
+            throw new OozieClientException(ce.getErrorCode().toString(), ce);
+        }
         catch (BaseEngineException ex) {
             throw new OozieClientException(ex.getErrorCode().toString(), ex);
         }
@@ -354,7 +370,7 @@ public class LocalOozieClientCoord extends OozieClient {
 
     /**
      * Get the info of a coordinator action.
-     * 
+     *
      * @param actionId Id.
      * @return the coordinator action info.
      * @throws OozieClientException thrown if the job info could not be
diff --git core/src/main/java/org/apache/oozie/WorkflowActionBean.java core/src/main/java/org/apache/oozie/WorkflowActionBean.java
index 10981d1..ddecdea 100644
--- core/src/main/java/org/apache/oozie/WorkflowActionBean.java
+++ core/src/main/java/org/apache/oozie/WorkflowActionBean.java
@@ -47,7 +47,7 @@ import org.apache.openjpa.persistence.jdbc.Index;
 @Entity
 @NamedQueries({
 
-    @NamedQuery(name = "UPDATE_ACTION", query = "update WorkflowActionBean a set a.conf = :conf, a.consoleUrl = :consoleUrl, a.data = :data, a.errorCode = :errorCode, a.errorMessage = :errorMessage, a.externalId = :externalId, a.externalStatus = :externalStatus, a.name = :name, a.cred = :cred , a.retries = :retries, a.trackerUri = :trackerUri, a.transition = :transition, a.type = :type, a.endTimestamp = :endTime, a.executionPath = :executionPath, a.lastCheckTimestamp = :lastCheckTime, a.logToken = :logToken, a.pending = :pending, a.pendingAgeTimestamp = :pendingAge, a.signalValue = :signalValue, a.slaXml = :slaXml, a.startTimestamp = :startTime, a.status = :status, a.wfId=:wfId where a.id = :id"),
+    @NamedQuery(name = "UPDATE_ACTION", query = "update WorkflowActionBean a set a.conf = :conf, a.consoleUrl = :consoleUrl, a.data = :data, a.stats = :stats, a.externalChildIDs = :externalChildIDs, a.errorCode = :errorCode, a.errorMessage = :errorMessage, a.externalId = :externalId, a.externalStatus = :externalStatus, a.name = :name, a.cred = :cred , a.retries = :retries, a.trackerUri = :trackerUri, a.transition = :transition, a.type = :type, a.endTimestamp = :endTime, a.executionPath = :executionPath, a.lastCheckTimestamp = :lastCheckTime, a.logToken = :logToken, a.pending = :pending, a.pendingAgeTimestamp = :pendingAge, a.signalValue = :signalValue, a.slaXml = :slaXml, a.startTimestamp = :startTime, a.status = :status, a.wfId=:wfId where a.id = :id"),
 
     @NamedQuery(name = "DELETE_ACTION", query = "delete from WorkflowActionBean a where a.id = :id"),
 
@@ -149,6 +149,8 @@ public class WorkflowActionBean extends JsonWorkflowAction implements Writable {
         dataOutput.writeLong((getLastCheckTime() != null) ? getLastCheckTime().getTime() : -1);
         WritableUtils.writeStr(dataOutput, getTransition());
         WritableUtils.writeStr(dataOutput, getData());
+        WritableUtils.writeStr(dataOutput, getStats());
+        WritableUtils.writeStr(dataOutput, getExternalChildIDs());
         WritableUtils.writeStr(dataOutput, getExternalId());
         WritableUtils.writeStr(dataOutput, getExternalStatus());
         WritableUtils.writeStr(dataOutput, getTrackerUri());
@@ -194,6 +196,8 @@ public class WorkflowActionBean extends JsonWorkflowAction implements Writable {
         }
         setTransition(WritableUtils.readStr(dataInput));
         setData(WritableUtils.readStr(dataInput));
+        setStats(WritableUtils.readStr(dataInput));
+        setExternalChildIDs(WritableUtils.readStr(dataInput));
         setExternalId(WritableUtils.readStr(dataInput));
         setExternalStatus(WritableUtils.readStr(dataInput));
         setTrackerUri(WritableUtils.readStr(dataInput));
@@ -353,6 +357,42 @@ public class WorkflowActionBean extends JsonWorkflowAction implements Writable {
     }
 
     /**
+     * Return the action statistics info.
+     *
+     * @return Json representation of the stats.
+     */
+    public String getExecutionStats() {
+        return getStats();
+    }
+
+    /**
+     * Set the action statistics info for the workflow action.
+     *
+     * @param Json representation of the stats.
+     */
+    public void setExecutionStats(String jsonStats) {
+        setStats(jsonStats);
+    }
+
+    /**
+     * Return the external child IDs.
+     *
+     * @return externalChildIDs as a string.
+     */
+    public String getExternalChildIDs() {
+        return super.getExternalChildIDs();
+    }
+
+    /**
+     * Set the external child IDs for the workflow action.
+     *
+     * @param externalChildIDs as a string.
+     */
+    public void setExternalChildIDs(String externalChildIDs) {
+        super.setExternalChildIDs(externalChildIDs);
+    }
+
+    /**
      * Set the completion information for an action end.
      *
      * @param status action status, {@link Action.Status#OK} or {@link Action.Status#ERROR} or {@link
diff --git core/src/main/java/org/apache/oozie/action/ActionExecutor.java core/src/main/java/org/apache/oozie/action/ActionExecutor.java
index 919d8b0..34991de 100644
--- core/src/main/java/org/apache/oozie/action/ActionExecutor.java
+++ core/src/main/java/org/apache/oozie/action/ActionExecutor.java
@@ -134,6 +134,22 @@ public abstract class ActionExecutor {
         void setExecutionData(String externalStatus, Properties actionData);
 
         /**
+         * Set execution statistics information for a particular action. The action status is set to {@link
+         * org.apache.oozie.client.WorkflowAction.Status#DONE}
+         *
+         * @param jsonStats the JSON string representation of the stats.
+         */
+        void setExecutionStats(String jsonStats);
+
+        /**
+         * Set external child IDs for a particular action (Eg: pig). The action status is set to {@link
+         * org.apache.oozie.client.WorkflowAction.Status#DONE}
+         *
+         * @param externalChildIDs the external child IDs as a comma-delimited string.
+         */
+        void setExternalChildIDs(String externalChildIDs);
+
+        /**
          * Set the action end completion information for a completed action.
          *
          * @param status the action end status, it can be {@link org.apache.oozie.client.WorkflowAction.Status#OK} or
diff --git core/src/main/java/org/apache/oozie/action/hadoop/ActionStats.java core/src/main/java/org/apache/oozie/action/hadoop/ActionStats.java
new file mode 100644
index 0000000..e6eaa3b
--- /dev/null
+++ core/src/main/java/org/apache/oozie/action/hadoop/ActionStats.java
@@ -0,0 +1,38 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.oozie.action.hadoop;
+
+/**
+ * Abstract class that represents the statistics for any type of Oozie action.
+ */
+public abstract class ActionStats {
+    protected ActionType currentActionType;
+
+    /**
+     * Return the statistics in JSON format. Sub-classes must override this
+     * method to provide statistics for their particular action type.
+     *
+     * @return a JSON string with the statistics information.
+     */
+    public abstract String toJSON();
+
+    public ActionType getCurrentActionType() {
+        return currentActionType;
+    }
+}
\ No newline at end of file
diff --git core/src/main/java/org/apache/oozie/action/hadoop/ActionType.java core/src/main/java/org/apache/oozie/action/hadoop/ActionType.java
new file mode 100644
index 0000000..fc74254
--- /dev/null
+++ core/src/main/java/org/apache/oozie/action/hadoop/ActionType.java
@@ -0,0 +1,26 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.oozie.action.hadoop;
+
+/**
+ * Enumerator that lists the various kinds of actions supported by Oozie.
+ */
+public enum ActionType {
+    MAP_REDUCE, PIG, HIVE
+}
\ No newline at end of file
diff --git core/src/main/java/org/apache/oozie/action/hadoop/FsActionExecutor.java core/src/main/java/org/apache/oozie/action/hadoop/FsActionExecutor.java
index 5370858..75b6c58 100644
--- core/src/main/java/org/apache/oozie/action/hadoop/FsActionExecutor.java
+++ core/src/main/java/org/apache/oozie/action/hadoop/FsActionExecutor.java
@@ -136,7 +136,7 @@ public class FsActionExecutor extends ActionExecutor {
         String user = context.getWorkflow().getUser();
         String group = context.getWorkflow().getGroup();
         return Services.get().get(HadoopAccessorService.class).createFileSystem(user, group, path.toUri(),
-                new Configuration());
+                context.getProtoActionConf());
     }
 
     /**
diff --git core/src/main/java/org/apache/oozie/action/hadoop/HiveActionExecutor.java core/src/main/java/org/apache/oozie/action/hadoop/HiveActionExecutor.java
new file mode 100644
index 0000000..d9133db
--- /dev/null
+++ core/src/main/java/org/apache/oozie/action/hadoop/HiveActionExecutor.java
@@ -0,0 +1,136 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.action.hadoop;
+
+import static org.apache.oozie.action.hadoop.LauncherMapper.CONF_OOZIE_ACTION_MAIN_CLASS;
+
+import java.io.StringReader;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.oozie.action.ActionExecutorException;
+import org.apache.oozie.client.WorkflowAction;
+import org.apache.oozie.util.XConfiguration;
+import org.apache.oozie.util.XmlUtils;
+import org.jdom.Element;
+import org.jdom.JDOMException;
+import org.jdom.Namespace;
+
+public class HiveActionExecutor extends JavaActionExecutor {
+    private static final String OOZIE_HIVE_DEFAULTS = "oozie.hive.defaults";
+
+    public HiveActionExecutor() {
+        super("hive");
+    }
+
+    @Override
+    protected List<Class> getLauncherClasses() {
+        List<Class> classes = super.getLauncherClasses();
+        classes.add(LauncherMain.class);
+        classes.add(MapReduceMain.class);
+        classes.add(HiveMain.class);
+        return classes;
+    }
+
+    @Override
+    protected String getLauncherMain(Configuration launcherConf, Element actionXml) {
+        return launcherConf.get(CONF_OOZIE_ACTION_MAIN_CLASS, HiveMain.class.getName());
+    }
+
+    public Configuration setupHiveDefault(Configuration conf, Path appPath, Element actionXml)
+            throws ActionExecutorException {
+        try {
+            //Setting up hive-default.xml file if specified by the Hive action
+            Element actionConf = actionXml.getChild("configuration", actionXml.getNamespace());
+            if (actionConf != null) {
+                String strConf = XmlUtils.prettyPrint(actionConf).toString();
+                XConfiguration inlineConf = new XConfiguration(new StringReader(strConf));
+                if (inlineConf.get(OOZIE_HIVE_DEFAULTS) != null) {
+                    Path hiveDefaults = new Path(inlineConf.get(OOZIE_HIVE_DEFAULTS));
+                    // hive-default.xml will be softlinked to the working dir which is in the launcher CP.
+                    // the softlink is done as 'oozie-user-hive-default.xml' and Oozie HiveMain class will
+                    // check if the Hive being used has a hive-default.xml or not, if not it will rename
+                    // it as hive-default.xml before invoking hive
+                    addToCache(conf, appPath, hiveDefaults + "#" + HiveMain.USER_HIVE_DEFAULT_FILE, false);
+                }
+            }
+            return conf;
+        }
+        catch (Exception ex) {
+            throw convertException(ex);
+        }
+    }
+
+    @Override
+    protected Configuration setupLauncherConf(Configuration conf, Element actionXml, Path appPath, Context context)
+            throws ActionExecutorException {
+        try {
+            super.setupLauncherConf(conf, actionXml, appPath, context);
+            Namespace ns = actionXml.getNamespace();
+
+            setupHiveDefault(conf, appPath, actionXml);
+
+            String script = actionXml.getChild("script", ns).getTextTrim();
+            String scriptName = new Path(script).getName();
+            addToCache(conf, appPath, script + "#" + scriptName, false);
+            return conf;
+        }
+        catch (Exception ex) {
+            throw convertException(ex);
+        }
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    Configuration setupActionConf(Configuration actionConf, Context context, Element actionXml,
+                                  Path appPath) throws ActionExecutorException {
+        Configuration conf = super.setupActionConf(actionConf, context, actionXml, appPath);
+
+        Namespace ns = actionXml.getNamespace();
+        String script = actionXml.getChild("script", ns).getTextTrim();
+        String scriptName = new Path(script).getName();
+        addToCache(conf, appPath, script + "#" + scriptName, false);
+
+        List<Element> params = (List<Element>) actionXml.getChildren("param", ns);
+        String[] strParams = new String[params.size()];
+        for (int i = 0; i < params.size(); i++) {
+            strParams[i] = params.get(i).getTextTrim();
+        }
+
+        HiveMain.setHiveScript(conf, scriptName, strParams);
+        return conf;
+    }
+
+    @Override
+    protected boolean getCaptureOutput(WorkflowAction action) throws JDOMException {
+        return true;
+    }
+
+    /**
+     * Return the sharelib postfix for the action.
+     *
+     * @param context executor context.
+     * @param actionXml the action XML.
+     * @return the action sharelib post fix, this implementation returns <code>hive</code>.
+     */
+    protected String getShareLibPostFix(Context context, Element actionXml) {
+        return "hive";
+    }
+
+}
diff --git core/src/main/java/org/apache/oozie/action/hadoop/HiveMain.java core/src/main/java/org/apache/oozie/action/hadoop/HiveMain.java
new file mode 100644
index 0000000..006a16e
--- /dev/null
+++ core/src/main/java/org/apache/oozie/action/hadoop/HiveMain.java
@@ -0,0 +1,355 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.action.hadoop;
+
+import java.io.BufferedReader;
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.FileReader;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.net.URL;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Properties;
+import java.util.regex.Pattern;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.cli.CliDriver;
+
+public class HiveMain extends LauncherMain {
+    public static final String USER_HIVE_DEFAULT_FILE = "oozie-user-hive-default.xml";
+
+    private static final Pattern[] HIVE_JOB_IDS_PATTERNS = {
+      Pattern.compile("Ended Job = (job_\\S*)")
+    };
+
+    public static final String HIVE_L4J_PROPS = "hive-log4j.properties";
+    public static final String HIVE_EXEC_L4J_PROPS = "hive-exec-log4j.properties";
+    public static final String HIVE_SITE_CONF = "hive-site.xml";
+    private static final String HIVE_SCRIPT = "oozie.hive.script";
+    private static final String HIVE_PARAMS = "oozie.hive.params";
+
+    public static void main(String[] args) throws Exception {
+        run(HiveMain.class, args);
+    }
+
+    private static Configuration initActionConf() {
+        // Loading action conf prepared by Oozie
+        Configuration hiveConf = new Configuration(false);
+
+        String actionXml = System.getProperty("oozie.action.conf.xml");
+
+        if (actionXml == null) {
+            throw new RuntimeException("Missing Java System Property [oozie.action.conf.xml]");
+        }
+        if (!new File(actionXml).exists()) {
+            throw new RuntimeException("Action Configuration XML file [" + actionXml + "] does not exist");
+        } else {
+            System.out.println("Using action configuration file " + actionXml);
+        }
+
+        hiveConf.addResource(new Path("file:///", actionXml));
+
+        // Propagate delegation related props from launcher job to Hive job
+        String delegationToken = System.getenv("HADOOP_TOKEN_FILE_LOCATION");
+        if (delegationToken != null) {
+            hiveConf.set("mapreduce.job.credentials.binary", delegationToken);
+            System.out.println("------------------------");
+            System.out.println("Setting env property for mapreduce.job.credentials.binary to: " + delegationToken);
+            System.out.println("------------------------");
+            System.setProperty("mapreduce.job.credentials.binary", delegationToken);
+        } else {
+            System.out.println("Non-Kerberos execution");
+        }
+
+        // Have to explicitly unset this property or Hive will not set it.
+        hiveConf.set("mapred.job.name", "");
+
+        // See https://issues.apache.org/jira/browse/HIVE-1411
+        hiveConf.set("datanucleus.plugin.pluginRegistryBundleCheck", "LOG");
+
+        // to force hive to use the jobclient to submit the job, never using HADOOPBIN (to do localmode)
+        hiveConf.setBoolean("hive.exec.mode.local.auto", false);
+
+        return hiveConf;
+    }
+
+    public static String setUpHiveLog4J(Configuration hiveConf) throws IOException {
+        //Logfile to capture job IDs
+        String hadoopJobId = System.getProperty("oozie.launcher.job.id");
+        if (hadoopJobId == null) {
+            throw new RuntimeException("Launcher Hadoop Job ID system property not set");
+        }
+
+        String logFile = new File("hive-oozie-" + hadoopJobId + ".log").getAbsolutePath();
+
+        Properties hadoopProps = new Properties();
+
+        // Preparing log4j configuration
+        URL log4jFile = Thread.currentThread().getContextClassLoader().getResource("log4j.properties");
+        if (log4jFile != null) {
+            // getting hadoop log4j configuration
+            hadoopProps.load(log4jFile.openStream());
+        }
+
+        String logLevel = hiveConf.get("oozie.hive.log.level", "INFO");
+
+        hadoopProps.setProperty("log4j.logger.org.apache.hadoop.hive", logLevel + ", A");
+        hadoopProps.setProperty("log4j.logger.hive", logLevel + ", A");
+        hadoopProps.setProperty("log4j.logger.DataNucleus", logLevel + ", A");
+        hadoopProps.setProperty("log4j.logger.DataStore", logLevel + ", A");
+        hadoopProps.setProperty("log4j.logger.JPOX", logLevel + ", A");
+        hadoopProps.setProperty("log4j.appender.A", "org.apache.log4j.ConsoleAppender");
+        hadoopProps.setProperty("log4j.appender.A.layout", "org.apache.log4j.PatternLayout");
+        hadoopProps.setProperty("log4j.appender.A.layout.ConversionPattern", "%-4r [%t] %-5p %c %x - %m%n");
+
+        hadoopProps.setProperty("log4j.appender.jobid", "org.apache.log4j.FileAppender");
+        hadoopProps.setProperty("log4j.appender.jobid.file", logFile);
+        hadoopProps.setProperty("log4j.appender.jobid.layout", "org.apache.log4j.PatternLayout");
+        hadoopProps.setProperty("log4j.appender.jobid.layout.ConversionPattern", "%-4r [%t] %-5p %c %x - %m%n");
+        hadoopProps.setProperty("log4j.logger.org.apache.hadoop.hive.ql.exec", "INFO, jobid");
+
+        String localProps = new File(HIVE_L4J_PROPS).getAbsolutePath();
+        OutputStream os1 = new FileOutputStream(localProps);
+        hadoopProps.store(os1, "");
+        os1.close();
+
+        localProps = new File(HIVE_EXEC_L4J_PROPS).getAbsolutePath();
+        os1 = new FileOutputStream(localProps);
+        hadoopProps.store(os1, "");
+        os1.close();
+        return logFile;
+    }
+
+    public static Configuration setUpHiveSite() throws Exception {
+        Configuration hiveConf = initActionConf();
+
+        // Write the action configuration out to hive-site.xml
+        OutputStream os = new FileOutputStream(HIVE_SITE_CONF);
+        hiveConf.writeXml(os);
+        os.close();
+
+        System.out.println();
+        System.out.println("Hive Configuration Properties:");
+        System.out.println("------------------------");
+        for (Entry<String, String> entry : hiveConf) {
+            System.out.println(entry.getKey() + "=" + entry.getValue());
+        }
+        System.out.flush();
+        System.out.println("------------------------");
+        System.out.println();
+        return hiveConf;
+    }
+
+    protected void run(String[] args) throws Exception {
+        if (System.getenv("HADOOP_HOME") == null) {
+            throw new RuntimeException("'HADOOP_HOME' environment variable undefined, Hive cannot run");
+        }
+        System.out.println();
+        System.out.println("Oozie Hive action configuration");
+        System.out.println("=================================================================");
+
+        Configuration hiveConf = setUpHiveSite();
+
+        List<String> arguments = new ArrayList<String>();
+        String scriptPath = hiveConf.get(HIVE_SCRIPT);
+
+        if (scriptPath == null) {
+            throw new RuntimeException("Action Configuration does not have [" +  HIVE_SCRIPT + "] property");
+        }
+
+        if (!new File(scriptPath).exists()) {
+            throw new RuntimeException("Hive script file [" + scriptPath + "] does not exist");
+        }
+
+        // check if hive-default.xml is in the classpath, if not look for oozie-user-hive-default.xml
+        // in the current directory (it will be there if the Hive action has the 'oozie.hive.defaults'
+        // property) and rename it to hive-default.xml
+        if (Thread.currentThread().getContextClassLoader().getResource("hive-default.xml") == null) {
+            File userProvidedDefault = new File(USER_HIVE_DEFAULT_FILE);
+            if (userProvidedDefault.exists()) {
+                if (!userProvidedDefault.renameTo(new File("hive-default.xml"))) {
+                    throw new RuntimeException(
+                            "Could not rename user provided Hive defaults file to 'hive-default.xml'");
+                }
+                System.out.println("Using 'hive-default.xml' defined in the Hive action");
+            }
+            else {
+                throw new RuntimeException(
+                        "Hive JAR does not bundle a 'hive-default.xml' and Hive action does not define one");
+            }
+        }
+        else {
+            System.out.println("Using 'hive-default.xml' defined in the Hive JAR");
+            File userProvidedDefault = new File(USER_HIVE_DEFAULT_FILE);
+            if (userProvidedDefault.exists()) {
+                System.out.println("WARNING: Ignoring user provided Hive defaults");
+            }
+        }
+        System.out.println();
+
+        String logFile = setUpHiveLog4J(hiveConf);
+
+        // print out current directory & its contents
+        File localDir = new File("dummy").getAbsoluteFile().getParentFile();
+        System.out.println("Current (local) dir = " + localDir.getAbsolutePath());
+        System.out.println("------------------------");
+        for (String file : localDir.list()) {
+            System.out.println("  " + file);
+        }
+        System.out.println("------------------------");
+        System.out.println();
+
+        // Prepare the Hive Script
+        String script = readStringFromFile(scriptPath);
+        System.out.println();
+        System.out.println("Original script [" + scriptPath + "] content: ");
+        System.out.println("------------------------");
+        System.out.println(script);
+        System.out.println("------------------------");
+        System.out.println();
+
+        String[] params = MapReduceMain.getStrings(hiveConf, HIVE_PARAMS);
+        if (params.length > 0) {
+            Map<String, String> varMap = new HashMap<String, String>();
+            System.out.println("Parameters:");
+            System.out.println("------------------------");
+            for (String param : params) {
+                System.out.println("  " + param);
+
+                int idx = param.indexOf('=');
+                if (idx == -1) {
+                    throw new RuntimeException("Parameter expression must contain an assignment: " + param);
+                } else if (idx == 0) {
+                    throw new RuntimeException("Parameter value not specified: " + param);
+                }
+                String var = param.substring(0, idx);
+                String val = param.substring(idx + 1, param.length());
+                varMap.put(var, val);
+            }
+            System.out.println("------------------------");
+            System.out.println();
+
+            String resolvedScript = substitute(varMap, script);
+            scriptPath = scriptPath + ".sub";
+            writeStringToFile(scriptPath, resolvedScript);
+
+            System.out.println("Resolved script [" + scriptPath + "] content: ");
+            System.out.println("------------------------");
+            System.out.println(resolvedScript);
+            System.out.println("------------------------");
+            System.out.println();
+        }
+
+        arguments.add("-f");
+        arguments.add(scriptPath);
+
+
+        System.out.println("Hive command arguments :");
+        for (String arg : arguments) {
+            System.out.println("             " + arg);
+        }
+        System.out.println();
+
+        System.out.println("=================================================================");
+        System.out.println();
+        System.out.println(">>> Invoking Hive command line now >>>");
+        System.out.println();
+        System.out.flush();
+
+        try {
+            runHive(arguments.toArray(new String[arguments.size()]));
+        }
+        catch (SecurityException ex) {
+            if (LauncherSecurityManager.getExitInvoked()) {
+                if (LauncherSecurityManager.getExitCode() != 0) {
+                    throw ex;
+                }
+            }
+        }
+
+        System.out.println("\n<<< Invocation of Hive command completed <<<\n");
+
+        // harvesting and recording Hadoop Job IDs
+        Properties jobIds = getHadoopJobIds(logFile, HIVE_JOB_IDS_PATTERNS);
+        File file = new File(System.getProperty("oozie.action.output.properties"));
+        OutputStream os = new FileOutputStream(file);
+        jobIds.store(os, "");
+        os.close();
+        System.out.println(" Hadoop Job IDs executed by Hive: " + jobIds.getProperty(HADOOP_JOBS));
+        System.out.println();
+    }
+
+    private void runHive(String[] args) throws Exception {
+        CliDriver.main(args);
+    }
+
+    public static void setHiveScript(Configuration conf, String script, String[] params) {
+        conf.set(HIVE_SCRIPT, script);
+        MapReduceMain.setStrings(conf, HIVE_PARAMS, params);
+    }
+
+    private static String readStringFromFile(String filePath) throws IOException {
+        String line;
+        BufferedReader br = null;
+        try {
+            br = new BufferedReader(new FileReader(filePath));
+            StringBuilder sb = new StringBuilder();
+            String sep = System.getProperty("line.separator");
+            while ((line = br.readLine()) != null) {
+                sb.append(line).append(sep);
+            }
+            return sb.toString();
+        }
+        finally {
+            if (br != null) {
+                br.close();
+            }
+        }
+     }
+
+    private static void writeStringToFile(String filePath, String str) throws IOException {
+        BufferedWriter out = null;
+        try {
+            out = new BufferedWriter(new FileWriter(filePath));
+            out.write(str);
+        }
+        finally {
+            if (out != null) {
+                out.close();
+            }
+        }
+    }
+
+    static String substitute(Map<String, String> vars, String expr) {
+        for (Map.Entry<String, String> entry : vars.entrySet()) {
+            String var = "${" + entry.getKey() + "}";
+            String value = entry.getValue();
+            expr = expr.replace(var, value);
+        }
+        return expr;
+    }
+
+}
diff --git core/src/main/java/org/apache/oozie/action/hadoop/JavaActionExecutor.java core/src/main/java/org/apache/oozie/action/hadoop/JavaActionExecutor.java
index 00bb37c..b688ae7 100644
--- core/src/main/java/org/apache/oozie/action/hadoop/JavaActionExecutor.java
+++ core/src/main/java/org/apache/oozie/action/hadoop/JavaActionExecutor.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -26,6 +26,7 @@ import java.io.InputStreamReader;
 import java.io.StringReader;
 import java.net.ConnectException;
 import java.net.URI;
+import java.net.URISyntaxException;
 import java.net.UnknownHostException;
 import java.util.ArrayList;
 import java.util.HashMap;
@@ -38,6 +39,7 @@ import java.util.Map.Entry;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.permission.AccessControlException;
@@ -74,17 +76,19 @@ public class JavaActionExecutor extends ActionExecutor {
     private static final String HADOOP_UGI = "hadoop.job.ugi";
     private static final String HADOOP_JOB_TRACKER = "mapred.job.tracker";
     private static final String HADOOP_NAME_NODE = "fs.default.name";
-
+    public static final String OOZIE_COMMON_LIBDIR = "oozie";
+    public static final int MAX_EXTERNAL_STATS_SIZE_DEFAULT = Integer.MAX_VALUE;
     private static final Set<String> DISALLOWED_PROPERTIES = new HashSet<String>();
-
+    public final static String MAX_EXTERNAL_STATS_SIZE = "oozie.external.stats.max.size";
     private static int maxActionOutputLen;
+    private static int maxExternalStatsSize;
 
     private static final String SUCCEEDED = "SUCCEEDED";
     private static final String KILLED = "KILLED";
     private static final String FAILED = "FAILED";
     private static final String FAILED_KILLED = "FAILED/KILLED";
     private static final String RUNNING = "RUNNING";
-    private XLog log = XLog.getLog(getClass());
+    protected XLog log = XLog.getLog(getClass());
 
     static {
         DISALLOWED_PROPERTIES.add(HADOOP_USER);
@@ -113,6 +117,8 @@ public class JavaActionExecutor extends ActionExecutor {
         classes.add(LauncherSecurityManager.class);
         classes.add(LauncherException.class);
         classes.add(LauncherMainException.class);
+        classes.add(ActionStats.class);
+        classes.add(ActionType.class);
         return classes;
     }
 
@@ -120,6 +126,9 @@ public class JavaActionExecutor extends ActionExecutor {
     public void initActionType() {
         super.initActionType();
         maxActionOutputLen = getOozieConf().getInt(CallbackServlet.CONF_MAX_DATA_LEN, 2 * 1024);
+        //Get the limit for the maximum allowed size of action stats
+        maxExternalStatsSize = getOozieConf().getInt(JavaActionExecutor.MAX_EXTERNAL_STATS_SIZE, MAX_EXTERNAL_STATS_SIZE_DEFAULT);
+        maxExternalStatsSize = (maxExternalStatsSize == -1) ? Integer.MAX_VALUE : maxExternalStatsSize;
         try {
             List<Class> classes = getLauncherClasses();
             Class[] launcherClasses = classes.toArray(new Class[classes.size()]);
@@ -144,6 +153,15 @@ public class JavaActionExecutor extends ActionExecutor {
         }
     }
 
+    /**
+     * Get the maximum allowed size of stats
+     *
+     * @return maximum size of stats
+     */
+    public static int getMaxExternalStatsSize() {
+        return maxExternalStatsSize;
+    }
+
     void checkForDisallowedProps(Configuration conf, String confName) throws ActionExecutorException {
         for (String prop : DISALLOWED_PROPERTIES) {
             if (conf.get(prop) != null) {
@@ -199,6 +217,7 @@ public class JavaActionExecutor extends ActionExecutor {
                 checkForDisallowedProps(launcherConf, "inline launcher configuration");
                 XConfiguration.copy(launcherConf, conf);
             }
+            conf.set("mapreduce.framework.name", "yarn");
             return conf;
         }
         catch (IOException ex) {
@@ -245,6 +264,7 @@ public class JavaActionExecutor extends ActionExecutor {
                 checkForDisallowedProps(inlineConf, "inline configuration");
                 XConfiguration.copy(inlineConf, actionConf);
             }
+            actionConf.set("mapreduce.framework.name", "yarn");
             return actionConf;
         }
         catch (IOException ex) {
@@ -348,13 +368,45 @@ public class JavaActionExecutor extends ActionExecutor {
         }
     }
 
+    protected void addShareLib(Path appPath, Configuration conf, String actionShareLibPostfix)
+    throws ActionExecutorException {
+        if (actionShareLibPostfix != null) {
+            try {
+                Path systemLibPath = Services.get().get(WorkflowAppService.class).getSystemLibPath();
+                if (systemLibPath != null) {
+                    Path actionLibPath = new Path(systemLibPath, actionShareLibPostfix);
+                    String user = conf.get("user.name");
+                    String group = conf.get("group.name");
+                    FileSystem fs =
+                        Services.get().get(HadoopAccessorService.class).createFileSystem(user, group, conf);
+                    if (fs.exists(actionLibPath)) {
+                        FileStatus[] files = fs.listStatus(actionLibPath);
+                        for (FileStatus file : files) {
+                            addToCache(conf, appPath, file.getPath().toUri().getPath(), false);
+                        }
+                    }
+                }
+            }
+            catch (HadoopAccessorException ex){
+                throw new ActionExecutorException(ActionExecutorException.ErrorType.FAILED,
+                        ex.getErrorCode().toString(), ex.getMessage());
+            }
+            catch (IOException ex){
+                throw new ActionExecutorException(ActionExecutorException.ErrorType.FAILED,
+                        "It should never happen", ex.getMessage());
+            }
+        }
+    }
+
     @SuppressWarnings("unchecked")
     void setLibFilesArchives(Context context, Element actionXml, Path appPath, Configuration conf)
             throws ActionExecutorException {
         Configuration proto = context.getProtoActionConf();
 
+        // launcher JAR
         addToCache(conf, appPath, getOozieLauncherJar(context), false);
 
+        // Workflow lib/
         String[] paths = proto.getStrings(WorkflowAppService.APP_LIB_PATH_LIST);
         if (paths != null) {
             for (String path : paths) {
@@ -362,6 +414,7 @@ public class JavaActionExecutor extends ActionExecutor {
             }
         }
 
+        // files and archives defined in the action
         for (Element eProp : (List<Element>) actionXml.getChildren()) {
             if (eProp.getName().equals("file")) {
                 String path = eProp.getTextTrim();
@@ -374,8 +427,36 @@ public class JavaActionExecutor extends ActionExecutor {
                 }
             }
         }
+
+        addAllShareLibs(appPath, conf, context, actionXml);
+	}
+
+    // Adds action specific share libs and common share libs
+    private void addAllShareLibs(Path appPath, Configuration conf, Context context, Element actionXml)
+            throws ActionExecutorException {
+        // Add action specific share libs
+        addActionShareLib(appPath, conf, context, actionXml);
+        // Add common sharelibs for Oozie
+        addShareLib(appPath, conf, JavaActionExecutor.OOZIE_COMMON_LIBDIR);
+    }
+
+    private void addActionShareLib(Path appPath, Configuration conf, Context context, Element actionXml) throws ActionExecutorException {
+        XConfiguration wfJobConf = null;
+        try {
+            wfJobConf = new XConfiguration(new StringReader(context.getWorkflow().getConf()));
+        }
+        catch (IOException ioe) {
+            throw new ActionExecutorException(ActionExecutorException.ErrorType.FAILED, "It should never happen",
+                    ioe.getMessage());
+        }
+        // Action sharelibs are only added if user has specified to use system libpath
+        if (wfJobConf.getBoolean(OozieClient.USE_SYSTEM_LIBPATH, false)) {
+            // add action specific sharelibs
+            addShareLib(appPath, conf, getShareLibPostFix(context, actionXml));
+        }
     }
 
+
     protected String getLauncherMain(Configuration launcherConf, Element actionXml) {
         Namespace ns = actionXml.getNamespace();
         Element e = actionXml.getChild("main-class", ns);
@@ -429,6 +510,7 @@ public class JavaActionExecutor extends ActionExecutor {
             LauncherMapper.setupMainClass(launcherJobConf, getLauncherMain(launcherConf, actionXml));
 
             LauncherMapper.setupMaxOutputData(launcherJobConf, maxActionOutputLen);
+            LauncherMapper.setupMaxExternalStatsSize(launcherJobConf, maxExternalStatsSize);
 
             Namespace ns = actionXml.getNamespace();
             List<Element> list = actionXml.getChildren("arg", ns);
@@ -840,18 +922,7 @@ public class JavaActionExecutor extends ActionExecutor {
                     XLog.getLog(getClass()).info(XLog.STD, "action completed, external ID [{0}]",
                             action.getExternalId());
                     if (runningJob.isSuccessful() && LauncherMapper.isMainSuccessful(runningJob)) {
-                        Properties props = null;
-                        if (getCaptureOutput(action)) {
-                            props = new Properties();
-                            if (LauncherMapper.hasOutputData(runningJob)) {
-                                Path actionOutput = LauncherMapper.getOutputDataPath(context.getActionDir());
-                                InputStream is = actionFs.open(actionOutput);
-                                BufferedReader reader = new BufferedReader(new InputStreamReader(is));
-                                props = PropertiesUtils.readProperties(reader, maxActionOutputLen);
-                                reader.close();
-                            }
-                        }
-                        context.setExecutionData(SUCCEEDED, props);
+                        getActionData(actionFs, runningJob, action, context);
                         XLog.getLog(getClass()).info(XLog.STD, "action produced output");
                     }
                     else {
@@ -922,6 +993,32 @@ public class JavaActionExecutor extends ActionExecutor {
         }
     }
 
+    /**
+     * Get the output data of an action. Subclasses should override this method
+     * to get action specific output data.
+     *
+     * @param actionFs the FileSystem object
+     * @param runningJob the runningJob
+     * @param action the Workflow action
+     * @param context executor context
+     *
+     */
+    protected void getActionData(FileSystem actionFs, RunningJob runningJob, WorkflowAction action, Context context)
+            throws HadoopAccessorException, JDOMException, IOException, URISyntaxException {
+        Properties props = null;
+        if (getCaptureOutput(action)) {
+            props = new Properties();
+            if (LauncherMapper.hasOutputData(runningJob)) {
+                Path actionOutput = LauncherMapper.getOutputDataPath(context.getActionDir());
+                InputStream is = actionFs.open(actionOutput);
+                BufferedReader reader = new BufferedReader(new InputStreamReader(is));
+                props = PropertiesUtils.readProperties(reader, maxActionOutputLen);
+                reader.close();
+            }
+        }
+        context.setExecutionData(SUCCEEDED, props);
+    }
+
     protected boolean getCaptureOutput(WorkflowAction action) throws JDOMException {
         Element eConf = XmlUtils.parseXml(action.getConf());
         Namespace ns = eConf.getNamespace();
@@ -983,4 +1080,22 @@ public class JavaActionExecutor extends ActionExecutor {
         return FINAL_STATUS.contains(externalStatus);
     }
 
+    /**
+     * Return the sharelib postfix for the action.
+     * <p/>
+     * If <code>NULL</code> or emtpy, it means that the action does not use the action
+     * sharelib.
+     * <p/>
+     * If a non-empty string, i.e. <code>foo</code>, it means the action uses the
+     * action sharelib subdirectory <code>foo</code> and all JARs in the <code>foo</code>
+     * directory will be in the action classpath.
+     *
+     * @param context executor context.
+     * @param actionXml the action XML.
+     * @return the action sharelib post fix, this implementation returns <code>NULL</code>.
+     */
+    protected String getShareLibPostFix(Context context, Element actionXml) {
+        return null;
+    }
+
 }
diff --git core/src/main/java/org/apache/oozie/action/hadoop/LauncherMain.java core/src/main/java/org/apache/oozie/action/hadoop/LauncherMain.java
index 39ec316..18530ef 100644
--- core/src/main/java/org/apache/oozie/action/hadoop/LauncherMain.java
+++ core/src/main/java/org/apache/oozie/action/hadoop/LauncherMain.java
@@ -17,18 +17,54 @@
  */
 package org.apache.oozie.action.hadoop;
 
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileReader;
 import java.io.IOException;
 import java.io.StringWriter;
 import java.util.Collection;
 import java.util.Map;
+import java.util.Properties;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
 
 public abstract class LauncherMain {
 
+    public static final String HADOOP_JOBS = "hadoopJobs";
+
     protected static void run(Class<? extends LauncherMain> klass, String[] args) throws Exception {
         LauncherMain main = klass.newInstance();
         main.run(args);
     }
 
+    public static Properties getHadoopJobIds(String logFile, Pattern[] patterns) throws IOException {
+        Properties props = new Properties();
+        StringBuffer sb = new StringBuffer(100);
+        if (!new File(logFile).exists()) {
+            System.err.println("Log file: " + logFile + "  not present. Therefore no Hadoop jobids found");
+            props.setProperty(HADOOP_JOBS, "");
+        }
+        else {
+            BufferedReader br = new BufferedReader(new FileReader(logFile));
+            String line = br.readLine();
+            String separator = "";
+            while (line != null) {
+                for (Pattern pattern : patterns) {
+                    Matcher matcher = pattern.matcher(line);
+                    if (matcher.find()) {
+                        String jobId = matcher.group(1);
+                        sb.append(separator).append(jobId);
+                        separator = ",";
+                    }
+                }
+                line = br.readLine();
+            }
+            br.close();
+            props.setProperty(HADOOP_JOBS, sb.toString());
+        }
+        return props;
+    }
+
     protected abstract void run(String[] args) throws Exception;
 
     /**
diff --git core/src/main/java/org/apache/oozie/action/hadoop/LauncherMapper.java core/src/main/java/org/apache/oozie/action/hadoop/LauncherMapper.java
index 8171f7b..f39931d 100644
--- core/src/main/java/org/apache/oozie/action/hadoop/LauncherMapper.java
+++ core/src/main/java/org/apache/oozie/action/hadoop/LauncherMapper.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -58,10 +58,12 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
     private static final String CONF_OOZIE_ACTION_MAIN_ARG_COUNT = "oozie.action.main.arg.count";
     private static final String CONF_OOZIE_ACTION_MAIN_ARG_PREFIX = "oozie.action.main.arg.";
     private static final String CONF_OOZIE_ACTION_MAX_OUTPUT_DATA = "oozie.action.max.output.data";
+    private static final String CONF_OOZIE_EXTERNAL_STATS_MAX_SIZE = "oozie.external.stats.max.size";
 
     private static final String COUNTER_GROUP = "oozie.launcher";
     private static final String COUNTER_DO_ID_SWAP = "oozie.do.id.swap";
     private static final String COUNTER_OUTPUT_DATA = "oozie.output.data";
+    private static final String COUNTER_STATS_DATA = "oozie.stats.data";
     private static final String COUNTER_LAUNCHER_ERROR = "oozie.launcher.error";
 
     private static final String OOZIE_JOB_ID = "oozie.job.id";
@@ -70,8 +72,14 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
     private static final String OOZIE_ACTION_DIR_PATH = "oozie.action.dir.path";
     private static final String OOZIE_ACTION_RECOVERY_ID = "oozie.action.recovery.id";
 
+    public static final String ACTION_PREFIX = "oozie.action.";
+    public static final String EXTERNAL_CHILD_IDS = ACTION_PREFIX + "externalChildIDs.properties";
+    public static final String EXTERNAL_ACTION_STATS = ACTION_PREFIX + "stats.properties";
+
     static final String ACTION_CONF_XML = "action.xml";
     private static final String ACTION_OUTPUT_PROPS = "output.properties";
+    private static final String ACTION_STATS_PROPS = "stats.properties";
+    private static final String ACTION_EXTERNAL_CHILD_IDS_PROPS = "externalChildIds.properties";
     private static final String ACTION_NEW_ID_PROPS = "newId.properties";
     private static final String ACTION_ERROR_PROPS = "error.properties";
 
@@ -151,6 +159,16 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
     }
 
     /**
+     * Set the maximum value of stats data
+     *
+     * @param launcherConf the oozie launcher configuration
+     * @param maxStatsData the maximum allowed size of stats data
+     */
+    public static void setupMaxExternalStatsSize(Configuration launcherConf, int maxStatsData){
+        launcherConf.setInt(CONF_OOZIE_EXTERNAL_STATS_MAX_SIZE, maxStatsData);
+    }
+
+    /**
      * @param launcherConf
      * @param jobId
      * @param actionId
@@ -225,6 +243,25 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
     }
 
     /**
+     * Check whether runningJob has stats data or not
+     *
+     * @param runningJob the runningJob
+     * @return returns whether the running Job has stats data or not
+     * @throws IOException
+     */
+    public static boolean hasStatsData(RunningJob runningJob) throws IOException{
+        boolean output = false;
+        Counters counters = runningJob.getCounters();
+        if (counters != null) {
+            Counters.Group group = counters.getGroup(COUNTER_GROUP);
+            if (group != null) {
+                output = group.getCounter(COUNTER_STATS_DATA) == 1;
+            }
+        }
+        return output;
+    }
+
+    /**
      * @param runningJob
      * @return
      * @throws IOException
@@ -289,6 +326,26 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
         return new Path(actionDir, ACTION_OUTPUT_PROPS);
     }
 
+    /**
+     * Get the location of stats file
+     *
+     * @param actionDir the action directory
+     * @return the hdfs location of the file
+     */
+    public static Path getActionStatsDataPath(Path actionDir){
+        return new Path(actionDir, ACTION_STATS_PROPS);
+    }
+
+    /**
+     * Get the location of external Child IDs file
+     *
+     * @param actionDir the action directory
+     * @return the hdfs location of the file
+     */
+    public static Path getExternalChildIDsDataPath(Path actionDir){
+        return new Path(actionDir, ACTION_EXTERNAL_CHILD_IDS_PROPS);
+    }
+
     public static Path getErrorPath(Path actionDir) {
         return new Path(actionDir, ACTION_ERROR_PROPS);
     }
@@ -306,6 +363,7 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
     public LauncherMapper() {
     }
 
+    @Override
     public void configure(JobConf jobConf) {
         System.out.println();
         System.out.println("Oozie Launcher starts");
@@ -321,6 +379,7 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
         }
     }
 
+    @Override
     public void map(K1 key, V1 value, OutputCollector<K2, V2> collector, Reporter reporter) throws IOException {
         try {
             if (configFailure) {
@@ -395,7 +454,7 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
                     }
                     catch (InvocationTargetException ex) {
                         if (LauncherMainException.class.isInstance(ex.getCause())) {
-                            errorMessage = msgPrefix + "exit code [" +((LauncherMainException)ex.getCause()).getErrorCode() 
+                            errorMessage = msgPrefix + "exit code [" +((LauncherMainException)ex.getCause()).getErrorCode()
                                 + "]";
                             errorCause = null;
                         }
@@ -425,8 +484,9 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
                     }
                     if (errorMessage == null) {
                         File outputData = new File(System.getProperty("oozie.action.output.properties"));
+                        FileSystem fs = FileSystem.get(getJobConf());
                         if (outputData.exists()) {
-                            FileSystem fs = FileSystem.get(getJobConf());
+
                             fs.copyFromLocalFile(new Path(outputData.toString()), new Path(actionDir,
                                                                                            ACTION_OUTPUT_PROPS));
                             reporter.incrCounter(COUNTER_GROUP, COUNTER_OUTPUT_DATA, 1);
@@ -447,6 +507,8 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
                             System.out.println("=======================");
                             System.out.println();
                         }
+                        handleActionStatsData(fs, reporter);
+                        handleExternalChildIDs(fs, reporter);
                         File newId = new File(System.getProperty("oozie.action.newId.properties"));
                         if (newId.exists()) {
                             Properties props = new Properties();
@@ -454,7 +516,7 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
                             if (props.getProperty("id") == null) {
                                 throw new IllegalStateException("ID swap file does not have [id] property");
                             }
-                            FileSystem fs = FileSystem.get(getJobConf());
+                            fs = FileSystem.get(getJobConf());
                             fs.copyFromLocalFile(new Path(newId.toString()), new Path(actionDir, ACTION_NEW_ID_PROPS));
                             reporter.incrCounter(COUNTER_GROUP, COUNTER_DO_ID_SWAP, 1);
 
@@ -498,6 +560,7 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
         }
     }
 
+    @Override
     public void close() throws IOException {
         System.out.println();
         System.out.println("Oozie Launcher ends");
@@ -508,6 +571,35 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
         return jobConf;
     }
 
+    private void handleActionStatsData(FileSystem fs, Reporter reporter) throws IOException, LauncherException{
+        File actionStatsData = new File(System.getProperty(EXTERNAL_ACTION_STATS));
+        // If stats are stored by the action, then stats file should exist
+        if (actionStatsData.exists()) {
+            int statsMaxOutputData = getJobConf().getInt(CONF_OOZIE_EXTERNAL_STATS_MAX_SIZE,
+                    Integer.MAX_VALUE);
+            reporter.incrCounter(COUNTER_GROUP, COUNTER_STATS_DATA, 1);
+            // fail the launcher if size of stats is greater than the maximum allowed size
+            if (actionStatsData.length() > statsMaxOutputData) {
+                String msg = MessageFormat.format("Output stats size [{0}] exceeds maximum [{1}]",
+                        actionStatsData.length(), statsMaxOutputData);
+                failLauncher(0, msg, null);
+            }
+            // copy the stats file to hdfs path which can be accessed by Oozie server
+            fs.copyFromLocalFile(new Path(actionStatsData.toString()), new Path(actionDir,
+                    ACTION_STATS_PROPS));
+        }
+    }
+
+    private void handleExternalChildIDs(FileSystem fs, Reporter reporter) throws IOException {
+        File externalChildIDs = new File(System.getProperty(EXTERNAL_CHILD_IDS));
+        // if external ChildIDs are stored by the action, then the file should exist
+        if (externalChildIDs.exists()) {
+            // copy the externalChildIDs file to hdfs path which can be accessed by Oozie server
+            fs.copyFromLocalFile(new Path(externalChildIDs.toString()), new Path(actionDir,
+                    ACTION_EXTERNAL_CHILD_IDS_PROPS));
+        }
+    }
+
     private void setupMainConfiguration() throws IOException {
         FileSystem fs = FileSystem.get(getJobConf());
         fs.copyToLocalFile(new Path(getJobConf().get(OOZIE_ACTION_DIR_PATH), ACTION_CONF_XML), new Path(new File(
@@ -518,6 +610,8 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
         System.setProperty("oozie.action.id", getJobConf().get(OOZIE_ACTION_ID));
         System.setProperty("oozie.action.conf.xml", new File(ACTION_CONF_XML).getAbsolutePath());
         System.setProperty("oozie.action.output.properties", new File(ACTION_OUTPUT_PROPS).getAbsolutePath());
+        System.setProperty(EXTERNAL_ACTION_STATS, new File(ACTION_STATS_PROPS).getAbsolutePath());
+        System.setProperty(EXTERNAL_CHILD_IDS, new File(ACTION_EXTERNAL_CHILD_IDS_PROPS).getAbsolutePath());
         System.setProperty("oozie.action.newId.properties", new File(ACTION_NEW_ID_PROPS).getAbsolutePath());
     }
 
@@ -544,6 +638,7 @@ public class LauncherMapper<K1, V1, K2, V2> implements Mapper<K1, V1, K2, V2>, R
         this.reporter = reporter;
     }
 
+    @Override
     public void run() {
         System.out.println("Heart beat");
         reporter.progress();
diff --git core/src/main/java/org/apache/oozie/action/hadoop/MRStats.java core/src/main/java/org/apache/oozie/action/hadoop/MRStats.java
new file mode 100644
index 0000000..8006472
--- /dev/null
+++ core/src/main/java/org/apache/oozie/action/hadoop/MRStats.java
@@ -0,0 +1,57 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.oozie.action.hadoop;
+
+import org.apache.hadoop.mapred.Counters;
+import org.apache.hadoop.mapred.Counters.Counter;
+import org.json.simple.JSONObject;
+
+/**
+ * Class to collect statistics for Map-Reduce action.
+ */
+public class MRStats extends ActionStats {
+    public static final String ACTION_TYPE_LABEL = "ACTION_TYPE";
+    private Counters counters = null;
+
+    public MRStats(Counters groups) {
+        this.currentActionType = ActionType.MAP_REDUCE;
+        this.counters = groups;
+    }
+
+    @SuppressWarnings("unchecked")
+    @Override
+    public String toJSON() {
+        if (counters == null) {
+            return null;
+        }
+
+        JSONObject groups = new JSONObject();
+        groups.put(ACTION_TYPE_LABEL, getCurrentActionType().toString());
+        for (String gName : counters.getGroupNames()) {
+            JSONObject group = new JSONObject();
+            for (Counter counter : counters.getGroup(gName)) {
+                String cName = counter.getName();
+                Long cValue = counter.getValue();
+                group.put(cName, cValue);
+            }
+            groups.put(gName, group);
+        }
+        return groups.toJSONString();
+    }
+}
diff --git core/src/main/java/org/apache/oozie/action/hadoop/MapReduceActionExecutor.java core/src/main/java/org/apache/oozie/action/hadoop/MapReduceActionExecutor.java
index 7c5ee78..0aec5c1 100644
--- core/src/main/java/org/apache/oozie/action/hadoop/MapReduceActionExecutor.java
+++ core/src/main/java/org/apache/oozie/action/hadoop/MapReduceActionExecutor.java
@@ -17,7 +17,10 @@
  */
 package org.apache.oozie.action.hadoop;
 
+import java.io.IOException;
+import java.io.StringReader;
 import java.util.List;
+import java.util.Map;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
@@ -34,9 +37,11 @@ import org.apache.oozie.util.XmlUtils;
 import org.jdom.Element;
 import org.jdom.Namespace;
 import org.json.simple.JSONObject;
+import org.mortbay.util.ajax.JSON;
 
 public class MapReduceActionExecutor extends JavaActionExecutor {
 
+    public static final String OOZIE_ACTION_EXTERNAL_STATS_WRITE = "oozie.action.external.stats.write";
     public static final String HADOOP_COUNTERS = "hadoop.counters";
     private XLog log = XLog.getLog(getClass());
 
@@ -141,15 +146,26 @@ public class MapReduceActionExecutor extends JavaActionExecutor {
                     throw new ActionExecutorException(ActionExecutorException.ErrorType.FAILED, "MR001",
                                                       "ID swap should have happened in launcher job [{0}]", action.getExternalId());
                 }
+
                 Counters counters = runningJob.getCounters();
                 if (counters != null) {
-                    JSONObject json = counterstoJson(counters);
-                    context.setVar(HADOOP_COUNTERS, json.toJSONString());
+                    ActionStats stats = new MRStats(counters);
+                    String statsJsonString = stats.toJSON();
+                    context.setVar(HADOOP_COUNTERS, statsJsonString);
+
+                    // If action stats write property is set to false by user or
+                    // size of stats is greater than the maximum allowed size,
+                    // do not store the action stats
+                    if (Boolean.parseBoolean(evaluateConfigurationProperty(actionXml,
+                            OOZIE_ACTION_EXTERNAL_STATS_WRITE, "false"))
+                            && (statsJsonString.getBytes().length <= getMaxExternalStatsSize())) {
+                        context.setExecutionStats(statsJsonString);
+                        log.debug(
+                                "Printing stats for Map-Reduce action as a JSON string : [{0}]" + statsJsonString);
+                    }
                 }
                 else {
-
                     context.setVar(HADOOP_COUNTERS, "");
-
                     XLog.getLog(getClass()).warn("Could not find Hadoop Counters for: [{0}]", action.getExternalId());
                 }
             }
@@ -175,6 +191,23 @@ public class MapReduceActionExecutor extends JavaActionExecutor {
         }
     }
 
+    // Return the value of the specified configuration property
+    private String evaluateConfigurationProperty(Element actionConf, String key, String defaultValue) throws ActionExecutorException {
+        try {
+            if (actionConf != null) {
+                Namespace ns = actionConf.getNamespace();
+                Element e = actionConf.getChild("configuration", ns);
+                String strConf = XmlUtils.prettyPrint(e).toString();
+                XConfiguration inlineConf = new XConfiguration(new StringReader(strConf));
+                return inlineConf.get(key, defaultValue);
+            }
+            return "";
+        }
+        catch (IOException ex) {
+            throw convertException(ex);
+        }
+    }
+
     @SuppressWarnings("unchecked")
     private JSONObject counterstoJson(Counters counters) {
 
@@ -195,4 +228,17 @@ public class MapReduceActionExecutor extends JavaActionExecutor {
         return groups;
     }
 
+    /**
+     * Return the sharelib postfix for the action.
+     *
+     * @param context executor context.
+     * @param actionXml the action XML.
+     * @return the action sharelib post fix, this implementation returns <code>NULL</code>
+     * or <code>streaming</code> if the mapreduce action is streaming.
+     */
+    protected String getShareLibPostFix(Context context, Element actionXml) {
+        Namespace ns = actionXml.getNamespace();
+        return (actionXml.getChild("streaming", ns) != null) ? "mapreduce-streaming" : null;
+    }
+
 }
diff --git core/src/main/java/org/apache/oozie/action/hadoop/MapReduceMain.java core/src/main/java/org/apache/oozie/action/hadoop/MapReduceMain.java
index 2dea2f4..1d93b9e 100644
--- core/src/main/java/org/apache/oozie/action/hadoop/MapReduceMain.java
+++ core/src/main/java/org/apache/oozie/action/hadoop/MapReduceMain.java
@@ -132,6 +132,9 @@ public class MapReduceMain extends LauncherMain {
         String[] values = new String[conf.getInt(key + ".size", 0)];
         for (int i = 0; i < values.length; i++) {
             values[i] = conf.get(key + "." + i);
+            if (values[i] == null) {
+                values[i] = "";
+            }
         }
         return values;
     }
diff --git core/src/main/java/org/apache/oozie/action/hadoop/OoziePigStats.java core/src/main/java/org/apache/oozie/action/hadoop/OoziePigStats.java
new file mode 100644
index 0000000..454e6fe
--- /dev/null
+++ core/src/main/java/org/apache/oozie/action/hadoop/OoziePigStats.java
@@ -0,0 +1,160 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.action.hadoop;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.hadoop.mapred.Counters;
+import org.apache.hadoop.mapred.Counters.Counter;
+import org.apache.oozie.util.ParamChecker;
+import org.apache.pig.tools.pigstats.JobStats;
+import org.apache.pig.tools.pigstats.PigStats;
+import org.apache.pig.tools.pigstats.PigStatsUtil;
+import org.json.simple.JSONObject;
+
+/**
+ * Class to collect the Pig statistics for a Pig action
+ *
+ */
+public class OoziePigStats extends ActionStats {
+    PigStats pigStats = null;
+
+    public OoziePigStats(PigStats pigStats) {
+        this.currentActionType = ActionType.PIG;
+        this.pigStats = pigStats;
+    }
+
+    /**
+     * The PigStats API is used to collect the statistics and the result is returned as a JSON String.
+     *
+     * @return a JSON string
+     */
+    @SuppressWarnings("unchecked")
+    @Override
+    public String toJSON() {
+        JSONObject pigStatsGroup = new JSONObject();
+        pigStatsGroup.put("ACTION_TYPE", getCurrentActionType().toString());
+
+        // pig summary related counters
+        pigStatsGroup.put("BYTES_WRITTEN", Long.toString(pigStats.getBytesWritten()));
+        pigStatsGroup.put("DURATION", Long.toString(pigStats.getDuration()));
+        pigStatsGroup.put("ERROR_CODE", Long.toString(pigStats.getErrorCode()));
+        pigStatsGroup.put("ERROR_MESSAGE", pigStats.getErrorMessage());
+        pigStatsGroup.put("FEATURES", pigStats.getFeatures());
+        pigStatsGroup.put("HADOOP_VERSION", pigStats.getHadoopVersion());
+        pigStatsGroup.put("NUMBER_JOBS", Long.toString(pigStats.getNumberJobs()));
+        pigStatsGroup.put("PIG_VERSION", pigStats.getPigVersion());
+        pigStatsGroup.put("PROACTIVE_SPILL_COUNT_OBJECTS", Long.toString(pigStats.getProactiveSpillCountObjects()));
+        pigStatsGroup.put("PROACTIVE_SPILL_COUNT_RECORDS", Long.toString(pigStats.getProactiveSpillCountRecords()));
+        pigStatsGroup.put("RECORD_WRITTEN", Long.toString(pigStats.getRecordWritten()));
+        pigStatsGroup.put("RETURN_CODE", Long.toString(pigStats.getReturnCode()));
+        pigStatsGroup.put("SCRIPT_ID", pigStats.getScriptId());
+        pigStatsGroup.put("SMM_SPILL_COUNT", Long.toString(pigStats.getSMMSpillCount()));
+
+        PigStats.JobGraph jobGraph = pigStats.getJobGraph();
+        StringBuffer sb = new StringBuffer();
+        String separator = ",";
+
+        for (JobStats jobStats : jobGraph) {
+            // Get all the HadoopIds and put them as comma separated string for JOB_GRAPH
+            String hadoopId = jobStats.getJobId();
+            if (sb.length() > 0) {
+                sb.append(separator);
+            }
+            sb.append(hadoopId);
+            // Hadoop Counters for pig created MR job
+            pigStatsGroup.put(hadoopId, toJSONFromJobStats(jobStats));
+        }
+        pigStatsGroup.put("JOB_GRAPH", sb.toString());
+        return pigStatsGroup.toJSONString();
+    }
+
+    // MR job related counters
+    @SuppressWarnings("unchecked")
+    private static JSONObject toJSONFromJobStats(JobStats jobStats) {
+        JSONObject jobStatsGroup = new JSONObject();
+
+        // hadoop counters
+        jobStatsGroup.put(PigStatsUtil.HDFS_BYTES_WRITTEN, Long.toString(jobStats.getHdfsBytesWritten()));
+        jobStatsGroup.put(PigStatsUtil.MAP_INPUT_RECORDS, Long.toString(jobStats.getMapInputRecords()));
+        jobStatsGroup.put(PigStatsUtil.MAP_OUTPUT_RECORDS, Long.toString(jobStats.getMapOutputRecords()));
+        jobStatsGroup.put(PigStatsUtil.REDUCE_INPUT_RECORDS, Long.toString(jobStats.getReduceInputRecords()));
+        jobStatsGroup.put(PigStatsUtil.REDUCE_OUTPUT_RECORDS, Long.toString(jobStats.getReduceOutputRecords()));
+        // currently returns null; pig bug
+        jobStatsGroup.put("HADOOP_COUNTERS", toJSONFromCounters(jobStats.getHadoopCounters()));
+
+        // pig generated hadoop counters and other stats
+        jobStatsGroup.put("Alias", jobStats.getAlias());
+        jobStatsGroup.put("AVG_MAP_TIME", Long.toString(jobStats.getAvgMapTime()));
+        jobStatsGroup.put("AVG_REDUCE_TIME", Long.toString(jobStats.getAvgREduceTime()));
+        jobStatsGroup.put("BYTES_WRITTEN", Long.toString(jobStats.getBytesWritten()));
+        jobStatsGroup.put("ERROR_MESSAGE", jobStats.getErrorMessage());
+        jobStatsGroup.put("FEATURE", jobStats.getFeature());
+        jobStatsGroup.put("JOB_ID", jobStats.getJobId());
+        jobStatsGroup.put("MAX_MAP_TIME", Long.toString(jobStats.getMaxMapTime()));
+        jobStatsGroup.put("MIN_MAP_TIME", Long.toString(jobStats.getMinMapTime()));
+        jobStatsGroup.put("MAX_REDUCE_TIME", Long.toString(jobStats.getMaxReduceTime()));
+        jobStatsGroup.put("MIN_REDUCE_TIME", Long.toString(jobStats.getMinReduceTime()));
+        jobStatsGroup.put("NUMBER_MAPS", Long.toString(jobStats.getNumberMaps()));
+        jobStatsGroup.put("NUMBER_REDUCES", Long.toString(jobStats.getNumberReduces()));
+        jobStatsGroup.put("PROACTIVE_SPILL_COUNT_OBJECTS", Long.toString(jobStats.getProactiveSpillCountObjects()));
+        jobStatsGroup.put("PROACTIVE_SPILL_COUNT_RECS", Long.toString(jobStats.getProactiveSpillCountRecs()));
+        jobStatsGroup.put("RECORD_WRITTEN", Long.toString(jobStats.getRecordWrittern()));
+        jobStatsGroup.put("SMMS_SPILL_COUNT", Long.toString(jobStats.getSMMSpillCount()));
+        jobStatsGroup.put("MULTI_STORE_COUNTERS", toJSONFromMultiStoreCounters(jobStats.getMultiStoreCounters()));
+
+        return jobStatsGroup;
+
+    }
+
+    // multistorecounters to JSON
+    @SuppressWarnings("unchecked")
+    private static JSONObject toJSONFromMultiStoreCounters(Map<String, Long> map) {
+        JSONObject group = new JSONObject();
+        for (String cName : map.keySet()) {
+            group.put(cName, map.get(cName));
+        }
+        return group;
+
+    }
+
+    // hadoop counters to JSON
+    @SuppressWarnings("unchecked")
+    private static JSONObject toJSONFromCounters(Counters counters) {
+        if (counters == null) {
+            return null;
+        }
+
+        JSONObject groups = new JSONObject();
+        for (String gName : counters.getGroupNames()) {
+            JSONObject group = new JSONObject();
+            for (Counter counter : counters.getGroup(gName)) {
+                String cName = counter.getName();
+                Long cValue = counter.getValue();
+                group.put(cName, Long.toString(cValue));
+            }
+            groups.put(gName, group);
+        }
+        return groups;
+
+    }
+
+}
diff --git core/src/main/java/org/apache/oozie/action/hadoop/PigActionExecutor.java core/src/main/java/org/apache/oozie/action/hadoop/PigActionExecutor.java
index bb8dcb9..4c23770 100644
--- core/src/main/java/org/apache/oozie/action/hadoop/PigActionExecutor.java
+++ core/src/main/java/org/apache/oozie/action/hadoop/PigActionExecutor.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -21,9 +21,13 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.RunningJob;
 import org.apache.oozie.action.ActionExecutorException;
 import org.apache.oozie.client.XOozieClient;
 import org.apache.oozie.client.WorkflowAction;
+import org.apache.oozie.service.HadoopAccessorException;
+import org.apache.oozie.util.ClassUtils;
+import org.apache.oozie.util.IOUtils;
 import org.apache.oozie.util.XLog;
 import org.apache.oozie.util.XmlUtils;
 import org.jdom.Element;
@@ -31,7 +35,11 @@ import org.jdom.Namespace;
 import org.jdom.JDOMException;
 import org.mortbay.log.Log;
 
+import java.io.BufferedReader;
 import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.net.URISyntaxException;
 import java.util.List;
 
 public class PigActionExecutor extends JavaActionExecutor {
@@ -40,18 +48,23 @@ public class PigActionExecutor extends JavaActionExecutor {
         super("pig");
     }
 
+    @Override
     protected List<Class> getLauncherClasses() {
         List<Class> classes = super.getLauncherClasses();
         classes.add(LauncherMain.class);
         classes.add(MapReduceMain.class);
         classes.add(PigMain.class);
+        classes.add(OoziePigStats.class);
         return classes;
     }
 
+
+    @Override
     protected String getLauncherMain(Configuration launcherConf, Element actionXml) {
         return launcherConf.get(LauncherMapper.CONF_OOZIE_ACTION_MAIN_CLASS, PigMain.class.getName());
     }
 
+    @Override
     void injectActionCallback(Context context, Configuration launcherConf) {
     }
 
@@ -99,6 +112,7 @@ public class PigActionExecutor extends JavaActionExecutor {
         return conf;
     }
 
+    @Override
     @SuppressWarnings("unchecked")
     Configuration setupActionConf(Configuration actionConf, Context context, Element actionXml, Path appPath)
             throws ActionExecutorException {
@@ -125,8 +139,77 @@ public class PigActionExecutor extends JavaActionExecutor {
         return actionConf;
     }
 
+    @Override
     protected boolean getCaptureOutput(WorkflowAction action) throws JDOMException {
-        return true;
+        return false;
+    }
+
+    /**
+     * Get the stats and external child IDs for a pig job
+     *
+     * @param actionFs the FileSystem object
+     * @param runningJob the runningJob
+     * @param action the Workflow action
+     * @param context executor context
+     *
+     */
+    @Override
+    protected void getActionData(FileSystem actionFs, RunningJob runningJob, WorkflowAction action, Context context) throws HadoopAccessorException, JDOMException, IOException, URISyntaxException{
+        super.getActionData(actionFs, runningJob, action, context);
+        String stats = getStats(context, actionFs);
+        context.setExecutionStats(stats);
+        String externalChildIDs = getExternalChildIDs(context, actionFs);
+        context.setExternalChildIDs(externalChildIDs);
+    }
+
+    private String getStats(Context context, FileSystem actionFs) throws IOException, HadoopAccessorException,
+            URISyntaxException {
+        Path actionOutput = LauncherMapper.getActionStatsDataPath(context.getActionDir());
+        String stats = null;
+        if (actionFs.exists(actionOutput)) {
+            stats = getDataFromPath(actionOutput, actionFs);
+
+        }
+        return stats;
+    }
+
+    private String getExternalChildIDs(Context context, FileSystem actionFs) throws IOException,
+            HadoopAccessorException, URISyntaxException {
+        Path actionOutput = LauncherMapper.getExternalChildIDsDataPath(context.getActionDir());
+        String externalIDs = null;
+        if (actionFs.exists(actionOutput)) {
+            externalIDs = getDataFromPath(actionOutput, actionFs);
+        }
+        return externalIDs;
+    }
+
+    private static String getDataFromPath(Path actionOutput, FileSystem actionFs) throws IOException{
+        BufferedReader reader = null;
+        String data = null;
+        try {
+            InputStream is = actionFs.open(actionOutput);
+            reader = new BufferedReader(new InputStreamReader(is));
+            data = IOUtils.getReaderAsString(reader, -1);
+
+        }
+        finally {
+            if (reader != null) {
+                reader.close();
+            }
+        }
+        return data;
+    }
+
+    /**
+     * Return the sharelib postfix for the action.
+     *
+     * @param context executor context.
+     * @param actionXml the action XML.
+     * @return the action sharelib post fix, this implementation returns <code>pig</code>.
+     */
+    @Override
+    protected String getShareLibPostFix(Context context, Element actionXml) {
+        return "pig";
     }
 
 }
diff --git core/src/main/java/org/apache/oozie/action/hadoop/PigMain.java core/src/main/java/org/apache/oozie/action/hadoop/PigMain.java
index 37a870d..e0e8418 100644
--- core/src/main/java/org/apache/oozie/action/hadoop/PigMain.java
+++ core/src/main/java/org/apache/oozie/action/hadoop/PigMain.java
@@ -19,11 +19,14 @@ package org.apache.oozie.action.hadoop;
 
 import org.apache.pig.Main;
 import org.apache.pig.PigRunner;
+import org.apache.pig.tools.pigstats.JobStats;
 import org.apache.pig.tools.pigstats.PigStats;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 
+import java.io.BufferedWriter;
 import java.io.FileNotFoundException;
+import java.io.FileWriter;
 import java.io.OutputStream;
 import java.io.FileOutputStream;
 import java.io.BufferedReader;
@@ -38,9 +41,19 @@ import java.util.ArrayList;
 import java.util.Properties;
 import java.util.Set;
 import java.net.URL;
+import java.util.regex.Pattern;
 
 public class PigMain extends LauncherMain {
     private static final Set<String> DISALLOWED_PIG_OPTIONS = new HashSet<String>();
+    public static final String ACTION_PREFIX = "oozie.action.";
+    public static final String EXTERNAL_CHILD_IDS = ACTION_PREFIX + "externalChildIDs.properties";
+    public static final String EXTERNAL_ACTION_STATS = ACTION_PREFIX + "stats.properties";
+    public static final String EXTERNAL_STATS_WRITE = ACTION_PREFIX + "external.stats.write";
+    public static final int STRING_BUFFER_SIZE = 100;
+
+    private static final Pattern[] PIG_JOB_IDS_PATTERNS = {
+      Pattern.compile("HadoopJobId: (job_\\S*)")
+    };
 
     static {
         DISALLOWED_PIG_OPTIONS.add("-4");
@@ -202,26 +215,29 @@ public class PigMain extends LauncherMain {
         System.out.flush();
 
         System.out.println();
-        runPigJob(new String[] { "-version" }, null, true);
+        runPigJob(new String[] { "-version" }, null, true, false);
         System.out.println();
         System.out.flush();
-
-        runPigJob(arguments.toArray(new String[arguments.size()]), pigLog, false);
+        boolean hasStats = Boolean.parseBoolean(actionConf.get(EXTERNAL_STATS_WRITE));
+        runPigJob(arguments.toArray(new String[arguments.size()]), pigLog, false, hasStats);
 
         System.out.println();
         System.out.println("<<< Invocation of Pig command completed <<<");
         System.out.println();
 
-        // harvesting and recording Hadoop Job IDs
-        Properties jobIds = getHadoopJobIds(logFile);
-        File file = new File(System.getProperty("oozie.action.output.properties"));
-        os = new FileOutputStream(file);
-        jobIds.store(os, "");
-        os.close();
-        System.out.println(" Hadoop Job IDs executed by Pig: " + jobIds.getProperty("hadoopJobs"));
-        System.out.println();
+        // For embedded python or for version of pig lower than 0.8, pig stats are not supported.
+        // So retrieving hadoop Ids here
+        File file = new File(System.getProperty(EXTERNAL_CHILD_IDS));
+        if (!file.exists()) {
+            Properties props = getHadoopJobIds(logFile, PIG_JOB_IDS_PATTERNS);
+            writeExternalData(props.getProperty(HADOOP_JOBS), file);
+            System.out.println(" Hadoop Job IDs executed by Pig: " + props.getProperty(HADOOP_JOBS));
+            System.out.println();
+        }
     }
 
+
+
     private void handleError(String pigLog) throws Exception {
         System.err.println();
         System.err.println("Pig logfile dump:");
@@ -247,9 +263,10 @@ public class PigMain extends LauncherMain {
      * @param args pig command line arguments
      * @param pigLog pig log file
      * @param resetSecurityManager specify if need to reset security manager
+     * @param retrieveStats specify if stats are to be retrieved
      * @throws Exception
      */
-    protected void runPigJob(String[] args, String pigLog, boolean resetSecurityManager) throws Exception {
+    protected void runPigJob(String[] args, String pigLog, boolean resetSecurityManager, boolean retrieveStats) throws Exception {
         // running as from the command line
         boolean pigRunnerExists = true;
         Class klass;
@@ -271,6 +288,34 @@ public class PigMain extends LauncherMain {
                 }
                 throw new LauncherMainException(PigRunner.ReturnCode.FAILURE);
             }
+            else {
+                // If pig command is ran with just the "version" option, then
+                // return
+                if (resetSecurityManager) {
+                    return;
+                }
+                String jobIds = getHadoopJobIds(stats);
+                if (jobIds != null) {
+                    System.out.println(" Hadoop Job IDs executed by Pig: " + jobIds);
+                    File f = new File(System.getProperty(EXTERNAL_CHILD_IDS));
+                    writeExternalData(jobIds, f);
+                }
+                // Retrieve stats only if user has specified in workflow
+                // configuration
+                if (retrieveStats) {
+                    ActionStats pigStats;
+                    String JSONString;
+                    try {
+                        pigStats = new OoziePigStats(stats);
+                        JSONString = pigStats.toJSON();
+                    } catch (UnsupportedOperationException uoe) {
+                        throw new UnsupportedOperationException(
+                                "Pig stats are not supported for this type of operation", uoe);
+                    }
+                    File f = new File(System.getProperty(EXTERNAL_ACTION_STATS));
+                    writeExternalData(JSONString, f);
+                }
+            }
         }
         else {
             try {
@@ -295,43 +340,52 @@ public class PigMain extends LauncherMain {
         }
     }
 
+    // write external data(stats, hadoopIds) to the file which will be read by the LauncherMapper
+    private static void writeExternalData(String data, File f) throws IOException {
+        BufferedWriter out = null;
+        try {
+            out = new BufferedWriter(new FileWriter(f));
+            out.write(data);
+        }
+        finally {
+            if (out != null) {
+                out.close();
+            }
+        }
+    }
+
     public static void setPigScript(Configuration conf, String script, String[] params, String[] args) {
         conf.set("oozie.pig.script", script);
         MapReduceMain.setStrings(conf, "oozie.pig.params", params);
         MapReduceMain.setStrings(conf, "oozie.pig.args", args);
     }
 
-    private static final String JOB_ID_LOG_PREFIX = "HadoopJobId: ";
-
-    protected Properties getHadoopJobIds(String logFile) throws IOException {
-        int jobCount = 0;
-        Properties props = new Properties();
-        StringBuffer sb = new StringBuffer(100);
-        if (new File(logFile).exists() == false) {
-            System.err.println("pig log file: " + logFile + "  not present. Therefore no Hadoop jobids found");
-            props.setProperty("hadoopJobs", "");
-        }
-        else {
-            BufferedReader br = new BufferedReader(new FileReader(logFile));
-            String line = br.readLine();
-            String separator = "";
-            while (line != null) {
-                if (line.contains(JOB_ID_LOG_PREFIX)) {
-                    int jobIdStarts = line.indexOf(JOB_ID_LOG_PREFIX) + JOB_ID_LOG_PREFIX.length();
-                    String jobId = line.substring(jobIdStarts);
-                    int jobIdEnds = jobId.indexOf(" ");
-                    if (jobIdEnds > -1) {
-                        jobId = jobId.substring(0, jobId.indexOf(" "));
-                    }
-                    sb.append(separator).append(jobId);
-                    separator = ",";
+    /**
+     * Get Hadoop Ids through PigStats API
+     *
+     * @param pigStats stats object obtained through PigStats API
+     * @return comma-separated String
+     */
+    protected String getHadoopJobIds(PigStats pigStats) {
+        StringBuilder sb = new StringBuilder(STRING_BUFFER_SIZE);
+        String separator = ",";
+        // Collect Hadoop Ids through JobGraph API of Pig and store them as
+        // comma separated string
+        try {
+            PigStats.JobGraph jobGraph = pigStats.getJobGraph();
+            for (JobStats jobStats : jobGraph) {
+                String hadoopJobId = jobStats.getJobId();
+                if (sb.length() > 0) {
+                    sb.append(separator);
                 }
-                line = br.readLine();
+                sb.append(hadoopJobId);
             }
-            br.close();
-            props.setProperty("hadoopJobs", sb.toString());
         }
-        return props;
+        // Return null if Pig API's are not supported
+        catch (UnsupportedOperationException uoe) {
+            return null;
+        }
+        return sb.toString();
     }
 
 }
diff --git core/src/main/java/org/apache/oozie/action/hadoop/PigMainWithOldAPI.java core/src/main/java/org/apache/oozie/action/hadoop/PigMainWithOldAPI.java
index 70c7057..e03578c 100644
--- core/src/main/java/org/apache/oozie/action/hadoop/PigMainWithOldAPI.java
+++ core/src/main/java/org/apache/oozie/action/hadoop/PigMainWithOldAPI.java
@@ -249,7 +249,7 @@ public class PigMainWithOldAPI extends LauncherMain {
         os = new FileOutputStream(file);
         jobIds.store(os, "");
         os.close();
-        System.out.println(" Hadoop Job IDs executed by Pig: " + jobIds.getProperty("hadoopJobs"));
+        System.out.println(" Hadoop Job IDs executed by Pig: " + jobIds.getProperty(HADOOP_JOBS));
         System.out.println();
     }
 
@@ -272,7 +272,7 @@ public class PigMainWithOldAPI extends LauncherMain {
         StringBuffer sb = new StringBuffer(100);
         if (new File(logFile).exists() == false) {
             System.err.println("pig log file: " + logFile + "  not present. Therefore no Hadoop jobids found");
-            props.setProperty("hadoopJobs", "");
+            props.setProperty(HADOOP_JOBS, "");
         }
         else {
             BufferedReader br = new BufferedReader(new FileReader(logFile));
@@ -292,7 +292,7 @@ public class PigMainWithOldAPI extends LauncherMain {
                 line = br.readLine();
             }
             br.close();
-            props.setProperty("hadoopJobs", sb.toString());
+            props.setProperty(HADOOP_JOBS, sb.toString());
         }
         return props;
     }
diff --git core/src/main/java/org/apache/oozie/action/hadoop/ShellActionExecutor.java core/src/main/java/org/apache/oozie/action/hadoop/ShellActionExecutor.java
new file mode 100644
index 0000000..bb159c6
--- /dev/null
+++ core/src/main/java/org/apache/oozie/action/hadoop/ShellActionExecutor.java
@@ -0,0 +1,163 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.oozie.action.hadoop;
+
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.oozie.action.ActionExecutorException;
+import org.jdom.Element;
+import org.jdom.Namespace;
+
+public class ShellActionExecutor extends JavaActionExecutor {
+
+    /**
+     * Config property name to set the child environment
+     */
+    public String OOZIE_LAUNCHER_CHILD_ENV = "mapred.child.env";
+
+    public ShellActionExecutor() {
+        super("shell");
+    }
+
+    @Override
+    protected List<Class> getLauncherClasses() {
+        List<Class> classes = super.getLauncherClasses();
+        // Base class of ShellMain dedicated for 'shell' action
+        classes.add(LauncherMain.class);
+        // Some utility methods used in ShelltMain
+        classes.add(MapReduceMain.class);
+        // Specific to Shell action
+        classes.add(ShellMain.class);
+        // ShellMain's inner class
+        classes.add(ShellMain.OutputWriteThread.class);
+        return classes;
+    }
+
+    @Override
+    protected String getLauncherMain(Configuration launcherConf, Element actionXml) {
+        return launcherConf.get(LauncherMapper.CONF_OOZIE_ACTION_MAIN_CLASS, ShellMain.class.getName());
+    }
+
+    @SuppressWarnings("unchecked")
+    @Override
+    Configuration setupActionConf(Configuration actionConf, Context context, Element actionXml, Path appPath)
+            throws ActionExecutorException {
+        super.setupActionConf(actionConf, context, actionXml, appPath);
+        Namespace ns = actionXml.getNamespace();
+
+        String exec = actionXml.getChild("exec", ns).getTextTrim();
+        String execName = new Path(exec).getName();
+        actionConf.set(ShellMain.CONF_OOZIE_SHELL_EXEC, execName);
+
+        // Setting Shell command's arguments
+        setListInConf("argument", actionXml, actionConf, ShellMain.CONF_OOZIE_SHELL_ARGS, false);
+        // Setting Shell command's environment variable key=value
+        setListInConf("env-var", actionXml, actionConf, ShellMain.CONF_OOZIE_SHELL_ENVS, true);
+
+        // Setting capture output flag
+        actionConf.setBoolean(ShellMain.CONF_OOZIE_SHELL_CAPTURE_OUTPUT,
+                actionXml.getChild("capture-output", ns) != null);
+
+        return actionConf;
+    }
+
+    /**
+     * This method read a list of tag from an XML element and set the
+     * Configuration accordingly
+     *
+     * @param tag
+     * @param actionXml
+     * @param actionConf
+     * @param key
+     * @param checkKeyValue
+     * @throws ActionExecutorException
+     */
+    protected void setListInConf(String tag, Element actionXml, Configuration actionConf, String key,
+            boolean checkKeyValue) throws ActionExecutorException {
+        String[] strTagValue = null;
+        Namespace ns = actionXml.getNamespace();
+        List<Element> eTags = actionXml.getChildren(tag, ns);
+        if (eTags != null && eTags.size() > 0) {
+            strTagValue = new String[eTags.size()];
+            for (int i = 0; i < eTags.size(); i++) {
+                strTagValue[i] = eTags.get(i).getTextTrim();
+                if (checkKeyValue) {
+                    checkPair(strTagValue[i]);
+                }
+            }
+        }
+        MapReduceMain.setStrings(actionConf, key, strTagValue);
+    }
+
+    /**
+     * Check if the key=value pair is appropriately formatted
+     * @param pair
+     * @throws ActionExecutorException
+     */
+    private void checkPair(String pair) throws ActionExecutorException {
+        String[] varValue = pair.split("=");
+        if (varValue == null || varValue.length <= 1) {
+            throw new ActionExecutorException(ActionExecutorException.ErrorType.FAILED, "JA010",
+                    "Wrong ENV format [{0}] in <env-var> , key=value format expected " + pair);
+        }
+    }
+
+    @Override
+    protected Configuration setupLauncherConf(Configuration conf, Element actionXml, Path appPath, Context context)
+            throws ActionExecutorException {
+        super.setupLauncherConf(conf, actionXml, appPath, context);
+        conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", true);
+        addDefaultChildEnv(conf);
+        return conf;
+    }
+
+    /**
+     * This method sets the PATH to current working directory for the launched
+     * map task from where shell command will run.
+     *
+     * @param conf
+     */
+    protected void addDefaultChildEnv(Configuration conf) {
+        String envValues = "PATH=.:$PATH";
+        updateProperty(conf, OOZIE_LAUNCHER_CHILD_ENV, envValues);
+    }
+
+    /**
+     * Utility method to append the new value to any property.
+     *
+     * @param conf
+     * @param propertyName
+     * @param appendValue
+     */
+    private void updateProperty(Configuration conf, String propertyName, String appendValue) {
+        if (conf != null) {
+            String val = conf.get(propertyName, "");
+            if (val.length() > 0) {
+                val += ",";
+            }
+            val += appendValue;
+            conf.set(propertyName, val);
+            log.debug("action conf is updated with default value for property " + propertyName + ", old value :"
+                    + conf.get(propertyName, "") + ", new value :" + val);
+        }
+    }
+
+}
diff --git core/src/main/java/org/apache/oozie/action/hadoop/ShellMain.java core/src/main/java/org/apache/oozie/action/hadoop/ShellMain.java
new file mode 100644
index 0000000..14be07f
--- /dev/null
+++ core/src/main/java/org/apache/oozie/action/hadoop/ShellMain.java
@@ -0,0 +1,316 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.oozie.action.hadoop;
+
+import java.io.BufferedReader;
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+
+public class ShellMain extends LauncherMain {
+    public static final String CONF_OOZIE_SHELL_ARGS = "oozie.shell.args";
+    public static final String CONF_OOZIE_SHELL_EXEC = "oozie.shell.exec";
+    public static final String CONF_OOZIE_SHELL_ENVS = "oozie.shell.envs";
+    public static final String CONF_OOZIE_SHELL_CAPTURE_OUTPUT = "oozie.shell.capture-output";
+    public static final String OOZIE_ACTION_CONF_XML = "OOZIE_ACTION_CONF_XML";
+
+    /**
+     * @param args Invoked from LauncherMapper:map()
+     * @throws Exception
+     */
+    public static void main(String[] args) throws Exception {
+        run(ShellMain.class, args);
+    }
+
+    @Override
+    protected void run(String[] args) throws Exception {
+
+        Configuration actionConf = loadActionConf();
+
+        int exitCode = execute(actionConf);
+        if (exitCode != 0) {
+            // Shell command failed. therefore make the action failed
+            throw new LauncherMainException(1);
+        }
+
+    }
+
+    /**
+     * Execute the shell command
+     *
+     * @param actionConf
+     * @return command exit value
+     * @throws IOException
+     */
+    private int execute(Configuration actionConf) throws Exception {
+        String exec = getExec(actionConf);
+        List<String> args = getShellArguments(actionConf);
+        ArrayList<String> cmdArray = getCmdList(exec, args.toArray(new String[args.size()]));
+        ProcessBuilder builder = new ProcessBuilder(cmdArray);
+        Map<String, String> envp = getEnvMap(builder.environment(), actionConf);
+
+        // Getting the Ccurrent working dir and setting it to processbuilder
+        File currDir = new File("dummy").getAbsoluteFile().getParentFile();
+        System.out.println("Current working dir " + currDir);
+        builder.directory(currDir);
+
+        printCommand(cmdArray, envp); // For debugging purpose
+
+        System.out.println("=================================================================");
+        System.out.println();
+        System.out.println(">>> Invoking Shell command line now >>");
+        System.out.println();
+        System.out.flush();
+
+        boolean captureOutput = actionConf.getBoolean(CONF_OOZIE_SHELL_CAPTURE_OUTPUT, false);
+
+        // Execute the Command
+        Process p = builder.start();
+
+        Thread[] thrArray = handleShellOutput(p, captureOutput);
+
+        int exitValue = p.waitFor();
+        // Wait for both the thread to exit
+        if (thrArray != null) {
+            for (Thread thr : thrArray) {
+                thr.join();
+            }
+        }
+        System.out.println("Exit code of the Shell command " + exitValue);
+        System.out.println("<<< Invocation of Shell command completed <<<");
+        System.out.println();
+        return exitValue;
+    }
+
+    /**
+     * Return the environment variable to pass to in shell command execution.
+     *
+     */
+    private Map<String, String> getEnvMap(Map<String, String> envp, Configuration actionConf) {
+        // Adding user-specified environments
+        String[] envs = MapReduceMain.getStrings(actionConf, CONF_OOZIE_SHELL_ENVS);
+        for (String env : envs) {
+            String[] varValue = env.split("="); // Error case is handled in
+                                                // ShellActionExecutor
+            envp.put(varValue[0], varValue[1]);
+        }
+        // Adding action.xml to env
+        envp.put(OOZIE_ACTION_CONF_XML, System.getProperty("oozie.action.conf.xml", ""));
+        return envp;
+    }
+
+    /**
+     * Get the shell commands with the arguments
+     *
+     * @param exec
+     * @param args
+     * @return command and list of args
+     */
+    private ArrayList<String> getCmdList(String exec, String[] args) {
+        ArrayList<String> cmdArray = new ArrayList<String>();
+        cmdArray.add(exec); // Main executable
+        for (String arg : args) { // Adding rest of the arguments
+            cmdArray.add(arg);
+        }
+        return cmdArray;
+    }
+
+    /**
+     * Print the output written by the Shell execution in its stdout/stderr.
+     * Also write the stdout output to a file for capturing.
+     *
+     * @param p process
+     * @param captureOutput indicates if STDOUT should be captured or not.
+     * @return Array of threads (one for stdout and another one for stderr
+     *         processing
+     * @throws IOException thrown if an IO error occurrs.
+     */
+    protected Thread[] handleShellOutput(Process p, boolean captureOutput)
+            throws IOException {
+        BufferedReader input = new BufferedReader(new InputStreamReader(p.getInputStream()));
+        BufferedReader error = new BufferedReader(new InputStreamReader(p.getErrorStream()));
+
+        OutputWriteThread thrStdout = new OutputWriteThread(input, true, captureOutput);
+        thrStdout.setDaemon(true);
+        thrStdout.start();
+
+        OutputWriteThread thrStderr = new OutputWriteThread(error, false, false);
+        thrStderr.setDaemon(true);
+        thrStderr.start();
+
+        return new Thread[]{ thrStdout, thrStderr };
+    }
+
+    /**
+     * Thread to write output to LM stdout/stderr. Also write the content for
+     * capture-output.
+     */
+    class OutputWriteThread extends Thread {
+        BufferedReader reader = null;
+        boolean isStdout = false;
+        boolean needCaptured = false;
+
+        public OutputWriteThread(BufferedReader reader, boolean isStdout, boolean needCaptured) {
+            this.reader = reader;
+            this.isStdout = isStdout;
+            this.needCaptured = needCaptured;
+        }
+
+        @Override
+        public void run() {
+            String line;
+            BufferedWriter os = null;
+
+            try {
+                if (needCaptured) {
+                    File file = new File(System.getProperty("oozie.action.output.properties"));
+                    os = new BufferedWriter(new FileWriter(file));
+                }
+                while ((line = reader.readLine()) != null) {
+                    if (isStdout) { // For stdout
+                        // 1. Writing to LM STDOUT
+                        System.out.println("Stdoutput " + line);
+                        // 2. Writing for capture output
+                        if (os != null) {
+                            os.write(line);
+                            os.newLine();
+                        }
+                    }
+                    else {
+                        System.err.println(line); // 1. Writing to LM STDERR
+                    }
+                }
+            }
+            catch (IOException e) {
+                e.printStackTrace();
+                throw new RuntimeException("Stdout/Stderr read/write error :" + e);
+            }finally {
+                try {
+                    reader.close();
+                }
+                catch (IOException ex) {
+                    //NOP ignoring error on close of STDOUT/STDERR
+                }
+                if (os != null) {
+                    try {
+                        os.close();
+                    }
+                    catch (IOException e) {
+                        e.printStackTrace();
+                        throw new RuntimeException("Unable to close the file stream :" + e);
+                    }
+                }
+            }
+        }
+    }
+
+    /**
+     * Print the command including the arguments as well as the environment
+     * setup
+     *
+     * @param cmdArray :Command Array
+     * @param envp :Environment array
+     */
+    protected void printCommand(ArrayList<String> cmdArray, Map<String, String> envp) {
+        int i = 0;
+        System.out.println("Full Command .. ");
+        System.out.println("-------------------------");
+        for (String arg : cmdArray) {
+            System.out.println(i++ + ":" + arg + ":");
+        }
+
+        if (envp != null) {
+            System.out.println("List of passing environment");
+            System.out.println("-------------------------");
+            for (Map.Entry<String, String> entry : envp.entrySet()) {
+                System.out.println(entry.getKey() + "=" + entry.getValue() + ":");
+            }
+        }
+
+    }
+
+    /**
+     * Retrieve the list of arguments that were originally specified to
+     * Workflow.xml.
+     *
+     * @param actionConf
+     * @return argument list
+     */
+    protected List<String> getShellArguments(Configuration actionConf) {
+        List<String> arguments = new ArrayList<String>();
+        String[] scrArgs = MapReduceMain.getStrings(actionConf, CONF_OOZIE_SHELL_ARGS);
+        for (String scrArg : scrArgs) {
+            arguments.add(scrArg);
+        }
+        return arguments;
+    }
+
+    /**
+     * Retrieve the executable name that was originally specified to
+     * Workflow.xml.
+     *
+     * @param actionConf
+     * @return executable
+     */
+    protected String getExec(Configuration actionConf) {
+        String exec = actionConf.get(CONF_OOZIE_SHELL_EXEC);
+
+        if (exec == null) {
+            throw new RuntimeException("Action Configuration does not have " + CONF_OOZIE_SHELL_EXEC + " property");
+        }
+        return exec;
+    }
+
+    /**
+     * Read action configuration passes through action xml file.
+     *
+     * @return action  Configuration
+     * @throws IOException
+     */
+    protected Configuration loadActionConf() throws IOException {
+        System.out.println();
+        System.out.println("Oozie Shell action configuration");
+        System.out.println("=================================================================");
+
+        // loading action conf prepared by Oozie
+        Configuration actionConf = new Configuration(false);
+
+        String actionXml = System.getProperty("oozie.action.conf.xml");
+
+        if (actionXml == null) {
+            throw new RuntimeException("Missing Java System Property [oozie.action.conf.xml]");
+        }
+        if (!new File(actionXml).exists()) {
+            throw new RuntimeException("Action Configuration XML file [" + actionXml + "] does not exist");
+        }
+
+        actionConf.addResource(new Path("file:///", actionXml));
+        logMasking("Shell configuration:", new HashSet<String>(), actionConf);
+        return actionConf;
+    }
+}
diff --git core/src/main/java/org/apache/oozie/action/hadoop/SqoopActionExecutor.java core/src/main/java/org/apache/oozie/action/hadoop/SqoopActionExecutor.java
new file mode 100644
index 0000000..fc4718a
--- /dev/null
+++ core/src/main/java/org/apache/oozie/action/hadoop/SqoopActionExecutor.java
@@ -0,0 +1,127 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.action.hadoop;
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.StringTokenizer;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.oozie.action.ActionExecutorException;
+import org.apache.oozie.client.WorkflowAction;
+import org.apache.oozie.util.XConfiguration;
+import org.apache.oozie.util.XmlUtils;
+import org.jdom.Element;
+import org.jdom.JDOMException;
+import org.jdom.Namespace;
+
+public class SqoopActionExecutor extends JavaActionExecutor {
+
+
+    public SqoopActionExecutor() {
+        super("sqoop");
+    }
+
+    @Override
+    protected List<Class> getLauncherClasses() {
+        List<Class> classes = super.getLauncherClasses();
+        classes.add(LauncherMain.class);
+        classes.add(MapReduceMain.class);
+        classes.add(HiveMain.class);
+        classes.add(SqoopMain.class);
+        return classes;
+    }
+
+    @Override
+    protected String getLauncherMain(Configuration launcherConf, Element actionXml) {
+        return launcherConf.get(LauncherMapper.CONF_OOZIE_ACTION_MAIN_CLASS, SqoopMain.class.getName());
+    }
+
+    @Override
+    Configuration setupLauncherConf(Configuration conf, Element actionXml, Path appPath, Context context)
+            throws ActionExecutorException {
+        super.setupLauncherConf(conf, actionXml, appPath, context);
+
+        HiveActionExecutor hiveAE = new HiveActionExecutor();
+        hiveAE.setupHiveDefault(conf, appPath, actionXml);
+
+        return conf;
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    Configuration setupActionConf(Configuration actionConf, Context context, Element actionXml, Path appPath)
+            throws ActionExecutorException {
+        super.setupActionConf(actionConf, context, actionXml, appPath);
+        Namespace ns = actionXml.getNamespace();
+
+        try {
+            Element e = actionXml.getChild("configuration", ns);
+            if (e != null) {
+                String strConf = XmlUtils.prettyPrint(e).toString();
+                XConfiguration inlineConf = new XConfiguration(new StringReader(strConf));
+                checkForDisallowedProps(inlineConf, "inline configuration");
+                XConfiguration.copy(inlineConf, actionConf);
+            }
+        } catch (IOException ex) {
+            throw convertException(ex);
+        }
+
+        String[] args;
+        if (actionXml.getChild("command", ns) != null) {
+            String command = actionXml.getChild("command", ns).getTextTrim();
+            StringTokenizer st = new StringTokenizer(command, " ");
+            List<String> l = new ArrayList<String>();
+            while (st.hasMoreTokens()) {
+                l.add(st.nextToken());
+            }
+            args = l.toArray(new String[l.size()]);
+        }
+        else {
+            List<Element> eArgs = (List<Element>) actionXml.getChildren("arg", ns);
+            args = new String[eArgs.size()];
+            for (int i = 0; i < eArgs.size(); i++) {
+                args[i] = eArgs.get(i).getTextTrim();
+            }
+        }
+
+        SqoopMain.setSqoopCommand(actionConf, args);
+        return actionConf;
+    }
+
+    @Override
+    protected boolean getCaptureOutput(WorkflowAction action) throws JDOMException {
+        return true;
+    }
+
+
+    /**
+     * Return the sharelib postfix for the action.
+     *
+     * @param context executor context.
+     * @param actionXml the action XML.
+     * @return the action sharelib post fix, this implementation returns <code>hive</code>.
+     */
+    protected String getShareLibPostFix(Context context, Element actionXml) {
+        return "sqoop";
+    }
+
+}
diff --git core/src/main/java/org/apache/oozie/action/hadoop/SqoopMain.java core/src/main/java/org/apache/oozie/action/hadoop/SqoopMain.java
new file mode 100644
index 0000000..7070820
--- /dev/null
+++ core/src/main/java/org/apache/oozie/action/hadoop/SqoopMain.java
@@ -0,0 +1,211 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.action.hadoop;
+
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.net.URL;
+import java.util.Map;
+import java.util.Properties;
+import java.util.regex.Pattern;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.log4j.PropertyConfigurator;
+import org.apache.sqoop.Sqoop;
+
+public class SqoopMain extends LauncherMain {
+
+    private static final String SQOOP_ARGS = "oozie.sqoop.args";
+
+    public static final String SQOOP_SITE_CONF = "sqoop-site.xml";
+
+    private static final Pattern[] SQOOP_JOB_IDS_PATTERNS = {
+      Pattern.compile("Job complete: (job_\\S*)"), Pattern.compile("Job (job_\\S*) completed successfully")
+    };
+
+    private static final String SQOOP_LOG4J_PROPS = "sqoop-log4j.properties";
+
+    public static void main(String[] args) throws Exception {
+        run(SqoopMain.class, args);
+    }
+
+    private static Configuration initActionConf() {
+        // loading action conf prepared by Oozie
+        Configuration sqoopConf = new Configuration(false);
+
+        String actionXml = System.getProperty("oozie.action.conf.xml");
+
+        if (actionXml == null) {
+            throw new RuntimeException("Missing Java System Property [oozie.action.conf.xml]");
+        }
+        if (!new File(actionXml).exists()) {
+            throw new RuntimeException("Action Configuration XML file [" + actionXml + "] does not exist");
+        }
+
+        sqoopConf.addResource(new Path("file:///", actionXml));
+
+        String delegationToken = System.getenv("HADOOP_TOKEN_FILE_LOCATION");
+        if (delegationToken != null) {
+            sqoopConf.set("mapreduce.job.credentials.binary", delegationToken);
+            System.out.println("------------------------");
+            System.out.println("Setting env property for mapreduce.job.credentials.binary to: " + delegationToken);
+            System.out.println("------------------------");
+            System.setProperty("mapreduce.job.credentials.binary", delegationToken);
+        } else {
+            System.out.println("Non-Kerberos execution");
+        }
+
+        return sqoopConf;
+    }
+
+    public static Configuration setUpSqoopSite() throws Exception {
+        Configuration sqoopConf = initActionConf();
+
+        // Write the action configuration out to sqoop-site.xml
+        OutputStream os = new FileOutputStream(SQOOP_SITE_CONF);
+        try {
+            sqoopConf.writeXml(os);
+        }
+        finally {
+            os.close();
+        }
+      
+        System.out.println();
+        System.out.println("Sqoop Configuration Properties:");
+        System.out.println("------------------------");
+        for (Map.Entry<String, String> entry : sqoopConf) {
+            System.out.println(entry.getKey() + "=" + entry.getValue());
+        }
+        System.out.flush();
+        System.out.println("------------------------");
+        System.out.println();
+        return sqoopConf;
+    }
+
+    public static String setUpSqoopLog4J(Configuration sqoopConf) throws IOException {
+        //Logfile to capture job IDs
+        String hadoopJobId = System.getProperty("oozie.launcher.job.id");
+        if (hadoopJobId == null) {
+            throw new RuntimeException("Launcher Hadoop Job ID system property not set");
+        }
+
+        String logFile = new File("sqoop-oozie-" + hadoopJobId + ".log").getAbsolutePath();
+
+        Properties hadoopProps = new Properties();
+
+        // Preparing log4j configuration
+        URL log4jFile = Thread.currentThread().getContextClassLoader().getResource("log4j.properties");
+        if (log4jFile != null) {
+            // getting hadoop log4j configuration
+            hadoopProps.load(log4jFile.openStream());
+        }
+
+        String logLevel = sqoopConf.get("oozie.sqoop.log.level", "INFO");
+
+        hadoopProps.setProperty("log4j.logger.org.apache.sqoop", logLevel + ", A");
+        hadoopProps.setProperty("log4j.appender.A", "org.apache.log4j.ConsoleAppender");
+        hadoopProps.setProperty("log4j.appender.A.layout", "org.apache.log4j.PatternLayout");
+        hadoopProps.setProperty("log4j.appender.A.layout.ConversionPattern", "%-4r [%t] %-5p %c %x - %m%n");
+
+        hadoopProps.setProperty("log4j.appender.jobid", "org.apache.log4j.FileAppender");
+        hadoopProps.setProperty("log4j.appender.jobid.file", logFile);
+        hadoopProps.setProperty("log4j.appender.jobid.layout", "org.apache.log4j.PatternLayout");
+        hadoopProps.setProperty("log4j.appender.jobid.layout.ConversionPattern", "%-4r [%t] %-5p %c %x - %m%n");
+        hadoopProps.setProperty("log4j.logger.org.apache.hadoop.mapred", "INFO, jobid");
+        hadoopProps.setProperty("log4j.logger.org.apache.hadoop.mapreduce.Job", "INFO, jobid");
+
+        String localProps = new File(SQOOP_LOG4J_PROPS).getAbsolutePath();
+        OutputStream os1 = new FileOutputStream(localProps);
+        try {
+            hadoopProps.store(os1, "");
+        }
+        finally {
+            os1.close();
+        }
+      
+        PropertyConfigurator.configure(SQOOP_LOG4J_PROPS);
+
+        return logFile;
+    }
+
+    protected void run(String[] args) throws Exception {
+        System.out.println();
+        System.out.println("Oozie Sqoop action configuration");
+        System.out.println("=================================================================");
+
+        Configuration sqoopConf = setUpSqoopSite();
+        String logFile = setUpSqoopLog4J(sqoopConf);
+
+        String[] sqoopArgs = MapReduceMain.getStrings(sqoopConf, SQOOP_ARGS);
+        if (sqoopArgs == null) {
+            throw new RuntimeException("Action Configuration does not have [" + SQOOP_ARGS + "] property");
+        }
+
+        System.out.println("Sqoop command arguments :");
+        for (String arg : sqoopArgs) {
+            System.out.println("             " + arg);
+        }
+
+        System.out.println("=================================================================");
+        System.out.println();
+        System.out.println(">>> Invoking Sqoop command line now >>>");
+        System.out.println();
+        System.out.flush();
+
+        try {
+            runSqoopJob(sqoopArgs);
+        }
+        catch (SecurityException ex) {
+            if (LauncherSecurityManager.getExitInvoked()) {
+                if (LauncherSecurityManager.getExitCode() != 0) {
+                    throw ex;
+                }
+            }
+        }
+
+        System.out.println();
+        System.out.println("<<< Invocation of Sqoop command completed <<<");
+        System.out.println();
+
+        // harvesting and recording Hadoop Job IDs
+        Properties jobIds = getHadoopJobIds(logFile, SQOOP_JOB_IDS_PATTERNS);
+
+        File file = new File(System.getProperty("oozie.action.output.properties"));
+        OutputStream os = new FileOutputStream(file);
+        try {
+            jobIds.store(os, "");
+        }
+        finally {
+            os.close();
+        }
+        System.out.println(" Hadoop Job IDs executed by Sqoop: " + jobIds.getProperty(HADOOP_JOBS));
+        System.out.println();
+    }
+
+    protected void runSqoopJob(String[] args) throws Exception {
+        // running as from the command line
+        Sqoop.main(args);
+    }
+
+    public static void setSqoopCommand(Configuration conf, String[] args) {
+        MapReduceMain.setStrings(conf, SQOOP_ARGS, args);
+    }
+}
diff --git core/src/main/java/org/apache/oozie/client/rest/JsonWorkflowAction.java core/src/main/java/org/apache/oozie/client/rest/JsonWorkflowAction.java
index 760d33f..64b7672 100644
--- core/src/main/java/org/apache/oozie/client/rest/JsonWorkflowAction.java
+++ core/src/main/java/org/apache/oozie/client/rest/JsonWorkflowAction.java
@@ -88,6 +88,14 @@ public class JsonWorkflowAction implements WorkflowAction, JsonBean {
     @Lob
     private String data = null;
 
+    @Column(name = "stats")
+    @Lob
+    private String stats = null;
+
+    @Column(name = "external_child_ids")
+    @Lob
+    private String externalChildIDs = null;
+
     @Basic
     @Column(name = "external_id")
     private String externalId = null;
@@ -129,6 +137,8 @@ public class JsonWorkflowAction implements WorkflowAction, JsonBean {
         json.put(JsonTags.WORKFLOW_ACTION_END_TIME, JsonUtils.formatDateRfc822(endTime));
         json.put(JsonTags.WORKFLOW_ACTION_TRANSITION, transition);
         json.put(JsonTags.WORKFLOW_ACTION_DATA, data);
+        json.put(JsonTags.WORKFLOW_ACTION_STATS, stats);
+        json.put(JsonTags.WORKFLOW_ACTION_EXTERNAL_CHILD_IDS, externalChildIDs);
         json.put(JsonTags.WORKFLOW_ACTION_EXTERNAL_ID, externalId);
         json.put(JsonTags.WORKFLOW_ACTION_EXTERNAL_STATUS, externalStatus);
         json.put(JsonTags.WORKFLOW_ACTION_TRACKER_URI, trackerUri);
@@ -255,6 +265,22 @@ public class JsonWorkflowAction implements WorkflowAction, JsonBean {
         this.data = data;
     }
 
+    public String getStats() {
+        return stats;
+    }
+
+    public void setStats(String stats) {
+        this.stats = stats;
+    }
+
+    public String getExternalChildIDs() {
+        return externalChildIDs;
+    }
+
+    public void setExternalChildIDs(String externalChildIDs) {
+        this.externalChildIDs = externalChildIDs;
+    }
+
     public String getExternalId() {
         return externalId;
     }
diff --git core/src/main/java/org/apache/oozie/command/coord/CoordActionInputCheckXCommand.java core/src/main/java/org/apache/oozie/command/coord/CoordActionInputCheckXCommand.java
index 904898b..36d3532 100644
--- core/src/main/java/org/apache/oozie/command/coord/CoordActionInputCheckXCommand.java
+++ core/src/main/java/org/apache/oozie/command/coord/CoordActionInputCheckXCommand.java
@@ -40,6 +40,7 @@ import org.apache.oozie.executor.jpa.JPAExecutorException;
 import org.apache.oozie.service.HadoopAccessorException;
 import org.apache.oozie.service.HadoopAccessorService;
 import org.apache.oozie.service.JPAService;
+import org.apache.oozie.service.Service;
 import org.apache.oozie.service.Services;
 import org.apache.oozie.util.DateUtils;
 import org.apache.oozie.util.ELEvaluator;
@@ -57,7 +58,17 @@ import org.jdom.Element;
 public class CoordActionInputCheckXCommand extends CoordinatorXCommand<Void> {
 
     private final String actionId;
-    private final int COMMAND_REQUEUE_INTERVAL = 60000; // 1 minute
+    /**
+     * Property name of command re-queue interval for coordinator action input check in
+     * milliseconds.
+     */
+    public static final String CONF_COORD_INPUT_CHECK_REQUEUE_INTERVAL = Service.CONF_PREFIX
+            + "coord.input.check.requeue.interval";
+    /**
+     * Default re-queue interval in ms. It is applied when no value defined in
+     * the oozie configuration.
+     */
+    private final int DEFAULT_COMMAND_REQUEUE_INTERVAL = 60000; // 1 minute
     private CoordinatorActionBean coordAction = null;
     private CoordinatorJobBean coordJob = null;
     private JPAService jpaService = null;
@@ -79,8 +90,8 @@ public class CoordActionInputCheckXCommand extends CoordinatorXCommand<Void> {
         Date nominalTime = coordAction.getNominalTime();
         Date currentTime = new Date();
         if (nominalTime.compareTo(currentTime) > 0) {
-            queue(new CoordActionInputCheckXCommand(coordAction.getId()), Math.max(
-                    (nominalTime.getTime() - currentTime.getTime()), COMMAND_REQUEUE_INTERVAL));
+            queue(new CoordActionInputCheckXCommand(coordAction.getId()), Math.max((nominalTime.getTime() - currentTime
+                    .getTime()), getCoordInputCheckRequeueInterval()));
             // update lastModifiedTime
             coordAction.setLastModifiedTime(new Date());
             try {
@@ -129,7 +140,7 @@ public class CoordActionInputCheckXCommand extends CoordinatorXCommand<Void> {
                     queue(new CoordActionTimeOutXCommand(coordAction), 100);
                 }
                 else {
-                    queue(new CoordActionInputCheckXCommand(coordAction.getId()), COMMAND_REQUEUE_INTERVAL);
+                    queue(new CoordActionInputCheckXCommand(coordAction.getId()), getCoordInputCheckRequeueInterval());
                 }
             }
             coordAction.setLastModifiedTime(new Date());
@@ -144,6 +155,20 @@ public class CoordActionInputCheckXCommand extends CoordinatorXCommand<Void> {
     }
 
     /**
+     * This function reads the value of re-queue interval for coordinator input
+     * check command from the Oozie configuration provided by Configuration
+     * Service. If nothing defined in the configuration, it uses the code
+     * specified default value.
+     *
+     * @return re-queue interval in ms
+     */
+    public long getCoordInputCheckRequeueInterval() {
+        long requeueInterval = Services.get().getConf().getLong(CONF_COORD_INPUT_CHECK_REQUEUE_INTERVAL,
+                DEFAULT_COMMAND_REQUEUE_INTERVAL);
+        return requeueInterval;
+    }
+
+    /**
      * To check the list of input paths if all of them exist
      *
      * @param actionXml action xml
diff --git core/src/main/java/org/apache/oozie/command/coord/CoordJobXCommand.java core/src/main/java/org/apache/oozie/command/coord/CoordJobXCommand.java
index 56b7081..03042cf 100644
--- core/src/main/java/org/apache/oozie/command/coord/CoordJobXCommand.java
+++ core/src/main/java/org/apache/oozie/command/coord/CoordJobXCommand.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -26,6 +26,7 @@ import org.apache.oozie.XException;
 import org.apache.oozie.command.CommandException;
 import org.apache.oozie.command.PreconditionException;
 import org.apache.oozie.executor.jpa.CoordActionsSubsetGetForJobJPAExecutor;
+import org.apache.oozie.executor.jpa.CoordJobGetActionsSubsetJPAExecutor;
 import org.apache.oozie.executor.jpa.CoordJobGetJPAExecutor;
 import org.apache.oozie.service.JPAService;
 import org.apache.oozie.service.Services;
@@ -118,7 +119,7 @@ public class CoordJobXCommand extends CoordinatorXCommand<CoordinatorJobBean> {
                 coordJob = jpaService.execute(new CoordJobGetJPAExecutor(id));
                 if (getActionInfo) {
                     List<CoordinatorActionBean> coordActions = jpaService
-                            .execute(new CoordActionsSubsetGetForJobJPAExecutor(id, start, len));
+                            .execute(new CoordJobGetActionsSubsetJPAExecutor(id, start, len));
                     coordJob.setActions(coordActions);
                 }
             }
diff --git core/src/main/java/org/apache/oozie/command/coord/CoordRerunXCommand.java core/src/main/java/org/apache/oozie/command/coord/CoordRerunXCommand.java
index 55c6a6e..9572f35 100644
--- core/src/main/java/org/apache/oozie/command/coord/CoordRerunXCommand.java
+++ core/src/main/java/org/apache/oozie/command/coord/CoordRerunXCommand.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -44,6 +44,7 @@ import org.apache.oozie.command.PreconditionException;
 import org.apache.oozie.command.RerunTransitionXCommand;
 import org.apache.oozie.command.bundle.BundleStatusUpdateXCommand;
 import org.apache.oozie.coord.CoordELFunctions;
+import org.apache.oozie.coord.CoordUtils;
 import org.apache.oozie.executor.jpa.CoordActionGetJPAExecutor;
 import org.apache.oozie.executor.jpa.CoordJobGetActionForNominalTimeJPAExecutor;
 import org.apache.oozie.executor.jpa.CoordJobGetActionsForDatesJPAExecutor;
@@ -103,158 +104,13 @@ public class CoordRerunXCommand extends RerunTransitionXCommand<CoordinatorActio
     }
 
     /**
-     * Get the list of actions for given id ranges
-     *
-     * @param jobId coordinator job id
-     * @param scope the id range to rerun separated by ","
-     * @return the list of all actions to rerun
-     * @throws CommandException thrown if failed to get coordinator actions by given id range
-     */
-    private List<CoordinatorActionBean> getCoordActionsFromIds(String jobId, String scope) throws CommandException {
-        ParamChecker.notEmpty(jobId, "jobId");
-        ParamChecker.notEmpty(scope, "scope");
-
-        Set<String> actions = new HashSet<String>();
-        String[] list = scope.split(",");
-        for (String s : list) {
-            s = s.trim();
-            if (s.contains("-")) {
-                String[] range = s.split("-");
-                if (range.length != 2) {
-                    throw new CommandException(ErrorCode.E0302, "format is wrong for action's range '" + s + "'");
-                }
-                int start;
-                int end;
-                try {
-                    start = Integer.parseInt(range[0].trim());
-                    end = Integer.parseInt(range[1].trim());
-                    if (start > end) {
-                        throw new CommandException(ErrorCode.E0302, "format is wrong for action's range '" + s + "'");
-                    }
-                }
-                catch (NumberFormatException ne) {
-                    throw new CommandException(ErrorCode.E0302, ne);
-                }
-                for (int i = start; i <= end; i++) {
-                    actions.add(jobId + "@" + i);
-                }
-            }
-            else {
-                try {
-                    Integer.parseInt(s);
-                }
-                catch (NumberFormatException ne) {
-                    throw new CommandException(ErrorCode.E0302, "format is wrong for action id'" + s
-                            + "'. Integer only.");
-                }
-                actions.add(jobId + "@" + s);
-            }
-        }
-
-        List<CoordinatorActionBean> coordActions = new ArrayList<CoordinatorActionBean>();
-        for (String id : actions) {
-            CoordinatorActionBean coordAction;
-            try {
-                coordAction = jpaService.execute(new CoordActionGetJPAExecutor(id));
-            }
-            catch (JPAExecutorException je) {
-                throw new CommandException(je);
-            }
-            coordActions.add(coordAction);
-            LOG.debug("Rerun coordinator for actionId='" + id + "'");
-        }
-        return coordActions;
-    }
-
-    /**
-     * Get the list of actions for given date ranges
-     *
-     * @param jobId coordinator job id
-     * @param scope the date range to rerun separated by ","
-     * @return the list of dates to rerun
-     * @throws CommandException thrown if failed to get coordinator actions by given date range
-     */
-    private List<CoordinatorActionBean> getCoordActionsFromDates(String jobId, String scope) throws CommandException {
-        ParamChecker.notEmpty(jobId, "jobId");
-        ParamChecker.notEmpty(scope, "scope");
-
-        Set<CoordinatorActionBean> actionSet = new HashSet<CoordinatorActionBean>();
-        String[] list = scope.split(",");
-        for (String s : list) {
-            s = s.trim();
-            if (s.contains("::")) {
-                String[] dateRange = s.split("::");
-                if (dateRange.length != 2) {
-                    throw new CommandException(ErrorCode.E0302, "format is wrong for date's range '" + s + "'");
-                }
-                Date start;
-                Date end;
-                try {
-                    start = DateUtils.parseDateUTC(dateRange[0].trim());
-                    end = DateUtils.parseDateUTC(dateRange[1].trim());
-                    if (start.after(end)) {
-                        throw new CommandException(ErrorCode.E0302, "start date is older than end date: '" + s + "'");
-                    }
-                }
-                catch (Exception e) {
-                    throw new CommandException(ErrorCode.E0302, e);
-                }
-
-                List<CoordinatorActionBean> listOfActions = getActionIdsFromDateRange(jobId, start, end);
-                actionSet.addAll(listOfActions);
-            }
-            else {
-                try {
-                    Date date = DateUtils.parseDateUTC(s.trim());
-                    CoordinatorActionBean coordAction = jpaService
-                            .execute(new CoordJobGetActionForNominalTimeJPAExecutor(jobId, date));
-                    actionSet.add(coordAction);
-                }
-                catch (JPAExecutorException e) {
-                    throw new CommandException(e);
-                }
-                catch (Exception e) {
-                    throw new CommandException(ErrorCode.E0302, e);
-                }
-            }
-        }
-
-        List<CoordinatorActionBean> coordActions = new ArrayList<CoordinatorActionBean>();
-        for (CoordinatorActionBean coordAction : actionSet) {
-            coordActions.add(coordAction);
-            LOG.debug("Rerun coordinator for actionId='" + coordAction.getId() + "'");
-        }
-        return coordActions;
-    }
-
-    /**
-     * Get coordinator action ids between given start and end time
-     *
-     * @param jobId coordinator job id
-     * @param start start time
-     * @param end end time
-     * @return a list of coordinator actions belong to the range of start and end time
-     * @throws CommandException thrown if failed to get coordinator actions
-     */
-    private List<CoordinatorActionBean> getActionIdsFromDateRange(String jobId, Date start, Date end)
-            throws CommandException {
-        List<CoordinatorActionBean> list;
-        try {
-            list = jpaService.execute(new CoordJobGetActionsForDatesJPAExecutor(jobId, start, end));
-        }
-        catch (JPAExecutorException je) {
-            throw new CommandException(je);
-        }
-        return list;
-    }
-
-    /**
      * Check if all given actions are eligible to rerun.
      *
      * @param actions list of CoordinatorActionBean
      * @return true if all actions are eligible to rerun
      */
-    private boolean checkAllActionsRunnable(List<CoordinatorActionBean> coordActions) {
+    private static boolean checkAllActionsRunnable(List<CoordinatorActionBean> coordActions) {
+        ParamChecker.notNull(coordActions, "Coord actions to be rerun");
         boolean ret = false;
         for (CoordinatorActionBean coordAction : coordActions) {
             ret = true;
@@ -267,6 +123,25 @@ public class CoordRerunXCommand extends RerunTransitionXCommand<CoordinatorActio
     }
 
     /**
+     * Get the list of actions for a given coordinator job
+     * @param rerunType the rerun type (date, action)
+     * @param jobId the coordinator job id
+     * @param scope the date scope or action id scope
+     * @return the list of Coordinator actions
+     * @throws CommandException
+     */
+    public static List<CoordinatorActionBean> getCoordActions(String rerunType, String jobId, String scope) throws CommandException{
+        List<CoordinatorActionBean> coordActions = null;
+        if (rerunType.equals(RestConstants.JOB_COORD_RERUN_DATE)) {
+            coordActions = CoordUtils.getCoordActionsFromDates(jobId, scope);
+        }
+        else if (rerunType.equals(RestConstants.JOB_COORD_RERUN_ACTION)) {
+            coordActions = CoordUtils.getCoordActionsFromIds(jobId, scope);
+        }
+        return coordActions;
+    }
+
+    /**
      * Cleanup output-events directories
      *
      * @param eAction coordinator action xml
@@ -436,17 +311,7 @@ public class CoordRerunXCommand extends RerunTransitionXCommand<CoordinatorActio
         try {
             CoordinatorActionInfo coordInfo = null;
             InstrumentUtils.incrJobCounter(getName(), 1, getInstrumentation());
-            List<CoordinatorActionBean> coordActions;
-            if (rerunType.equals(RestConstants.JOB_COORD_RERUN_DATE)) {
-                coordActions = getCoordActionsFromDates(jobId, scope);
-            }
-            else if (rerunType.equals(RestConstants.JOB_COORD_RERUN_ACTION)) {
-                coordActions = getCoordActionsFromIds(jobId, scope);
-            }
-            else {
-                isError = true;
-                throw new CommandException(ErrorCode.E1018, "date or action expected.");
-            }
+            List<CoordinatorActionBean> coordActions = getCoordActions(rerunType, jobId, scope);
             if (checkAllActionsRunnable(coordActions)) {
                 for (CoordinatorActionBean coordAction : coordActions) {
                     String actionXml = coordAction.getActionXml();
diff --git core/src/main/java/org/apache/oozie/command/coord/CoordSubmitXCommand.java core/src/main/java/org/apache/oozie/command/coord/CoordSubmitXCommand.java
index e78ba2f..e3fdecb 100644
--- core/src/main/java/org/apache/oozie/command/coord/CoordSubmitXCommand.java
+++ core/src/main/java/org/apache/oozie/command/coord/CoordSubmitXCommand.java
@@ -24,13 +24,17 @@ import java.io.StringReader;
 import java.io.StringWriter;
 import java.net.URI;
 import java.net.URISyntaxException;
+import java.text.DateFormat;
+import java.text.SimpleDateFormat;
 import java.util.ArrayList;
 import java.util.Date;
 import java.util.HashMap;
 import java.util.HashSet;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.TimeZone;
 import java.util.TreeSet;
 
 import javax.xml.transform.stream.StreamSource;
@@ -101,6 +105,10 @@ public class CoordSubmitXCommand extends SubmitTransitionXCommand {
 
     public static final String CONFIG_DEFAULT = "coord-config-default.xml";
     public static final String COORDINATOR_XML_FILE = "coordinator.xml";
+    public final String COORD_INPUT_EVENTS ="input-events";
+    public final String COORD_OUTPUT_EVENTS = "output-events";
+    public final String COORD_INPUT_EVENTS_DATA_IN ="data-in";
+    public final String COORD_OUTPUT_EVENTS_DATA_OUT = "data-out";
 
     private static final Set<String> DISALLOWED_USER_PROPERTIES = new HashSet<String>();
     private static final Set<String> DISALLOWED_DEFAULT_PROPERTIES = new HashSet<String>();
@@ -211,6 +219,12 @@ public class CoordSubmitXCommand extends SubmitTransitionXCommand {
             appXml = XmlUtils.removeComments(appXml);
             initEvaluators();
             Element eJob = basicResolveAndIncludeDS(appXml, conf, coordJob);
+
+            // checking if the coordinator application data input/output events
+            // specify multiple data instance values in erroneous manner
+            checkMultipleTimeInstances(eJob, COORD_INPUT_EVENTS, COORD_INPUT_EVENTS_DATA_IN);
+            checkMultipleTimeInstances(eJob, COORD_OUTPUT_EVENTS, COORD_OUTPUT_EVENTS_DATA_OUT);
+
             LOG.debug("jobXml after all validation " + XmlUtils.prettyPrint(eJob).toString());
 
             jobId = storeToDB(eJob, coordJob);
@@ -279,6 +293,43 @@ public class CoordSubmitXCommand extends SubmitTransitionXCommand {
         return jobId;
     }
 
+    /*
+     * Check against multiple data instance values inside a single <instance> tag
+     * If found, the job is not submitted and user is informed to correct the error, instead of defaulting to the first instance value in the list
+     */
+    private void checkMultipleTimeInstances(Element eCoordJob, String eventType, String dataType) throws CoordinatorJobException {
+        Element eventsSpec, dataSpec, instance;
+        List<Element> instanceSpecList;
+        Namespace ns = eCoordJob.getNamespace();
+        String instanceValue;
+        eventsSpec = eCoordJob.getChild(eventType, ns);
+        if (eventsSpec != null) {
+            dataSpec = eventsSpec.getChild(dataType, ns);
+            if (dataSpec != null) {
+                // In case of input-events, there can be multiple child <instance> datasets. Iterating to ensure none of them have errors
+                instanceSpecList = dataSpec.getChildren("instance", ns);
+                Iterator instanceIter = instanceSpecList.iterator();
+                while(instanceIter.hasNext()) {
+                    instance = ((Element) instanceIter.next());
+                    if(instance.getContentSize() == 0) { //empty string or whitespace
+                        throw new CoordinatorJobException(ErrorCode.E1021, "<instance> tag within " + eventType + " is empty!");
+                    }
+                    instanceValue = instance.getContent(0).toString();
+                    if (instanceValue.contains(",")) { // reaching this block implies instance is not empty i.e. length > 0
+                        String correctAction = null;
+                        if(dataType.equals(COORD_INPUT_EVENTS_DATA_IN)) {
+                            correctAction = "Coordinator app definition should have separate <instance> tag per data-in instance";
+                        } else if(dataType.equals(COORD_OUTPUT_EVENTS_DATA_OUT)) {
+                            correctAction = "Coordinator app definition can have only one <instance> tag per data-out instance";
+                        }
+                        throw new CoordinatorJobException(ErrorCode.E1021, eventType + " instance '" + instanceValue
+                                + "' contains more than one date instance. Coordinator job NOT SUBMITTED. " + correctAction);
+                    }
+                }
+            }
+        }
+    }
+
     /**
      * Read the application XML and validate against coordinator Schema
      *
@@ -715,6 +766,7 @@ public class CoordSubmitXCommand extends SubmitTransitionXCommand {
                     .toString() : ((TimeUnit) evalFreq.getVariable("endOfDuration")).toString());
             val = resolveAttribute("initial-instance", dsElem, evalNofuncs);
             ParamChecker.checkUTC(val, "initial-instance");
+            checkInitialInstance(val);
             val = resolveAttribute("timezone", dsElem, evalNofuncs);
             ParamChecker.checkTimeZone(val, "timezone");
             resolveTagContents("uri-template", dsElem, evalNofuncs);
@@ -958,6 +1010,26 @@ public class CoordSubmitXCommand extends SubmitTransitionXCommand {
         return jobId;
     }
 
+    /*
+     * this method checks if the initial-instance specified for a particular
+       is not a date earlier than the oozie server default Jan 01, 1970 00:00Z UTC
+     */
+    private void checkInitialInstance(String val) throws CoordinatorJobException, IllegalArgumentException {
+        Date initialInstance, givenInstance;
+        DateFormat df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm'Z'");
+        df.setTimeZone(TimeZone.getTimeZone("UTC"));
+        try {
+            initialInstance = DateUtils.parseDateUTC("1970-01-01T00:00Z");
+            givenInstance = DateUtils.parseDateUTC(val);
+        }
+        catch (Exception e) {
+            throw new IllegalArgumentException("Unable to parse dataset initial-instance string '" + val + "' to Date object. ",e);
+        }
+        if(givenInstance.compareTo(initialInstance) < 0) {
+            throw new CoordinatorJobException(ErrorCode.E1021, "Dataset initial-instance " + df.format(givenInstance) + " is earlier than the default initial instance " + df.format(initialInstance));
+        }
+    }
+
     /* (non-Javadoc)
      * @see org.apache.oozie.command.XCommand#getEntityKey()
      */
diff --git core/src/main/java/org/apache/oozie/command/wf/ActionCommand.java core/src/main/java/org/apache/oozie/command/wf/ActionCommand.java
index e51a750..cfbed24 100644
--- core/src/main/java/org/apache/oozie/command/wf/ActionCommand.java
+++ core/src/main/java/org/apache/oozie/command/wf/ActionCommand.java
@@ -260,6 +260,16 @@ public abstract class ActionCommand<T> extends WorkflowCommand<Void> {
             executed = true;
         }
 
+        public void setExecutionStats(String jsonStats) {
+            action.setExecutionStats(jsonStats);
+            executed = true;
+        }
+
+        public void setExternalChildIDs(String externalChildIDs) {
+            action.setExternalChildIDs(externalChildIDs);
+            executed = true;
+        }
+
         public void setEndData(WorkflowAction.Status status, String signalValue) {
             action.setEndData(status, signalValue);
             ended = true;
diff --git core/src/main/java/org/apache/oozie/command/wf/ActionXCommand.java core/src/main/java/org/apache/oozie/command/wf/ActionXCommand.java
index b348b50..aa0054a 100644
--- core/src/main/java/org/apache/oozie/command/wf/ActionXCommand.java
+++ core/src/main/java/org/apache/oozie/command/wf/ActionXCommand.java
@@ -305,6 +305,16 @@ public abstract class ActionXCommand<T> extends WorkflowXCommand<Void> {
             executed = true;
         }
 
+        public void setExecutionStats(String jsonStats) {
+            action.setExecutionStats(jsonStats);
+            executed = true;
+        }
+
+        public void setExternalChildIDs(String externalChildIDs) {
+            action.setExternalChildIDs(externalChildIDs);
+            executed = true;
+        }
+
         public void setEndData(WorkflowAction.Status status, String signalValue) {
             action.setEndData(status, signalValue);
             ended = true;
diff --git core/src/main/java/org/apache/oozie/command/wf/ReRunXCommand.java core/src/main/java/org/apache/oozie/command/wf/ReRunXCommand.java
index 7b991a0..11e4e5e 100644
--- core/src/main/java/org/apache/oozie/command/wf/ReRunXCommand.java
+++ core/src/main/java/org/apache/oozie/command/wf/ReRunXCommand.java
@@ -69,7 +69,7 @@ import org.apache.oozie.workflow.lite.NodeHandler;
  */
 public class ReRunXCommand extends WorkflowXCommand<Void> {
     private final String jobId;
-    private final Configuration conf;
+    private Configuration conf;
     private final String authToken;
     private final Set<String> nodesToSkip = new HashSet<String>();
     public static final String TO_SKIP = "TO_SKIP";
@@ -138,6 +138,11 @@ public class ReRunXCommand extends WorkflowXCommand<Void> {
 
             PropertiesUtils.checkDisallowedProperties(conf, DISALLOWED_USER_PROPERTIES);
 
+            // Resolving all variables in the job properties. This ensures the Hadoop Configuration semantics are preserved.
+            // The Configuration.get function within XConfiguration.resolve() works recursively to get the final value corresponding to a key in the map
+            // Resetting the conf to contain all the resolved values is necessary to ensure propagation of Oozie properties to Hadoop calls downstream
+            conf = ((XConfiguration) conf).resolve();
+
             try {
                 newWfInstance = workflowLib.createInstance(app, conf, jobId);
             }
diff --git core/src/main/java/org/apache/oozie/command/wf/SubmitCommand.java core/src/main/java/org/apache/oozie/command/wf/SubmitCommand.java
index 1f61187..fc9edc7 100644
--- core/src/main/java/org/apache/oozie/command/wf/SubmitCommand.java
+++ core/src/main/java/org/apache/oozie/command/wf/SubmitCommand.java
@@ -115,7 +115,8 @@ public class SubmitCommand extends WorkflowCommand<String> {
             String user = conf.get(OozieClient.USER_NAME);
             String group = conf.get(OozieClient.GROUP_NAME);
             FileSystem fs = Services.get().get(HadoopAccessorService.class).createFileSystem(user, group,
-                                                                                             configDefault.toUri(), new Configuration());
+                                                                                             configDefault.toUri(),
+                                                                                             conf);
 
             if (fs.exists(configDefault)) {
                 try {
diff --git core/src/main/java/org/apache/oozie/coord/CoordUtils.java core/src/main/java/org/apache/oozie/coord/CoordUtils.java
index 4403778..8e5ef1e 100644
--- core/src/main/java/org/apache/oozie/coord/CoordUtils.java
+++ core/src/main/java/org/apache/oozie/coord/CoordUtils.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -17,8 +17,26 @@
  */
 package org.apache.oozie.coord;
 
+import java.text.ParseException;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
 import org.apache.hadoop.conf.Configuration;
+import org.apache.oozie.CoordinatorActionBean;
+import org.apache.oozie.ErrorCode;
+import org.apache.oozie.XException;
 import org.apache.oozie.client.OozieClient;
+import org.apache.oozie.command.CommandException;
+import org.apache.oozie.executor.jpa.CoordActionGetJPAExecutor;
+import org.apache.oozie.executor.jpa.CoordJobGetActionForNominalTimeJPAExecutor;
+import org.apache.oozie.executor.jpa.JPAExecutorException;
+import org.apache.oozie.service.JPAService;
+import org.apache.oozie.service.Services;
+import org.apache.oozie.util.CoordActionsInDateRange;
+import org.apache.oozie.util.DateUtils;
 import org.apache.oozie.util.ParamChecker;
 import org.jdom.Element;
 
@@ -45,4 +63,134 @@ public class CoordUtils {
         conf.set(HADOOP_UGI, user + "," + group);
         return conf;
     }
+
+    /**
+     * Get the list of actions for given date ranges
+     *
+     * @param jobId coordinator job id
+     * @param scope a comma-separated list of date ranges. Each date range element is specified with two dates separated by '::'
+     * @return the list of Coordinator actions for the date range
+     * @throws CommandException thrown if failed to get coordinator actions by given date range
+     */
+    public static List<CoordinatorActionBean> getCoordActionsFromDates(String jobId, String scope) throws CommandException {
+        JPAService jpaService = Services.get().get(JPAService.class);
+        ParamChecker.notEmpty(jobId, "jobId");
+        ParamChecker.notEmpty(scope, "scope");
+
+        Set<CoordinatorActionBean> actionSet = new HashSet<CoordinatorActionBean>();
+        String[] list = scope.split(",");
+        for (String s : list) {
+            s = s.trim();
+            // A date range is specified with two dates separated by '::'
+            if (s.contains("::")) {
+            List<CoordinatorActionBean> listOfActions;
+            try {
+                // Get list of actions within the range of date
+                listOfActions = CoordActionsInDateRange.getCoordActionsFromDateRange(jobId, s);
+            }
+            catch (XException e) {
+                throw new CommandException(e);
+            }
+            actionSet.addAll(listOfActions);
+            }
+            else {
+                try {
+                    // Get action for the nominal time
+                    Date date = DateUtils.parseDateUTC(s.trim());
+                    CoordinatorActionBean coordAction = jpaService
+                            .execute(new CoordJobGetActionForNominalTimeJPAExecutor(jobId, date));
+
+                    if (coordAction != null) {
+                        actionSet.add(coordAction);
+                    }
+                    else {
+                        throw new RuntimeException("This should never happen, Coordinator Action shouldn't be null");
+                    }
+                }
+                catch (ParseException e) {
+                    throw new CommandException(ErrorCode.E0302, e);
+                }
+                catch (JPAExecutorException e) {
+                    throw new CommandException(e);
+                }
+
+            }
+        }
+
+        List<CoordinatorActionBean> coordActions = new ArrayList<CoordinatorActionBean>();
+        for (CoordinatorActionBean coordAction : actionSet) {
+            coordActions.add(coordAction);
+        }
+        return coordActions;
+    }
+
+    /**
+     * Get the list of actions for given id ranges
+     *
+     * @param jobId coordinator job id
+     * @param scope a comma-separated list of action ranges. The action range is specified with two action numbers separated by '-'
+     * @return the list of all Coordinator actions for action range
+     * @throws CommandException thrown if failed to get coordinator actions by given id range
+     */
+     public static List<CoordinatorActionBean> getCoordActionsFromIds(String jobId, String scope) throws CommandException {
+        JPAService jpaService = Services.get().get(JPAService.class);
+        ParamChecker.notEmpty(jobId, "jobId");
+        ParamChecker.notEmpty(scope, "scope");
+
+        Set<String> actions = new HashSet<String>();
+        String[] list = scope.split(",");
+        for (String s : list) {
+            s = s.trim();
+            // An action range is specified with two actions separated by '-'
+            if (s.contains("-")) {
+                String[] range = s.split("-");
+                // Check the format for action's range
+                if (range.length != 2) {
+                    throw new CommandException(ErrorCode.E0302, "format is wrong for action's range '" + s + "', an example of correct format is 1-5");
+                }
+                int start;
+                int end;
+                try {
+                    //Get the starting and ending action numbers
+                    start = Integer.parseInt(range[0].trim());
+                    end = Integer.parseInt(range[1].trim());
+                    if (start > end) {
+                        throw new CommandException(ErrorCode.E0302, "format is wrong for action's range '" + s
+                                + "', starting action number of the range should be less than ending action number, an example will be 1-4");
+                    }
+                }
+                catch (NumberFormatException ne) {
+                    throw new CommandException(ErrorCode.E0302, ne);
+                }
+                // Add the actionIds
+                for (int i = start; i <= end; i++) {
+                    actions.add(jobId + "@" + i);
+                }
+            }
+            else {
+                try {
+                    Integer.parseInt(s);
+                }
+                catch (NumberFormatException ne) {
+                    throw new CommandException(ErrorCode.E0302, "format is wrong for action id'" + s
+                            + "'. Integer only.");
+                }
+                actions.add(jobId + "@" + s);
+            }
+        }
+        // Retrieve the actions using the corresponding actionIds
+        List<CoordinatorActionBean> coordActions = new ArrayList<CoordinatorActionBean>();
+        for (String id : actions) {
+            CoordinatorActionBean coordAction;
+            try {
+                coordAction = jpaService.execute(new CoordActionGetJPAExecutor(id));
+            }
+            catch (JPAExecutorException je) {
+                throw new CommandException(je);
+            }
+            coordActions.add(coordAction);
+        }
+        return coordActions;
+    }
+
 }
diff --git core/src/main/java/org/apache/oozie/coord/TimeUnit.java core/src/main/java/org/apache/oozie/coord/TimeUnit.java
index 4bde648..72e9ba6 100644
--- core/src/main/java/org/apache/oozie/coord/TimeUnit.java
+++ core/src/main/java/org/apache/oozie/coord/TimeUnit.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -20,7 +20,7 @@ package org.apache.oozie.coord;
 import java.util.Calendar;
 
 public enum TimeUnit {
-    MINUTE(Calendar.MINUTE), HOUR(Calendar.HOUR), DAY(Calendar.DATE), MONTH(Calendar.MONTH), END_OF_DAY(Calendar.DATE), END_OF_MONTH(
+    MINUTE(Calendar.MINUTE), HOUR(Calendar.HOUR), DAY(Calendar.DATE), MONTH(Calendar.MONTH), YEAR(Calendar.YEAR), END_OF_DAY(Calendar.DATE), END_OF_MONTH(
         Calendar.MONTH), NONE(-1);
 
     private int calendarUnit;
diff --git core/src/main/java/org/apache/oozie/executor/jpa/CoordJobGetActionsSubsetJPAExecutor.java core/src/main/java/org/apache/oozie/executor/jpa/CoordJobGetActionsSubsetJPAExecutor.java
index e4f6d01..e8cd59f 100644
--- core/src/main/java/org/apache/oozie/executor/jpa/CoordJobGetActionsSubsetJPAExecutor.java
+++ core/src/main/java/org/apache/oozie/executor/jpa/CoordJobGetActionsSubsetJPAExecutor.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -58,7 +58,7 @@ public class CoordJobGetActionsSubsetJPAExecutor implements JPAExecutor<List<Coo
         List<CoordinatorActionBean> actions;
         List<CoordinatorActionBean> actionList = new ArrayList<CoordinatorActionBean>();
         try {
-            Query q = em.createNamedQuery("GET_ACTIONS_FOR_COORD_JOB");
+            Query q = em.createNamedQuery("GET_ACTIONS_FOR_COORD_JOB_ORDER_BY_NOMINAL_TIME");
             q.setParameter("jobId", coordJobId);
             q.setFirstResult(start - 1);
             q.setMaxResults(len);
diff --git core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionGetJPAExecutor.java core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionGetJPAExecutor.java
index 617f3ce..a86e1b8 100644
--- core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionGetJPAExecutor.java
+++ core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionGetJPAExecutor.java
@@ -79,6 +79,8 @@ public class WorkflowActionGetJPAExecutor implements JPAExecutor<WorkflowActionB
             action.setConf(a.getConf());
             action.setConsoleUrl(a.getConsoleUrl());
             action.setData(a.getData());
+            action.setStats(a.getStats());
+            action.setExternalChildIDs(a.getExternalChildIDs());
             action.setErrorInfo(a.getErrorCode(), a.getErrorMessage());
             action.setExternalId(a.getExternalId());
             action.setExternalStatus(a.getExternalStatus());
diff --git core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionSubsetGetJPAExecutor.java core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionSubsetGetJPAExecutor.java
index 99f9d0f..ae3a623 100644
--- core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionSubsetGetJPAExecutor.java
+++ core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionSubsetGetJPAExecutor.java
@@ -94,6 +94,8 @@ public class WorkflowActionSubsetGetJPAExecutor implements JPAExecutor<List<Work
             action.setConf(a.getConf());
             action.setConsoleUrl(a.getConsoleUrl());
             action.setData(a.getData());
+            action.setStats(a.getStats());
+            action.setExternalChildIDs(a.getExternalChildIDs());
             action.setErrorInfo(a.getErrorCode(), a.getErrorMessage());
             action.setExternalId(a.getExternalId());
             action.setExternalStatus(a.getExternalStatus());
diff --git core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionsGetForJobJPAExecutor.java core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionsGetForJobJPAExecutor.java
index aa4da54..294c920 100644
--- core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionsGetForJobJPAExecutor.java
+++ core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionsGetForJobJPAExecutor.java
@@ -78,6 +78,8 @@ public class WorkflowActionsGetForJobJPAExecutor implements JPAExecutor<List<Wor
             action.setConf(a.getConf());
             action.setConsoleUrl(a.getConsoleUrl());
             action.setData(a.getData());
+            action.setStats(a.getStats());
+            action.setExternalChildIDs(a.getExternalChildIDs());
             action.setErrorInfo(a.getErrorCode(), a.getErrorMessage());
             action.setExternalId(a.getExternalId());
             action.setExternalStatus(a.getExternalStatus());
diff --git core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionsRunningGetJPAExecutor.java core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionsRunningGetJPAExecutor.java
index f3eb526..f9b4f9d 100644
--- core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionsRunningGetJPAExecutor.java
+++ core/src/main/java/org/apache/oozie/executor/jpa/WorkflowActionsRunningGetJPAExecutor.java
@@ -83,6 +83,8 @@ public class WorkflowActionsRunningGetJPAExecutor implements JPAExecutor<List<Wo
             action.setConf(bean.getConf());
             action.setConsoleUrl(bean.getConsoleUrl());
             action.setData(bean.getData());
+            action.setStats(bean.getStats());
+            action.setExternalChildIDs(bean.getExternalChildIDs());
             action.setErrorInfo(bean.getErrorCode(), bean.getErrorMessage());
             action.setExternalId(bean.getExternalId());
             action.setExternalStatus(bean.getExternalStatus());
diff --git core/src/main/java/org/apache/oozie/executor/jpa/WorkflowJobGetActionsJPAExecutor.java core/src/main/java/org/apache/oozie/executor/jpa/WorkflowJobGetActionsJPAExecutor.java
index beff6f6..416f93e 100644
--- core/src/main/java/org/apache/oozie/executor/jpa/WorkflowJobGetActionsJPAExecutor.java
+++ core/src/main/java/org/apache/oozie/executor/jpa/WorkflowJobGetActionsJPAExecutor.java
@@ -72,6 +72,8 @@ public class WorkflowJobGetActionsJPAExecutor implements JPAExecutor<List<Workfl
             action.setConf(a.getConf());
             action.setConsoleUrl(a.getConsoleUrl());
             action.setData(a.getData());
+            action.setStats(a.getStats());
+            action.setExternalChildIDs(a.getExternalChildIDs());
             action.setErrorInfo(a.getErrorCode(), a.getErrorMessage());
             action.setExternalId(a.getExternalId());
             action.setExternalStatus(a.getExternalStatus());
diff --git core/src/main/java/org/apache/oozie/service/GroupsService.java core/src/main/java/org/apache/oozie/service/GroupsService.java
new file mode 100644
index 0000000..37afceb
--- /dev/null
+++ core/src/main/java/org/apache/oozie/service/GroupsService.java
@@ -0,0 +1,81 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.service;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.oozie.util.XConfiguration;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * The GroupsService class delegates to the Hadoop's <code>org.apache.hadoop.security.Groups</code>
+ * to retrieve the groups a user belongs to.
+ */
+public class GroupsService implements Service {
+    public static final String CONF_PREFIX = Service.CONF_PREFIX + "GroupsService.";
+
+    private org.apache.hadoop.security.Groups hGroups;
+
+    /**
+     * Returns the service interface.
+     * @return <code>GroupService</code>
+     */
+    @Override
+    public Class<? extends Service> getInterface() {
+        return GroupsService.class;
+    }
+
+    /**
+     * Initializes the service.
+     *
+     * @param services services singleton initializing the service.
+     */
+    @Override
+    public void init(Services services) {
+        Configuration sConf = services.getConf();
+        Configuration gConf = new XConfiguration();
+        for (Map.Entry<String, String> entry : sConf) {
+            String name = entry.getKey();
+            if (name.startsWith(CONF_PREFIX)) {
+                gConf.set(name.substring(CONF_PREFIX.length()), sConf.get(name));
+            }
+        }
+        hGroups = new org.apache.hadoop.security.Groups(gConf);
+    }
+
+    /**
+     * Destroys the service.
+     */
+    @Override
+    public void destroy() {
+    }
+
+    /**
+     * Returns the list of groups a user belongs to.
+     *
+     * @param user user name.
+     * @return the groups the given user belongs to.
+     * @throws IOException thrown if there was an error retrieving the groups of the user.
+     */
+    public List<String> getGroups(String user) throws IOException {
+        return hGroups.getGroups(user);
+    }
+
+}
diff --git core/src/main/java/org/apache/oozie/service/KerberosHadoopAccessorService.java core/src/main/java/org/apache/oozie/service/KerberosHadoopAccessorService.java
index a6d4034..2acfd33 100644
--- core/src/main/java/org/apache/oozie/service/KerberosHadoopAccessorService.java
+++ core/src/main/java/org/apache/oozie/service/KerberosHadoopAccessorService.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -31,10 +31,6 @@ import org.apache.oozie.util.XLog;
 import org.apache.oozie.util.XConfiguration;
 import org.apache.oozie.util.ParamChecker;
 import org.apache.oozie.ErrorCode;
-import org.apache.oozie.service.HadoopAccessorService;
-import org.apache.oozie.service.HadoopAccessorException;
-import org.apache.oozie.service.Service;
-import org.apache.oozie.service.ServiceException;
 
 import java.io.IOException;
 import java.net.URI;
@@ -129,6 +125,7 @@ public class KerberosHadoopAccessorService extends HadoopAccessorService {
             UserGroupInformation ugi = getUGI(user);
             JobClient jobClient = ugi.doAs(new PrivilegedExceptionAction<JobClient>() {
                 public JobClient run() throws Exception {
+                    conf.set("mapreduce.framework.name", "yarn");
                     return new JobClient(conf);
                 }
             });
diff --git core/src/main/java/org/apache/oozie/service/LiteWorkflowAppService.java core/src/main/java/org/apache/oozie/service/LiteWorkflowAppService.java
index 0906aa3..54f7afb 100644
--- core/src/main/java/org/apache/oozie/service/LiteWorkflowAppService.java
+++ core/src/main/java/org/apache/oozie/service/LiteWorkflowAppService.java
@@ -40,7 +40,7 @@ public class LiteWorkflowAppService extends WorkflowAppService {
         String appPath = ParamChecker.notEmpty(jobConf.get(OozieClient.APP_PATH), OozieClient.APP_PATH);
         String user = ParamChecker.notEmpty(jobConf.get(OozieClient.USER_NAME), OozieClient.USER_NAME);
         String group = ParamChecker.notEmpty(jobConf.get(OozieClient.GROUP_NAME), OozieClient.GROUP_NAME);
-        String workflowXml = readDefinition(appPath, user, group, authToken);
+        String workflowXml = readDefinition(appPath, user, group, authToken, jobConf);
         return parseDef(workflowXml);
     }
 
diff --git core/src/main/java/org/apache/oozie/service/ProxyUserService.java core/src/main/java/org/apache/oozie/service/ProxyUserService.java
new file mode 100644
index 0000000..afa13b8
--- /dev/null
+++ core/src/main/java/org/apache/oozie/service/ProxyUserService.java
@@ -0,0 +1,189 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.service;
+
+import org.apache.oozie.ErrorCode;
+import org.apache.oozie.util.ParamChecker;
+import org.apache.oozie.util.XLog;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.security.AccessControlException;
+import java.text.MessageFormat;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+/**
+ * The ProxyUserService checks if a user of a request has proxyuser privileges.
+ * <p/>
+ * This check is based on the following criteria:
+ * <p/>
+ * <ul>
+ *     <li>The user of the request must be configured as proxy user in Oozie configuration.</li>
+ *     <li>The user of the request must be making the request from a whitelisted host.</li>
+ *     <li>The user of the request must be making the request on behalf of a user of a whitelisted group.</li>
+ * </ul>
+ * <p/>
+ */
+public class ProxyUserService implements Service {
+    private static XLog LOG = XLog.getLog(ProxyUserService.class);
+
+    public static final String CONF_PREFIX = Service.CONF_PREFIX + "ProxyUserService.proxyuser.";
+    public static final String GROUPS = ".groups";
+    public static final String HOSTS = ".hosts";
+
+    private Services services;
+    private Map<String, Set<String>> proxyUserHosts = new HashMap<String, Set<String>>();
+    private Map<String, Set<String>> proxyUserGroups = new HashMap<String, Set<String>>();
+
+    /**
+     * Returns the service interface.
+     * @return <code>ProxyUserService</code>
+     */
+    @Override
+    public Class<? extends Service> getInterface() {
+        return ProxyUserService.class;
+    }
+
+    /**
+     * Initializes the service.
+     *
+     * @param services services singleton initializing the service.
+     * @throws ServiceException thrown if the service could not be configured correctly.
+     */
+    @Override
+    public void init(Services services) throws ServiceException {
+        this.services = services;
+        for (Map.Entry<String, String> entry : services.getConf()) {
+            String key = entry.getKey();
+            if (key.startsWith(CONF_PREFIX) && key.endsWith(GROUPS)) {
+                String proxyUser = key.substring(0, key.lastIndexOf(GROUPS));
+                if (services.getConf().get(proxyUser + HOSTS) == null) {
+                    throw new ServiceException(ErrorCode.E0551, CONF_PREFIX + proxyUser + HOSTS);
+                }
+                proxyUser = proxyUser.substring(CONF_PREFIX.length());
+                String value = entry.getValue().trim();
+                LOG.info("Loading proxyuser settings [{0}]=[{1}]", key, value);
+                Set<String> values = null;
+                if (!value.equals("*")) {
+                    values = new HashSet<String>(Arrays.asList(value.split(",")));
+                }
+                proxyUserGroups.put(proxyUser, values);
+            }
+            if (key.startsWith(CONF_PREFIX) && key.endsWith(HOSTS)) {
+                String proxyUser = key.substring(0, key.lastIndexOf(HOSTS));
+                if (services.getConf().get(proxyUser + GROUPS) == null) {
+                    throw new ServiceException(ErrorCode.E0551, CONF_PREFIX + proxyUser + GROUPS);
+                }
+                proxyUser = proxyUser.substring(CONF_PREFIX.length());
+                String value = entry.getValue().trim();
+                LOG.info("Loading proxyuser settings [{0}]=[{1}]", key, value);
+                Set<String> values = null;
+                if (!value.equals("*")) {
+                    String[] hosts = value.split(",");
+                    for (int i = 0; i < hosts.length; i++) {
+                        String originalName = hosts[i];
+                        try {
+                            hosts[i] = normalizeHostname(originalName);
+                        }
+                        catch (Exception ex) {
+                            throw new ServiceException(ErrorCode.E0550, originalName, ex.getMessage(), ex);
+                        }
+                        LOG.info("  Hostname, original [{0}], normalized [{1}]", originalName, hosts[i]);
+                    }
+                    values = new HashSet<String>(Arrays.asList(hosts));
+                }
+                proxyUserHosts.put(proxyUser, values);
+            }
+        }
+    }
+
+    /**
+     * Verifies a proxyuser.
+     *
+     * @param proxyUser user name of the proxy user.
+     * @param proxyHost host the proxy user is making the request from.
+     * @param doAsUser user the proxy user is impersonating.
+     * @throws IOException thrown if an error during the validation has occurred.
+     * @throws AccessControlException thrown if the user is not allowed to perform the proxyuser request.
+     */
+    public void validate(String proxyUser, String proxyHost, String doAsUser) throws IOException,
+        AccessControlException {
+        ParamChecker.notEmpty(proxyUser, "proxyUser");
+        ParamChecker.notEmpty(proxyHost, "proxyHost");
+        ParamChecker.notEmpty(doAsUser, "doAsUser");
+        LOG.debug("Authorization check proxyuser [{0}] host [{1}] doAs [{2}]",
+                  new Object[]{proxyUser, proxyHost, doAsUser});
+        if (proxyUserHosts.containsKey(proxyUser)) {
+            proxyHost = normalizeHostname(proxyHost);
+            validateRequestorHost(proxyUser, proxyHost, proxyUserHosts.get(proxyUser));
+            validateGroup(proxyUser, doAsUser, proxyUserGroups.get(proxyUser));
+        }
+        else {
+            throw new AccessControlException(MessageFormat.format("User [{0}] not defined as proxyuser", proxyUser));
+        }
+    }
+
+    private void validateRequestorHost(String proxyUser, String hostname, Set<String> validHosts)
+        throws IOException, AccessControlException {
+        if (validHosts != null) {
+            if (!validHosts.contains(hostname) && !validHosts.contains(normalizeHostname(hostname))) {
+                throw new AccessControlException(MessageFormat.format("Unauthorized host [{0}] for proxyuser [{1}]",
+                                                                      hostname, proxyUser));
+            }
+        }
+    }
+
+    private void validateGroup(String proxyUser, String user, Set<String> validGroups) throws IOException,
+        AccessControlException {
+        if (validGroups != null) {
+            List<String> userGroups = services.get(GroupsService.class).getGroups(user);
+            for (String g : validGroups) {
+                if (userGroups.contains(g)) {
+                    return;
+                }
+            }
+            throw new AccessControlException(
+                MessageFormat.format("Unauthorized proxyuser [{0}] for user [{1}], not in proxyuser groups",
+                                     proxyUser, user));
+        }
+    }
+
+    private String normalizeHostname(String name) {
+        try {
+            InetAddress address = InetAddress.getByName(name);
+            return address.getCanonicalHostName();
+        }
+        catch (IOException ex) {
+            throw new AccessControlException(MessageFormat.format("Could not resolve host [{0}], {1}", name,
+                                                                  ex.getMessage()));
+        }
+    }
+
+    /**
+     * Destroys the service.
+     */
+    @Override
+    public void destroy() {
+    }
+
+}
diff --git core/src/main/java/org/apache/oozie/service/Services.java core/src/main/java/org/apache/oozie/service/Services.java
index de62b5a..f0e1a06 100644
--- core/src/main/java/org/apache/oozie/service/Services.java
+++ core/src/main/java/org/apache/oozie/service/Services.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -199,19 +199,7 @@ public class Services {
         log.trace("Initializing");
         SERVICES = this;
         try {
-            Class<? extends Service>[] serviceClasses = (Class<? extends Service>[]) conf.getClasses(
-                    CONF_SERVICE_CLASSES);
-            if (serviceClasses != null) {
-                for (Class<? extends Service> serviceClass : serviceClasses) {
-                    setService(serviceClass);
-                }
-            }
-            serviceClasses = (Class<? extends Service>[]) conf.getClasses(CONF_SERVICE_EXT_CLASSES);
-            if (serviceClasses != null) {
-                for (Class<? extends Service> serviceClass : serviceClasses) {
-                    setService(serviceClass);
-                }
-            }
+            loadServices();
         }
         catch (RuntimeException ex) {
             XLog.getLog(getClass()).fatal(ex.getMessage(), ex);
@@ -235,6 +223,66 @@ public class Services {
     }
 
     /**
+     * Loads the specified services.
+     *
+     * @param classes services classes to load.
+     * @param list    list of loaded service in order of appearance in the
+     *                configuration.
+     * @throws ServiceException thrown if a service class could not be loaded.
+     */
+    private void loadServices(Class[] classes, List<Service> list) throws ServiceException {
+        XLog log = new XLog(LogFactory.getLog(getClass()));
+        for (Class klass : classes) {
+            try {
+                Service service = (Service) klass.newInstance();
+                log.debug("Loading service [{0}] implementation [{1}]", service.getInterface(),
+                        service.getClass());
+                if (!service.getInterface().isInstance(service)) {
+                    throw new ServiceException(ErrorCode.E0101, klass, service.getInterface().getName());
+                }
+                list.add(service);
+            } catch (ServiceException ex) {
+                throw ex;
+            } catch (Exception ex) {
+                throw new ServiceException(ErrorCode.E0102, klass, ex.getMessage(), ex);
+            }
+        }
+    }
+
+    /**
+     * Loads services defined in <code>services</code> and
+     * <code>services.ext</code> and de-dups them.
+     *
+     * @return List of final services to initialize.
+     * @throws ServiceException throw if the services could not be loaded.
+     */
+    private void loadServices() throws ServiceException {
+        XLog log = new XLog(LogFactory.getLog(getClass()));
+        try {
+            Map<Class, Service> map = new LinkedHashMap<Class, Service>();
+            Class[] classes = conf.getClasses(CONF_SERVICE_CLASSES);
+            Class[] classesExt = conf.getClasses(CONF_SERVICE_EXT_CLASSES);
+            List<Service> list = new ArrayList<Service>();
+            loadServices(classes, list);
+            loadServices(classesExt, list);
+
+            //removing duplicate services, strategy: last one wins
+            for (Service service : list) {
+                if (map.containsKey(service.getInterface())) {
+                    log.debug("Replacing service [{0}] implementation [{1}]", service.getInterface(),
+                            service.getClass());
+                }
+                map.put(service.getInterface(), service);
+            }
+            for (Map.Entry<Class, Service> entry : map.entrySet()) {
+                setService(entry.getValue().getClass());
+            }
+        } catch (RuntimeException ex) {
+            throw new ServiceException(ErrorCode.E0103, ex.getMessage(), ex);
+        }
+    }
+
+    /**
      * Destroy all services.
      */
     public void destroy() {
diff --git core/src/main/java/org/apache/oozie/service/WorkflowAppService.java core/src/main/java/org/apache/oozie/service/WorkflowAppService.java
index 99d212b..eb8d2ab 100644
--- core/src/main/java/org/apache/oozie/service/WorkflowAppService.java
+++ core/src/main/java/org/apache/oozie/service/WorkflowAppService.java
@@ -98,7 +98,7 @@ public abstract class WorkflowAppService implements Service {
      * @return workflow definition.
      * @throws WorkflowException thrown if the definition could not be read.
      */
-    protected String readDefinition(String appPath, String user, String group, String autToken)
+    protected String readDefinition(String appPath, String user, String group, String autToken, Configuration conf)
             throws WorkflowException {
         try {
             URI uri = new URI(appPath);
@@ -189,11 +189,6 @@ public abstract class WorkflowAppService implements Service {
                 }
             }
 
-            if (systemLibPath != null && jobConf.getBoolean(OozieClient.USE_SYSTEM_LIBPATH, false)) {
-                List<String> libFilePaths = getLibFiles(fs, systemLibPath);
-                filePaths.addAll(libFilePaths);
-            }
-
             conf.setStrings(APP_LIB_PATH_LIST, filePaths.toArray(new String[filePaths.size()]));
 
             //Add all properties start with 'oozie.'
@@ -278,4 +273,13 @@ public abstract class WorkflowAppService implements Service {
             return true;
         }
     }
+
+    /**
+     * Returns Oozie system libpath.
+     *
+     * @return Oozie system libpath (sharelib) in HDFS if present, otherwise it returns <code>NULL</code>.
+     */
+    public Path getSystemLibPath() {
+        return systemLibPath;
+    }
 }
diff --git core/src/main/java/org/apache/oozie/servlet/AuthFilter.java core/src/main/java/org/apache/oozie/servlet/AuthFilter.java
new file mode 100644
index 0000000..9fb7147
--- /dev/null
+++ core/src/main/java/org/apache/oozie/servlet/AuthFilter.java
@@ -0,0 +1,134 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.servlet;
+
+import org.apache.hadoop.security.authentication.server.AuthenticationFilter;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.oozie.service.Services;
+
+import javax.servlet.FilterChain;
+import javax.servlet.FilterConfig;
+import javax.servlet.ServletException;
+import javax.servlet.ServletRequest;
+import javax.servlet.ServletResponse;
+import javax.servlet.http.HttpServlet;
+import javax.servlet.http.HttpServletRequest;
+import java.io.IOException;
+import java.util.Map;
+import java.util.Properties;
+
+/**
+ * Authentication filter that extends Hadoop-auth AuthenticationFilter to override
+ * the configuration loading.
+ */
+public class AuthFilter extends AuthenticationFilter {
+    private static final String OOZIE_PREFIX = "oozie.authentication.";
+
+    private HttpServlet optionsServlet;
+
+    /**
+     * Initialize the filter.
+     *
+     * @param filterConfig filter configuration.
+     * @throws ServletException thrown if the filter could not be initialized.
+     */
+    @Override
+    public void init(FilterConfig filterConfig) throws ServletException {
+        super.init(filterConfig);
+        optionsServlet = new HttpServlet() {};
+        optionsServlet.init();
+    }
+
+    /**
+     * Destroy the filter.
+     */
+    @Override
+    public void destroy() {
+        optionsServlet.destroy();
+        super.destroy();
+    }
+
+    /**
+     * Returns the configuration from Oozie configuration to be used by the authentication filter.
+     * <p/>
+     * All properties from Oozie configuration which name starts with {@link #OOZIE_PREFIX} will
+     * be returned. The keys of the returned properties are trimmed from the {@link #OOZIE_PREFIX}
+     * prefix, for example the Oozie configuration property name 'oozie.authentication.type' will
+     * be just 'type'.
+     *
+     * @param configPrefix configuration prefix, this parameter is ignored by this implementation.
+     * @param filterConfig filter configuration, this parameter is ignored by this implementation.
+     * @return all Oozie configuration properties prefixed with {@link #OOZIE_PREFIX}, without the
+     * prefix.
+     */
+    @Override
+    protected Properties getConfiguration(String configPrefix, FilterConfig filterConfig) {
+        Properties props = new Properties();
+        Configuration conf = Services.get().getConf();
+
+        //setting the cookie path to root '/' so it is used for all resources.
+        props.setProperty(AuthenticationFilter.COOKIE_PATH, "/");
+
+        for (Map.Entry<String, String> entry : conf) {
+            String name = entry.getKey();
+            if (name.startsWith(OOZIE_PREFIX)) {
+                String value = conf.get(name);
+                name = name.substring(OOZIE_PREFIX.length());
+                props.setProperty(name, value);
+            }
+        }
+
+        return props;
+    }
+
+    /**
+     * Enforces authentication using Hadoop-auth AuthenticationFilter.
+     * <p/>
+     * This method is overriden to respond to HTTP OPTIONS requests for authenticated calls, regardless
+     * of the target servlet supporting OPTIONS or not and to inject the authenticated user name as
+     * request attribute for Oozie to retrieve the user id.
+     *
+     * @param request http request.
+     * @param response http response.
+     * @param filterChain filter chain.
+     * @throws IOException thrown if an IO error occurs.
+     * @throws ServletException thrown if a servlet error occurs.
+     */
+    @Override
+    public void doFilter(final ServletRequest request, final ServletResponse response, final FilterChain filterChain)
+            throws IOException, ServletException {
+
+        FilterChain filterChainWrapper = new FilterChain() {
+            @Override
+            public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse)
+                    throws IOException, ServletException {
+                HttpServletRequest httpRequest = (HttpServletRequest) servletRequest;
+                if (httpRequest.getMethod().equals("OPTIONS")) {
+                    optionsServlet.service(request, response);
+                }
+                else {
+                  httpRequest.setAttribute(JsonRestServlet.USER_NAME, httpRequest.getRemoteUser());
+                  filterChain.doFilter(servletRequest, servletResponse);
+                }
+            }
+        };
+
+        super.doFilter(request, response, filterChainWrapper);
+    }
+
+}
diff --git core/src/main/java/org/apache/oozie/servlet/HostnameFilter.java core/src/main/java/org/apache/oozie/servlet/HostnameFilter.java
new file mode 100644
index 0000000..ee6c71a
--- /dev/null
+++ core/src/main/java/org/apache/oozie/servlet/HostnameFilter.java
@@ -0,0 +1,91 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.oozie.servlet;
+
+import javax.servlet.Filter;
+import javax.servlet.FilterChain;
+import javax.servlet.FilterConfig;
+import javax.servlet.ServletException;
+import javax.servlet.ServletRequest;
+import javax.servlet.ServletResponse;
+import java.io.IOException;
+import java.net.InetAddress;
+
+/**
+ * Filter that resolves the requester hostname.
+ */
+public class HostnameFilter implements Filter {
+    static final ThreadLocal<String> HOSTNAME_TL = new ThreadLocal<String>();
+
+    /**
+     * Initializes the filter.
+     * <p/>
+     * This implementation is a NOP.
+     *
+     * @param config filter configuration.
+     *
+     * @throws ServletException thrown if the filter could not be initialized.
+     */
+    @Override
+    public void init(FilterConfig config) throws ServletException {
+    }
+
+    /**
+     * Resolves the requester hostname and delegates the request to the chain.
+     * <p/>
+     * The requester hostname is available via the {@link #get} method.
+     *
+     * @param request servlet request.
+     * @param response servlet response.
+     * @param chain filter chain.
+     *
+     * @throws IOException thrown if an IO error occurrs.
+     * @throws ServletException thrown if a servet error occurrs.
+     */
+    @Override
+    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)
+        throws IOException, ServletException {
+        try {
+            String hostname = InetAddress.getByName(request.getRemoteAddr()).getCanonicalHostName();
+            HOSTNAME_TL.set(hostname);
+            chain.doFilter(request, response);
+        }
+        finally {
+            HOSTNAME_TL.remove();
+        }
+    }
+
+    /**
+     * Returns the requester hostname.
+     *
+     * @return the requester hostname.
+     */
+    public static String get() {
+        return HOSTNAME_TL.get();
+    }
+
+    /**
+     * Destroys the filter.
+     * <p/>
+     * This implementation is a NOP.
+     */
+    @Override
+    public void destroy() {
+    }
+}
diff --git core/src/main/java/org/apache/oozie/servlet/JobServlet.java core/src/main/java/org/apache/oozie/servlet/JobServlet.java
index 727a1ff..b97a716 100644
--- core/src/main/java/org/apache/oozie/servlet/JobServlet.java
+++ core/src/main/java/org/apache/oozie/servlet/JobServlet.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -37,6 +37,8 @@ import org.apache.oozie.client.OozieClient;
 import org.apache.oozie.client.rest.JsonTags;
 import org.apache.oozie.client.rest.JsonWorkflowJob;
 import org.apache.oozie.client.rest.RestConstants;
+import org.apache.oozie.command.CommandException;
+import org.apache.oozie.command.coord.CoordRerunXCommand;
 import org.apache.oozie.service.AuthorizationException;
 import org.apache.oozie.service.AuthorizationService;
 import org.apache.oozie.service.CoordinatorEngineService;
@@ -128,9 +130,19 @@ public class JobServlet extends JsonRestServlet {
                 String scope = request.getParameter(RestConstants.JOB_COORD_RERUN_SCOPE_PARAM);
                 String refresh = request.getParameter(RestConstants.JOB_COORD_RERUN_REFRESH_PARAM);
                 String noCleanup = request.getParameter(RestConstants.JOB_COORD_RERUN_NOCLEANUP_PARAM);
+                if (!(rerunType.equals(RestConstants.JOB_COORD_RERUN_DATE) || rerunType
+                        .equals(RestConstants.JOB_COORD_RERUN_ACTION))) {
+                    throw new CommandException(ErrorCode.E1018, "date or action expected.");
+                }
                 CoordinatorActionInfo coordInfo = coordEngine.reRun(jobId, rerunType, scope, Boolean.valueOf(refresh),
                         Boolean.valueOf(noCleanup));
-                List<CoordinatorActionBean> actions = coordInfo.getCoordActions();
+                List<CoordinatorActionBean> actions;
+                if (coordInfo != null) {
+                    actions = coordInfo.getCoordActions();
+                }
+                else {
+                    actions = CoordRerunXCommand.getCoordActions(rerunType, jobId, scope);
+                }
                 JSONObject json = new JSONObject();
                 json.put(JsonTags.COORDINATOR_ACTIONS, CoordinatorActionBean.toJSONArray(actions));
                 startCron();
@@ -141,6 +153,9 @@ public class JobServlet extends JsonRestServlet {
                         RestConstants.ACTION_PARAM, action);
             }
         }
+        catch (CommandException ex) {
+            throw new XServletException(HttpServletResponse.SC_BAD_REQUEST, ex);
+        }
         catch (DagEngineException ex) {
             throw new XServletException(HttpServletResponse.SC_BAD_REQUEST, ex);
         }
diff --git core/src/main/java/org/apache/oozie/servlet/JsonRestServlet.java core/src/main/java/org/apache/oozie/servlet/JsonRestServlet.java
index 46d3363..316cb7e 100644
--- core/src/main/java/org/apache/oozie/servlet/JsonRestServlet.java
+++ core/src/main/java/org/apache/oozie/servlet/JsonRestServlet.java
@@ -22,6 +22,7 @@ import org.apache.oozie.client.rest.JsonBean;
 import org.apache.oozie.client.rest.RestConstants;
 import org.apache.oozie.service.DagXLogInfoService;
 import org.apache.oozie.service.InstrumentationService;
+import org.apache.oozie.service.ProxyUserService;
 import org.apache.oozie.service.Services;
 import org.apache.oozie.service.XLogService;
 import org.apache.oozie.util.Instrumentation;
@@ -37,6 +38,7 @@ import javax.servlet.http.HttpServlet;
 import javax.servlet.http.HttpServletRequest;
 import javax.servlet.http.HttpServletResponse;
 import java.io.IOException;
+import java.security.AccessControlException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashMap;
@@ -278,7 +280,6 @@ public abstract class JsonRestServlet extends HttpServlet {
             validateRestUrl(request.getMethod(), getResourceName(request), request.getParameterMap());
             XLog.Info.get().clear();
             String user = getUser(request);
-            XLog.Info.get().setParameter(XLogService.USER, user);
             TOTAL_REQUESTS_SAMPLER_COUNTER.incrementAndGet();
             samplerCounter.incrementAndGet();
             super.service(request, response);
@@ -293,6 +294,13 @@ public abstract class JsonRestServlet extends HttpServlet {
             incrCounter(INSTR_TOTAL_FAILED_REQUESTS_COUNTER, 1);
             sendErrorResponse(response, ex.getHttpStatusCode(), ex.getErrorCode().toString(), ex.getMessage());
         }
+        catch (AccessControlException ex) {
+            XLog log = XLog.getLog(getClass());
+            log.error("URL[{0} {1}] error, {2}", request.getMethod(), getRequestUrl(request), ex.getMessage(), ex);
+            incrCounter(INSTR_TOTAL_FAILED_REQUESTS_COUNTER, 1);
+            sendErrorResponse(response, HttpServletResponse.SC_UNAUTHORIZED, ErrorCode.E1400.toString(),
+                              ex.getMessage());
+        }
         catch (RuntimeException ex) {
             XLog log = XLog.getLog(getClass());
             log.error("URL[{0} {1}] error, {2}", request.getMethod(), getRequestUrl(request), ex.getMessage(), ex);
@@ -520,6 +528,26 @@ public abstract class JsonRestServlet extends HttpServlet {
      */
     protected String getUser(HttpServletRequest request) {
         String userName = (String) request.getAttribute(USER_NAME);
+
+        String doAsUserName = request.getParameter(RestConstants.DO_AS_PARAM);
+        if (doAsUserName != null && !doAsUserName.equals(userName)) {
+            ProxyUserService proxyUser = Services.get().get(ProxyUserService.class);
+            try {
+                proxyUser.validate(userName, HostnameFilter.get(), doAsUserName);
+            }
+            catch (IOException ex) {
+                throw new RuntimeException(ex);
+            }
+            auditLog.info("Proxy user [{0}] DoAs user [{1}] Request [{2}]", userName, doAsUserName,
+                          getRequestUrl(request));
+
+            XLog.Info.get().setParameter(XLogService.USER, userName + " doAs " + doAsUserName);
+
+            userName = doAsUserName;
+        }
+        else {
+            XLog.Info.get().setParameter(XLogService.USER, userName);
+        }
         return (userName != null) ? userName : UNDEF;
     }
 
diff --git core/src/main/java/org/apache/oozie/servlet/V1JobServlet.java core/src/main/java/org/apache/oozie/servlet/V1JobServlet.java
index 885e6dc..7ae5b85 100644
--- core/src/main/java/org/apache/oozie/servlet/V1JobServlet.java
+++ core/src/main/java/org/apache/oozie/servlet/V1JobServlet.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -33,6 +33,8 @@ import org.apache.oozie.CoordinatorActionInfo;
 import org.apache.oozie.CoordinatorEngine;
 import org.apache.oozie.CoordinatorEngineException;
 import org.apache.oozie.command.CommandException;
+import org.apache.oozie.command.coord.CoordRerunXCommand;
+import org.apache.oozie.coord.CoordUtils;
 import org.apache.oozie.DagEngine;
 import org.apache.oozie.DagEngineException;
 import org.apache.oozie.ErrorCode;
@@ -51,6 +53,7 @@ import org.json.simple.JSONObject;
 public class V1JobServlet extends BaseJobServlet {
 
     private static final String INSTRUMENTATION_NAME = "v1job";
+    public static final String COORD_ACTIONS_DEFAULT_LENGTH = "oozie.coord.actions.default.length";
 
     public V1JobServlet() {
         super(INSTRUMENTATION_NAME);
@@ -614,18 +617,33 @@ public class V1JobServlet extends BaseJobServlet {
                         + refresh + ", noCleanup=" + noCleanup);
 
         try {
+            if (!(rerunType.equals(RestConstants.JOB_COORD_RERUN_DATE) || rerunType
+                    .equals(RestConstants.JOB_COORD_RERUN_ACTION))) {
+                throw new CommandException(ErrorCode.E1018, "date or action expected.");
+            }
             CoordinatorActionInfo coordInfo = coordEngine.reRun(jobId, rerunType, scope, Boolean.valueOf(refresh),
                     Boolean.valueOf(noCleanup));
-            List<CoordinatorActionBean> actions = coordInfo.getCoordActions();
-            json.put(JsonTags.COORDINATOR_ACTIONS, CoordinatorActionBean.toJSONArray(actions));
+            List<CoordinatorActionBean> coordActions;
+            if (coordInfo != null) {
+                coordActions = coordInfo.getCoordActions();
+            }
+            else {
+                coordActions = CoordRerunXCommand.getCoordActions(rerunType, jobId, scope);
+            }
+            json.put(JsonTags.COORDINATOR_ACTIONS, CoordinatorActionBean.toJSONArray(coordActions));
         }
         catch (BaseEngineException ex) {
             throw new XServletException(HttpServletResponse.SC_BAD_REQUEST, ex);
         }
+        catch (CommandException ex) {
+            throw new XServletException(HttpServletResponse.SC_BAD_REQUEST, ex);
+        }
 
         return json;
     }
 
+
+
     /**
      * Get workflow job
      *
@@ -699,9 +717,11 @@ public class V1JobServlet extends BaseJobServlet {
         String lenStr = request.getParameter(RestConstants.LEN_PARAM);
         int start = (startStr != null) ? Integer.parseInt(startStr) : 1;
         start = (start < 1) ? 1 : start;
+        // Get default number of coordinator actions to be retrieved
+        int defaultLen = Services.get().getConf().getInt(COORD_ACTIONS_DEFAULT_LENGTH, 1000);
         int len = (lenStr != null) ? Integer.parseInt(lenStr) : 0;
-        len = (len < 1) ? Integer.MAX_VALUE : len;
-        try {
+        len = (len < 1) ? defaultLen : len;
+    try {
             JsonCoordinatorJob coordJob = coordEngine.getCoordJob(jobId, start, len);
             jobBean = coordJob;
         }
diff --git core/src/main/java/org/apache/oozie/store/StoreStatusFilter.java core/src/main/java/org/apache/oozie/store/StoreStatusFilter.java
index 0e26ee2..088f30c 100644
--- core/src/main/java/org/apache/oozie/store/StoreStatusFilter.java
+++ core/src/main/java/org/apache/oozie/store/StoreStatusFilter.java
@@ -44,6 +44,7 @@ public class StoreStatusFilter {
         boolean isEnabled = false;
         boolean isFrequency = false;
         boolean isId = false;
+        boolean isUnit = false;
 
         int index = 0;
 
@@ -239,6 +240,40 @@ public class StoreStatusFilter {
                                 colArray.add(colVar);
                             }
                         }
+                        // Filter map has time unit filter specified
+                        else if (entry.getKey().equals(OozieClient.FILTER_UNIT)) {
+                            List<String> values = filter.get(OozieClient.FILTER_UNIT);
+                            colName = "timeUnitStr";
+                            for (int i = 0; i < values.size(); ++i) {
+                                colVar = colName + index;
+                                // This unit filter value is the first condition to be added to the where clause of
+                                // query
+                                if (!isEnabled && !isUnit) {
+                                    sb.append(seletStr).append(" where w.timeUnitStr IN (:timeUnitStr" + index);
+                                    isUnit = true;
+                                    isEnabled = true;
+                                } else {
+                                    // Unit filter is neither the first nor the last condition to be added to the where
+                                    // clause of query
+                                    if (isEnabled && !isUnit) {
+                                        sb.append(" and w.timeUnitStr IN (:timeUnitStr" + index);
+                                        isUnit = true;
+                                    } else {
+                                        if (isUnit) {
+                                            sb.append(", :timeUnitStr" + index);
+                                        }
+                                    }
+                                }
+                                // This unit filter value is the last condition to be added to the where clause of query
+                                if (i == values.size() - 1) {
+                                    sb.append(")");
+                                }
+                                ++index;
+                                valArray.add(values.get(i));
+                                orArray.add(colName);
+                                colArray.add(colVar);
+                            }
+                        }
                     }
                 }
             }
diff --git core/src/main/java/org/apache/oozie/store/WorkflowStore.java core/src/main/java/org/apache/oozie/store/WorkflowStore.java
index 06ed913..f29b7c1 100644
--- core/src/main/java/org/apache/oozie/store/WorkflowStore.java
+++ core/src/main/java/org/apache/oozie/store/WorkflowStore.java
@@ -921,6 +921,8 @@ public class WorkflowStore extends Store {
             action.setConf(a.getConf());
             action.setConsoleUrl(a.getConsoleUrl());
             action.setData(a.getData());
+            action.setStats(a.getStats());
+            action.setExternalChildIDs(a.getExternalChildIDs());
             action.setErrorInfo(a.getErrorCode(), a.getErrorMessage());
             action.setExternalId(a.getExternalId());
             action.setExternalStatus(a.getExternalStatus());
@@ -975,6 +977,8 @@ public class WorkflowStore extends Store {
         q.setParameter("conf", aBean.getConf());
         q.setParameter("consoleUrl", aBean.getConsoleUrl());
         q.setParameter("data", aBean.getData());
+        q.setParameter("stats", aBean.getStats());
+        q.setParameter("externalChildIDs", aBean.getExternalChildIDs());
         q.setParameter("errorCode", aBean.getErrorCode());
         q.setParameter("errorMessage", aBean.getErrorMessage());
         q.setParameter("externalId", aBean.getExternalId());
diff --git core/src/main/java/org/apache/oozie/util/CoordActionsInDateRange.java core/src/main/java/org/apache/oozie/util/CoordActionsInDateRange.java
index 3b745c3..93751f8 100644
--- core/src/main/java/org/apache/oozie/util/CoordActionsInDateRange.java
+++ core/src/main/java/org/apache/oozie/util/CoordActionsInDateRange.java
@@ -18,6 +18,7 @@
 
 package org.apache.oozie.util;
 
+import java.text.ParseException;
 import java.util.ArrayList;
 import java.util.Date;
 import java.util.HashSet;
@@ -59,25 +60,8 @@ public class CoordActionsInDateRange {
         String[] list = scope.split(",");
         for (String s : list) {
             s = s.trim();
-            // This block checks for errors in the format of specifying date range
             if (s.contains("::")) {
-                String[] dateRange = s.split("::");
-                if (dateRange.length != 2) {
-                    throw new XException(ErrorCode.E0308, "'" + s + "'. Date value expected on both sides of the scope resolution operator '::' to signify start and end of range");
-                }
-                Date start;
-                Date end;
-                try {
-                start = DateUtils.parseDateUTC(dateRange[0].trim());
-                end = DateUtils.parseDateUTC(dateRange[1].trim());
-                }
-                catch (Exception dx) {
-                    throw new XException(ErrorCode.E0308, "Error in parsing start or end date");
-                }
-                if (start.after(end)) {
-                    throw new XException(ErrorCode.E0308, "'" + s + "'. Start date '" + start + "' is older than end date: '" + end + "'");
-                }
-                List<CoordinatorActionBean> listOfActions = getActionIdsFromDateRange(jobId, start, end);
+                List<CoordinatorActionBean> listOfActions = getCoordActionsFromDateRange(jobId, s);
                 actionSet.addAll(listOfActions);
             }
             else {
@@ -91,6 +75,36 @@ public class CoordActionsInDateRange {
         return coordActions;
     }
 
+    /**
+     * Get the coordinator actions for a given date range
+     * @param jobId the coordinator job id
+     * @param range the date range separated by '::'
+     * @return the list of Coordinator actions for the date range
+     * @throws XException
+     */
+    public static List<CoordinatorActionBean> getCoordActionsFromDateRange(String jobId, String range) throws XException{
+            String[] dateRange = range.split("::");
+            // This block checks for errors in the format of specifying date range
+            if (dateRange.length != 2) {
+                throw new XException(ErrorCode.E0308, "'" + range + "'. Date value expected on both sides of the scope resolution operator '::' to signify start and end of range");
+            }
+            Date start;
+            Date end;
+            try {
+            // Get the start and end dates for the range
+                start = DateUtils.parseDateUTC(dateRange[0].trim());
+                end = DateUtils.parseDateUTC(dateRange[1].trim());
+            }
+            catch (ParseException dx) {
+                throw new XException(ErrorCode.E0308, "Error in parsing start or end date. " + dx);
+            }
+            if (start.after(end)) {
+                throw new XException(ErrorCode.E0308, "'" + range + "'. Start date '" + start + "' is older than end date: '" + end + "'");
+            }
+            List<CoordinatorActionBean> listOfActions = getActionIdsFromDateRange(jobId, start, end);
+            return listOfActions;
+    }
+
     /*
      * Get coordinator action ids between given start and end time
      *
diff --git core/src/main/java/org/apache/oozie/util/DateUtils.java core/src/main/java/org/apache/oozie/util/DateUtils.java
index 4beffe9..49fcee7 100644
--- core/src/main/java/org/apache/oozie/util/DateUtils.java
+++ core/src/main/java/org/apache/oozie/util/DateUtils.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -19,6 +19,7 @@ package org.apache.oozie.util;
 
 import java.sql.Timestamp;
 import java.text.DateFormat;
+import java.text.ParseException;
 import java.text.ParsePosition;
 import java.text.SimpleDateFormat;
 import java.util.Calendar;
@@ -101,7 +102,7 @@ public class DateUtils {
         dateFormat.setTimeZone(UTC);
         return dateFormat;
     }
-    
+
     private static DateFormat getSpecificDateFormat(String format) {
         DateFormat dateFormat = new SimpleDateFormat(format);
         dateFormat.setTimeZone(UTC);
@@ -119,7 +120,7 @@ public class DateUtils {
         return tz;
     }
 
-    public static Date parseDateUTC(String s) throws Exception {
+    public static Date parseDateUTC(String s) throws ParseException {
         return getISO8601DateFormat().parse(s);
     }
 
diff --git core/src/main/java/org/apache/oozie/workflow/lite/LiteWorkflowAppParser.java core/src/main/java/org/apache/oozie/workflow/lite/LiteWorkflowAppParser.java
index d16e932..fc85d2b 100644
--- core/src/main/java/org/apache/oozie/workflow/lite/LiteWorkflowAppParser.java
+++ core/src/main/java/org/apache/oozie/workflow/lite/LiteWorkflowAppParser.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -38,8 +38,11 @@ import java.io.StringReader;
 import java.io.StringWriter;
 import java.util.ArrayList;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
+import java.util.Stack;
 
 /**
  * Class to parse and validate workflow xml
@@ -74,6 +77,7 @@ public class LiteWorkflowAppParser {
     private static final String DECISION_DEFAULT_E = "default";
 
     private static final String KILL_MESSAGE_E = "message";
+    public static final String VALIDATE_FORK_JOIN = "oozie.validate.ForkJoin";
 
     private Schema schema;
     private Class<? extends DecisionNodeHandler> decisionHandlerClass;
@@ -83,8 +87,8 @@ public class LiteWorkflowAppParser {
         VISITING, VISITED
     }
 
-    ;
-
+    private List<String> forkList = new ArrayList<String>();
+    private List<String> joinList = new ArrayList<String>();
 
     public LiteWorkflowAppParser(Schema schema, Class<? extends DecisionNodeHandler> decisionHandlerClass,
                                  Class<? extends ActionNodeHandler> actionHandlerClass) throws WorkflowException {
@@ -116,6 +120,10 @@ public class LiteWorkflowAppParser {
             Map<String, VisitStatus> traversed = new HashMap<String, VisitStatus>();
             traversed.put(app.getNode(StartNodeDef.START).getName(), VisitStatus.VISITING);
             validate(app, app.getNode(StartNodeDef.START), traversed);
+            //Validate whether fork/join are in pair or not
+            if (Services.get().getConf().getBoolean(VALIDATE_FORK_JOIN, true)) {
+                validateForkJoin(app);
+            }
             return app;
         }
         catch (JDOMException ex) {
@@ -130,6 +138,90 @@ public class LiteWorkflowAppParser {
     }
 
     /**
+     * Validate whether fork/join are in pair or not
+     * @param app LiteWorkflowApp
+     * @throws WorkflowException
+     */
+    private void validateForkJoin(LiteWorkflowApp app) throws WorkflowException {
+        // Make sure the number of forks and joins in wf are equal
+        if (forkList.size() != joinList.size()) {
+            throw new WorkflowException(ErrorCode.E0730);
+        }
+
+        while(!forkList.isEmpty()){
+            // Make sure each of the fork node has a corresponding join; start with the root fork Node first
+            validateFork(app.getNode(forkList.remove(0)), app);
+        }
+
+    }
+
+    /*
+     * Test whether the fork node has a corresponding join
+     * @param node - the fork node
+     * @param app - the WorkflowApp
+     * @return
+     * @throws WorkflowException
+     */
+    private NodeDef validateFork(NodeDef forkNode, LiteWorkflowApp app) throws WorkflowException {
+        List<String> transitions = new ArrayList<String>(forkNode.getTransitions());
+        String joinNode = null;
+        for (int i = 0; i < transitions.size(); i++) {
+            NodeDef node = app.getNode(transitions.get(i));
+            if (node instanceof DecisionNodeDef) {
+                Set<String> decisionSet = new HashSet<String>(node.getTransitions());
+                for (String ds : decisionSet) {
+                    if (transitions.contains(ds)) {
+                        throw new WorkflowException(ErrorCode.E0734, node.getName(), ds);
+                    } else {
+                        transitions.add(ds);
+                    }
+                }
+            } else if (node instanceof ActionNodeDef) {
+                // Get only the "ok-to" transition of node
+                String okToTransition = node.getTransitions().get(0);
+                // Make sure the transition is valid
+                validateTransition(transitions, app, okToTransition, node);
+                transitions.add(okToTransition);
+            } else if (node instanceof ForkNodeDef) {
+                forkList.remove(node.getName());
+                // Make a recursive call to resolve this fork node
+                NodeDef joinNd = validateFork(node, app);
+                String okToTransition = joinNd.getTransitions().get(0);
+                // Make sure the transition is valid
+                validateTransition(transitions, app, okToTransition, node);
+                transitions.add(okToTransition);
+            } else if (node instanceof JoinNodeDef) {
+                // If joinNode encountered for the first time, remove it from the joinList and remember it
+                String currentJoin = node.getName();
+                if (joinList.contains(currentJoin)) {
+                    joinList.remove(currentJoin);
+                    joinNode = currentJoin;
+                } else {
+                    // Make sure this join is the same as the join seen from the first time
+                    if (joinNode == null) {
+                        throw new WorkflowException(ErrorCode.E0733, forkNode);
+                    }
+                    if (!joinNode.equals(currentJoin)) {
+                        throw new WorkflowException(ErrorCode.E0732, forkNode, joinNode);
+                    }
+                }
+            } else {
+                throw new WorkflowException(ErrorCode.E0730);
+            }
+        }
+        return app.getNode(joinNode);
+
+    }
+
+    private void validateTransition(List<String> transitions, LiteWorkflowApp app, String okToTransition, NodeDef node)
+            throws WorkflowException {
+        // Make sure the transition node is either a join node or is not already visited
+        if (transitions.contains(okToTransition) && !(app.getNode(okToTransition) instanceof JoinNodeDef)) {
+            throw new WorkflowException(ErrorCode.E0734, node.getName(), okToTransition);
+        }
+    }
+
+    /**
      * Parse xml to {@link LiteWorkflowApp}
      *
      * @param strDef
@@ -201,11 +293,11 @@ public class LiteWorkflowAppParser {
                                                 }
                                             }
                                         }
-                                        
+
                                         String credStr = eNode.getAttributeValue(CRED_A);
                                         String userRetryMaxStr = eNode.getAttributeValue(USER_RETRY_MAX_A);
                                         String userRetryIntervalStr = eNode.getAttributeValue(USER_RETRY_INTERVAL_A);
-                                        
+
                                         String actionConf = XmlUtils.prettyPrint(eActionConf).toString();
                                         def.addNode(new ActionNodeDef(eNode.getAttributeValue(NAME_A), actionConf, actionHandlerClass,
                                                                       transitions[0], transitions[1], credStr,
@@ -259,6 +351,14 @@ public class LiteWorkflowAppParser {
             }
         }
 
+        if(node instanceof ForkNodeDef){
+            forkList.add(node.getName());
+        }
+
+        if(node instanceof JoinNodeDef){
+            joinList.add(node.getName());
+        }
+
         if (node instanceof EndNodeDef) {
             traversed.put(node.getName(), VisitStatus.VISITED);
             return;
diff --git core/src/main/resources/oozie-default.xml core/src/main/resources/oozie-default.xml
index 65973cd..f9e504d 100644
--- core/src/main/resources/oozie-default.xml
+++ core/src/main/resources/oozie-default.xml
@@ -89,7 +89,9 @@
             org.apache.oozie.service.DagEngineService,
             org.apache.oozie.service.CoordMaterializeTriggerService,
             org.apache.oozie.service.StatusTransitService,
-            org.apache.oozie.service.PauseTransitService
+            org.apache.oozie.service.PauseTransitService,
+            org.apache.oozie.service.GroupsService,
+            org.apache.oozie.service.ProxyUserService
         </value>
         <description>
             All services to be created and managed by Oozie Services singleton.
@@ -321,7 +323,15 @@
 		<description>Default maximum timeout for a coordinator action input check (in minutes). 86400= 60days
         </description>
 	</property>
-	
+
+	<property>
+		<name>oozie.service.coord.input.check.requeue.interval
+		</name>
+		<value>60000</value>
+		<description>Command re-queue interval for coordinator data input check (in millisecond).
+        </description>
+	</property>
+
 	<property>
 		<name>oozie.service.coord.default.concurrency
 		</name>
@@ -1091,6 +1101,16 @@
         </description>
     </property>
 
+    <!-- External stats-->
+
+    <property>
+        <name>oozie.external.stats.max.size</name>
+        <value>-1</value>
+        <description>
+            Max size in bytes for action stats. -1 means infinite value.
+        </description>
+    </property>
+
     <!-- JobCommand -->
 
     <property>
@@ -1400,4 +1420,88 @@
         </description>
     </property>
 
+    <!-- Oozie Authentication -->
+
+    <property>
+        <name>oozie.authentication.type</name>
+        <value>simple</value>
+        <description>
+            Defines authentication used for Oozie HTTP endpoint.
+            Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#
+        </description>
+    </property>
+
+    <property>
+        <name>oozie.authentication.token.validity</name>
+        <value>36000</value>
+        <description>
+            Indicates how long (in seconds) an authentication token is valid before it has
+            to be renewed.
+        </description>
+    </property>
+
+    <property>
+        <name>oozie.authentication.signature.secret</name>
+        <value>oozie</value>
+        <description>
+            The signature secret for signing the authentication tokens.
+            If not set a random secret is generated at startup time.
+            In order to authentiation to work correctly across multiple hosts
+            the secret must be the same across al the hosts.
+        </description>
+    </property>
+
+    <property>
+      <name>oozie.authentication.cookie.domain</name>
+      <value></value>
+      <description>
+        The domain to use for the HTTP cookie that stores the authentication token.
+        In order to authentiation to work correctly across multiple hosts
+        the domain must be correctly set.
+      </description>
+    </property>
+
+    <property>
+        <name>oozie.authentication.simple.anonymous.allowed</name>
+        <value>true</value>
+        <description>
+            Indicates if anonymous requests are allowed when using 'simple' authentication.
+        </description>
+    </property>
+
+    <property>
+        <name>oozie.authentication.kerberos.principal</name>
+        <value>HTTP/localhost@${local.realm}</value>
+        <description>
+            Indicates the Kerberos principal to be used for HTTP endpoint.
+            The principal MUST start with 'HTTP/' as per Kerberos HTTP SPNEGO specification.
+        </description>
+    </property>
+
+    <property>
+        <name>oozie.authentication.kerberos.keytab</name>
+        <value>${oozie.service.HadoopAccessorService.keytab.file}</value>
+        <description>
+            Location of the keytab file with the credentials for the principal.
+            Referring to the same keytab file Oozie uses for its Kerberos credentials for Hadoop.
+        </description>
+    </property>
+	<!-- Coordinator Actions default length -->
+	<property>
+		<name>oozie.coord.actions.default.length</name>
+		<value>1000</value>
+		<description>
+			Default number of coordinator actions to be retrieved by the info command
+		</description>
+	</property>
+
+	<!-- ForkJoin validation -->
+	<property>
+		<name>oozie.validate.ForkJoin</name>
+		<value>true</value>
+		<description>
+			If true, fork and join should be validated at wf submission time.
+		</description>
+	</property>
+
 </configuration>
diff --git core/src/test/java/org/apache/oozie/TestActionBean.java core/src/test/java/org/apache/oozie/TestActionBean.java
index 4d06c7c..23a3604 100644
--- core/src/test/java/org/apache/oozie/TestActionBean.java
+++ core/src/test/java/org/apache/oozie/TestActionBean.java
@@ -56,6 +56,11 @@ public class TestActionBean extends XTestCase {
         assertEquals("externalId", action.getExternalId());
         assertEquals("trackerUri", action.getTrackerUri());
         assertEquals("consoleUrl", action.getConsoleUrl());
+
+        action.setStats("jsonStats");
+        action.setExternalChildIDs("job1,job2");
+        assertEquals("jsonStats", action.getStats());
+        assertEquals("job1,job2", action.getExternalChildIDs());
     }
 
     public void testEmptyWriteRead() throws Exception {
diff --git core/src/test/java/org/apache/oozie/TestCoordinatorEngine.java core/src/test/java/org/apache/oozie/TestCoordinatorEngine.java
index 8cb4403..8ae6496 100644
--- core/src/test/java/org/apache/oozie/TestCoordinatorEngine.java
+++ core/src/test/java/org/apache/oozie/TestCoordinatorEngine.java
@@ -167,7 +167,6 @@ public class TestCoordinatorEngine extends XTestCase {
             }
         });
 
-
         List<CoordinatorAction> actions = ce.getCoordJob(jobId).getActions();
         assertTrue(actions.size() > 0);
         CoordinatorAction action = actions.get(0);
@@ -176,7 +175,6 @@ public class TestCoordinatorEngine extends XTestCase {
         assertEquals("file://" + getTestCaseDir() + "/workflows/2009/02/01/consume_me", missingDeps);
     }
 
-
     /**
      * Test Missing Dependencies with Empty Done Flag in Schema
      *
@@ -224,7 +222,6 @@ public class TestCoordinatorEngine extends XTestCase {
             }
         });
 
-
         List<CoordinatorAction> actions = ce.getCoordJob(jobId).getActions();
         assertTrue(actions.size() > 0);
         CoordinatorAction action = actions.get(0);
@@ -233,7 +230,6 @@ public class TestCoordinatorEngine extends XTestCase {
         assertEquals("file://" + getTestCaseDir() + "/workflows/2009/02/01", missingDeps);
     }
 
-
     /**
      * Test Missing Dependencies with Done Flag in Schema
      *
@@ -329,7 +325,7 @@ public class TestCoordinatorEngine extends XTestCase {
         conf.set(OozieClient.USER_NAME, getTestUser());
         conf.set(OozieClient.GROUP_NAME, "other");
         injectKerberosInfo(conf);
-        
+
         final CoordinatorEngine ce = new CoordinatorEngine(getTestUser(), "UNIT_TESTING");
         final String jobId = ce.submitJob(conf, true);
         waitFor(5000, new Predicate() {
@@ -355,14 +351,46 @@ public class TestCoordinatorEngine extends XTestCase {
         assertEquals(job.getAppPath(), appPath);
     }
 
+    /**
+     * Test to validate frequency and time unit filters for jobs
+     *
+     * @throws Exception
+     */
     public void _testGetJobs(String jobId) throws Exception {
         CoordinatorEngine ce = new CoordinatorEngine(getTestUser(), "UNIT_TESTING");
-        CoordinatorJobInfo jobInfo = ce.getCoordJobs("", 1, 10); // TODO: use
-        // valid
-        // filter
+        // Test with no job filter specified
+        CoordinatorJobInfo jobInfo = ce.getCoordJobs("", 1, 10);
         assertEquals(1, jobInfo.getCoordJobs().size());
         CoordinatorJob job = jobInfo.getCoordJobs().get(0);
         assertEquals(jobId, job.getId());
+
+        // Test specifying the value for unit but leaving out the value for frequency
+        try {
+            jobInfo = ce.getCoordJobs("unit=minutes", 1, 10);
+        }
+        catch (CoordinatorEngineException ex) {
+            assertEquals("E0420: Invalid jobs filter [unit=minutes], time unit should be added only when "
+                    + "frequency is specified. Either specify frequency also or else remove the time unit", ex
+                    .getMessage());
+        }
+
+        // Test for invalid frequency value(Non-numeric value)
+        try {
+            jobInfo = ce.getCoordJobs("frequency=ghj;unit=minutes", 1, 10);
+        }
+        catch (CoordinatorEngineException ex) {
+            assertEquals("E0420: Invalid jobs filter [frequency=ghj;unit=minutes], "
+                    + "invalid value [ghj] for frequency. A numerical value is expected", ex.getMessage());
+        }
+
+        // Test for invalid unit value(Other than months, days, minutes or hours)
+        try {
+            jobInfo = ce.getCoordJobs("frequency=60;unit=min", 1, 10);
+        }
+        catch (CoordinatorEngineException ex) {
+            assertEquals("E0420: Invalid jobs filter [frequency=60;unit=min], invalid value [min] for time unit. "
+                    + "Valid value is one of months, days, hours or minutes", ex.getMessage());
+        }
     }
 
     private void _testGetDefinition(String jobId) throws Exception {
diff --git core/src/test/java/org/apache/oozie/action/decision/TestDecisionActionExecutor.java core/src/test/java/org/apache/oozie/action/decision/TestDecisionActionExecutor.java
index 2177d02..01f61b0 100644
--- core/src/test/java/org/apache/oozie/action/decision/TestDecisionActionExecutor.java
+++ core/src/test/java/org/apache/oozie/action/decision/TestDecisionActionExecutor.java
@@ -82,6 +82,14 @@ public class TestDecisionActionExecutor extends XFsTestCase {
             executed = true;
         }
 
+        public void setExecutionStats(String jsonStats) {
+            action.setExecutionStats(jsonStats);
+        }
+
+        public void setExternalChildIDs(String externalChildIDs) {
+            action.setExternalChildIDs(externalChildIDs);
+        }
+
         public void setEndData(WorkflowAction.Status status, String signalValue) {
             action.setEndData(status, signalValue);
             ended = true;
diff --git core/src/test/java/org/apache/oozie/action/hadoop/ActionExecutorTestCase.java core/src/test/java/org/apache/oozie/action/hadoop/ActionExecutorTestCase.java
index b14a5b1..da50f3d 100644
--- core/src/test/java/org/apache/oozie/action/hadoop/ActionExecutorTestCase.java
+++ core/src/test/java/org/apache/oozie/action/hadoop/ActionExecutorTestCase.java
@@ -134,6 +134,22 @@ public abstract class ActionExecutorTestCase extends XFsTestCase {
             executed = true;
         }
 
+        public String getExecutionStats() {
+            return action.getExecutionStats();
+        }
+
+        public void setExecutionStats(String jsonStats) {
+            action.setExecutionStats(jsonStats);
+        }
+
+        public String getExternalChildIDs() {
+            return action.getExternalChildIDs();
+        }
+
+        public void setExternalChildIDs(String externalChildIDs) {
+            action.setExternalChildIDs(externalChildIDs);
+        }
+
         public void setEndData(WorkflowAction.Status status, String signalValue) {
             action.setEndData(status, signalValue);
             ended = true;
diff --git core/src/test/java/org/apache/oozie/action/hadoop/PigTestCase.java core/src/test/java/org/apache/oozie/action/hadoop/PigTestCase.java
index 9216caa..f24c3b6 100644
--- core/src/test/java/org/apache/oozie/action/hadoop/PigTestCase.java
+++ core/src/test/java/org/apache/oozie/action/hadoop/PigTestCase.java
@@ -35,14 +35,18 @@ import com.google.common.primitives.Booleans;
 
 public abstract class PigTestCase extends XFsTestCase implements Callable<Void> {
     protected static String pigScript;
+    protected static boolean writeStats;
     private static String commonPigScript = "set job.name 'test'\n" +
              "set debug on\n" +
              "A = load '$IN' using PigStorage(':');\n" +
              "B = foreach A generate $0 as id;\n" +
              "store B into '$OUT' USING PigStorage();";
 
+
+
     public void testPigScript() throws Exception {
         pigScript = commonPigScript;
+        writeStats = true;
         DoAs.call(getTestUser(), this);
 
     }
@@ -78,6 +82,8 @@ public abstract class PigTestCase extends XFsTestCase implements Callable<Void>
                 + "\noutput = " + "'"+outputDir.toUri().getPath()+"'"
                 + "\nQ = P.bind({'IN':input, 'OUT':output})"
                 + "\nQ.runSingle()";
+
+        writeStats = false;
         DoAs.call(getTestUser(), this);
     }
 
diff --git core/src/test/java/org/apache/oozie/action/hadoop/SharelibUtils.java core/src/test/java/org/apache/oozie/action/hadoop/SharelibUtils.java
new file mode 100644
index 0000000..845a202
--- /dev/null
+++ core/src/test/java/org/apache/oozie/action/hadoop/SharelibUtils.java
@@ -0,0 +1,105 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.action.hadoop;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.oozie.util.IOUtils;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileReader;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ *
+ */
+public class SharelibUtils {
+
+    private static String findDir(String name) throws Exception {
+        return findDir(new File("foo").getAbsoluteFile().getParent(), name);
+    }
+
+    private static String findDir(String baseDir, String name) throws Exception {
+        File dir = new File(baseDir, name).getAbsoluteFile();
+        if (!dir.exists()) {
+            File parent = dir.getParentFile().getParentFile();
+            if (parent != null) {
+                return findDir(parent.getAbsolutePath(), name);
+            }
+            else {
+                throw new RuntimeException("Sharelib dir not found: " + name);
+            }
+        }
+        return dir.getAbsolutePath();
+    }
+
+    private static String findClasspathFile(String sharelib) throws Exception {
+        String classpathFile = null;
+        String sharelibDir = findDir("sharelib");
+        if (sharelibDir != null) {
+            File file = new File(new File(new File(sharelibDir, sharelib), "target"), "classpath");
+            if (file.exists()) {
+                classpathFile = file.getAbsolutePath();
+            }
+            else {
+                throw new RuntimeException("Sharelib classpath file for '" + sharelib +
+                "' not found, Run 'mvn generate-test-resources' from Oozie source root");
+            }
+        }
+        return classpathFile;
+    }
+
+    private static String[] getSharelibJars(String sharelib) throws Exception {
+        String classpathFile = findClasspathFile(sharelib);
+        BufferedReader br = new BufferedReader(new FileReader(classpathFile));
+        String line = br.readLine();
+        br.close();
+        return line.split(System.getProperty("path.separator"));
+    }
+
+    private static Path[] copySharelibJarsToFileSytem(String sharelib, FileSystem fs, Path targetDir) throws Exception {
+        String[] jars = getSharelibJars(sharelib);
+        List<Path> paths = new ArrayList<Path>();
+        for (String jar : jars) {
+            if (jar.endsWith(".jar")) {
+                Path targetPath = new Path(targetDir, new File(jar).getName());
+                InputStream is = new FileInputStream(jar);
+                OutputStream os = fs.create(targetPath);
+                IOUtils.copyStream(is, os);
+                paths.add(targetPath);
+            }
+        }
+        return paths.toArray(new Path[paths.size()]);
+    }
+
+    public static void addToDistributedCache(String sharelib, FileSystem fs, Path targetDir,
+                                             Configuration conf) throws Exception {
+        Path[] paths = copySharelibJarsToFileSytem(sharelib, fs, targetDir);
+        for (Path path : paths) {
+            DistributedCache.addFileToClassPath(path, conf, fs);
+        }
+    }
+
+}
diff --git core/src/test/java/org/apache/oozie/action/hadoop/ShellTestCase.java core/src/test/java/org/apache/oozie/action/hadoop/ShellTestCase.java
new file mode 100644
index 0000000..059897e
--- /dev/null
+++ core/src/test/java/org/apache/oozie/action/hadoop/ShellTestCase.java
@@ -0,0 +1,56 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.oozie.action.hadoop;
+
+import java.util.concurrent.Callable;
+
+import org.apache.oozie.test.XFsTestCase;
+
+public abstract class ShellTestCase extends XFsTestCase implements Callable<Void> {
+    protected static String scriptContent = "";
+    protected static String scriptName = "";
+    protected boolean expectedSuccess = true;
+
+    private static final String SUCCESS_SHELL_SCRIPT_CONTENT = "ls -ltr\necho $1 $2\necho $PATH\npwd\ntype sh";
+    private static final String FAIL_SHELLSCRIPT_CONTENT = "ls -ltr\necho $1 $2\nexit 1";
+
+    /**
+     * Test a shell script that returns success
+     *
+     * @throws Exception
+     */
+    public void testShellScriptSuccess() throws Exception {
+        scriptContent = SUCCESS_SHELL_SCRIPT_CONTENT;
+        scriptName = "script.sh";
+        expectedSuccess = true;
+        DoAs.call(getTestUser(), this);
+    }
+
+    /**
+     * Test a shell script that returns failure
+     *
+     * @throws Exception
+     */
+    public void testShellScriptFailure() throws Exception {
+        scriptContent = FAIL_SHELLSCRIPT_CONTENT;
+        scriptName = "script.sh";
+        expectedSuccess = false;
+        DoAs.call(getTestUser(), this);
+    }
+}
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestDistCpActionExecutor.java core/src/test/java/org/apache/oozie/action/hadoop/TestDistCpActionExecutor.java
index 6da8c58..d006e71 100644
--- core/src/test/java/org/apache/oozie/action/hadoop/TestDistCpActionExecutor.java
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestDistCpActionExecutor.java
@@ -31,6 +31,8 @@ import org.apache.oozie.WorkflowActionBean;
 import org.apache.oozie.WorkflowJobBean;
 import org.apache.oozie.client.OozieClient;
 import org.apache.oozie.client.WorkflowAction;
+import org.apache.oozie.service.HadoopAccessorService;
+import org.apache.oozie.service.Services;
 import org.apache.oozie.service.WorkflowAppService;
 import org.apache.oozie.util.IOUtils;
 import org.apache.oozie.util.XConfiguration;
@@ -108,7 +110,8 @@ public class TestDistCpActionExecutor extends ActionExecutorTestCase{
         JobConf jobConf = new JobConf();
         jobConf.set("mapred.job.tracker", jobTracker);
         injectKerberosInfo(jobConf);
-        JobClient jobClient = new JobClient(jobConf);
+        JobClient jobClient =
+            Services.get().get(HadoopAccessorService.class).createJobClient(getTestUser(), getTestGroup(), jobConf);
         final RunningJob runningJob = jobClient.getJob(JobID.forName(jobId));
         assertNotNull(runningJob);
         return runningJob;
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestHadoopELFunctions.java core/src/test/java/org/apache/oozie/action/hadoop/TestHadoopELFunctions.java
index 27a48cc..ebedffa 100644
--- core/src/test/java/org/apache/oozie/action/hadoop/TestHadoopELFunctions.java
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestHadoopELFunctions.java
@@ -36,17 +36,16 @@ import org.apache.oozie.workflow.lite.StartNodeDef;
 public class TestHadoopELFunctions extends ActionExecutorTestCase {
 
 
-    public void testCountersEL() throws Exception {
-        String counters = "{\"g\":{\"c\":10},\"org.apache.hadoop.mapred.JobInProgress$Counter\":" +
-                "{\"TOTAL_LAUNCHED_REDUCES\":1,\"TOTAL_LAUNCHED_MAPS\":2,\"DATA_LOCAL_MAPS\":2}," +
-                "\"FileSystemCounters\":{\"FILE_BYTES_READ\":38,\"HDFS_BYTES_READ\":19," +
-                "\"FILE_BYTES_WRITTEN\":146,\"HDFS_BYTES_WRITTEN\":16}," +
-                "\"org.apache.hadoop.mapred.Task$Counter\":{\"REDUCE_INPUT_GROUPS\":2," +
-                "\"COMBINE_OUTPUT_RECORDS\":0,\"MAP_INPUT_RECORDS\":2,\"REDUCE_SHUFFLE_BYTES\":22," +
-                "\"REDUCE_OUTPUT_RECORDS\":2,\"SPILLED_RECORDS\":4,\"MAP_OUTPUT_BYTES\":28," +
-                "\"MAP_INPUT_BYTES\":12,\"MAP_OUTPUT_RECORDS\":2,\"COMBINE_INPUT_RECORDS\":0," +
-                "\"REDUCE_INPUT_RECORDS\":2}}";
-
+    public void testELFunctionsReturningMapReduceStats() throws Exception {
+        String counters = "{\"g\":{\"c\":10},\"org.apache.hadoop.mapred.JobInProgress$Counter\":"
+                + "{\"TOTAL_LAUNCHED_REDUCES\":1,\"TOTAL_LAUNCHED_MAPS\":2,\"DATA_LOCAL_MAPS\":2},\"ACTION_TYPE\":\"MAP_REDUCE\","
+                + "\"FileSystemCounters\":{\"FILE_BYTES_READ\":38,\"HDFS_BYTES_READ\":19,"
+                + "\"FILE_BYTES_WRITTEN\":146,\"HDFS_BYTES_WRITTEN\":16},"
+                + "\"org.apache.hadoop.mapred.Task$Counter\":{\"REDUCE_INPUT_GROUPS\":2,"
+                + "\"COMBINE_OUTPUT_RECORDS\":0,\"MAP_INPUT_RECORDS\":2,\"REDUCE_SHUFFLE_BYTES\":22,"
+                + "\"REDUCE_OUTPUT_RECORDS\":2,\"SPILLED_RECORDS\":4,\"MAP_OUTPUT_BYTES\":28,"
+                + "\"MAP_INPUT_BYTES\":12,\"MAP_OUTPUT_RECORDS\":2,\"COMBINE_INPUT_RECORDS\":0,"
+                + "\"REDUCE_INPUT_RECORDS\":2}}";
 
         WorkflowJobBean workflow = new WorkflowJobBean();
         workflow.setProtoActionConf("<configuration/>");
@@ -68,14 +67,58 @@ public class TestHadoopELFunctions extends ActionExecutorTestCase {
         String group = "g";
         String name = "c";
         assertEquals(new Long(10),
-                     eval.evaluate("${hadoop:counters('H')['" + group + "']['" + name + "']}", Long.class));
+                eval.evaluate("${hadoop:counters('H')['" + group + "']['" + name + "']}", Long.class));
 
         assertEquals(new Long(2), eval.evaluate("${hadoop:counters('H')[RECORDS][GROUPS]}", Long.class));
         assertEquals(new Long(2), eval.evaluate("${hadoop:counters('H')[RECORDS][REDUCE_IN]}", Long.class));
         assertEquals(new Long(2), eval.evaluate("${hadoop:counters('H')[RECORDS][REDUCE_OUT]}", Long.class));
         assertEquals(new Long(2), eval.evaluate("${hadoop:counters('H')[RECORDS][MAP_IN]}", Long.class));
         assertEquals(new Long(2), eval.evaluate("${hadoop:counters('H')[RECORDS][MAP_OUT]}", Long.class));
+        assertEquals(ActionType.MAP_REDUCE.toString(),
+                eval.evaluate("${hadoop:counters('H')['ACTION_TYPE']}", String.class));
     }
 
-}
+    public void testELFunctionsReturningPigStats() throws Exception {
+        String pigStats = "{\"ACTION_TYPE\":\"PIG\","
+                + "\"PIG_VERSION\":\"0.9.0\","
+                + "\"FEATURES\":\"UNKNOWN\","
+                + "\"ERROR_MESSAGE\":null,"
+                + "\"NUMBER_JOBS\":\"2\","
+                + "\"RECORD_WRITTEN\":\"33\","
+                + "\"JOB_GRAPH\":\"job_201111300933_0004,job_201111300933_0005\","
+                + "\"job_201111300933_0004\":{\"MAP_INPUT_RECORDS\":\"33\",\"MIN_REDUCE_TIME\":\"0\",\"MULTI_STORE_COUNTERS\":{},\"ERROR_MESSAGE\":null,\"JOB_ID\":\"job_201111300933_0004\"},"
+                + "\"job_201111300933_0005\":{\"MAP_INPUT_RECORDS\":\"37\",\"MIN_REDUCE_TIME\":\"0\",\"MULTI_STORE_COUNTERS\":{},\"ERROR_MESSAGE\":null,\"JOB_ID\":\"job_201111300933_0005\"},"
+                + "\"BYTES_WRITTEN\":\"1410\"," + "\"HADOOP_VERSION\":\"0.20.2\"," + "\"RETURN_CODE\":\"0\","
+                + "\"ERROR_CODE\":\"-1\"," + "}";
 
+        WorkflowJobBean workflow = new WorkflowJobBean();
+        workflow.setProtoActionConf("<configuration/>");
+        LiteWorkflowApp wfApp = new LiteWorkflowApp("x", "<workflow-app/>", new StartNodeDef("a"));
+        wfApp.addNode(new EndNodeDef("a"));
+        WorkflowInstance wi = new LiteWorkflowInstance(wfApp, new XConfiguration(), "1");
+
+        workflow.setWorkflowInstance(wi);
+        workflow.setId(Services.get().get(UUIDService.class).generateId(ApplicationType.WORKFLOW));
+        final WorkflowActionBean action = new WorkflowActionBean();
+        action.setName("H");
+
+        ActionCommand.ActionExecutorContext context = new ActionCommand.ActionExecutorContext(workflow, action, false);
+        context.setVar(MapReduceActionExecutor.HADOOP_COUNTERS, pigStats);
+
+        ELEvaluator eval = Services.get().get(ELService.class).createEvaluator("workflow");
+        DagELFunctions.configureEvaluator(eval, workflow, action);
+
+        String version = "0.9.0";
+        String jobGraph = "job_201111300933_0004,job_201111300933_0005";
+        String job1Stats = "{\"MAP_INPUT_RECORDS\":\"33\",\"MIN_REDUCE_TIME\":\"0\",\"MULTI_STORE_COUNTERS\":{},\"ERROR_MESSAGE\":null,\"JOB_ID\":\"job_201111300933_0004\"}";
+        String job2Stats = "{\"MAP_INPUT_RECORDS\":\"37\",\"MIN_REDUCE_TIME\":\"0\",\"MULTI_STORE_COUNTERS\":{},\"ERROR_MESSAGE\":null,\"JOB_ID\":\"job_201111300933_0005\"}";
+
+        assertEquals(ActionType.PIG.toString(), eval.evaluate("${hadoop:counters('H')['ACTION_TYPE']}", String.class));
+        assertEquals(version, eval.evaluate("${hadoop:counters('H')['PIG_VERSION']}", String.class));
+        assertEquals(jobGraph, eval.evaluate("${hadoop:counters('H')['JOB_GRAPH']}", String.class));
+        assertEquals(job1Stats, eval.evaluate("${hadoop:counters('H')['job_201111300933_0004']}", String.class));
+        assertEquals(job2Stats, eval.evaluate("${hadoop:counters('H')['job_201111300933_0005']}", String.class));
+        assertEquals(new Long(33),
+                eval.evaluate("${hadoop:counters('H')['job_201111300933_0004']['MAP_INPUT_RECORDS']}", Long.class));
+    }
+}
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestHiveActionExecutor.java core/src/test/java/org/apache/oozie/action/hadoop/TestHiveActionExecutor.java
new file mode 100644
index 0000000..52399c4
--- /dev/null
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestHiveActionExecutor.java
@@ -0,0 +1,252 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.action.hadoop;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.StringReader;
+import java.io.Writer;
+import java.text.MessageFormat;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Properties;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.JobID;
+import org.apache.hadoop.mapred.RunningJob;
+import org.apache.oozie.WorkflowActionBean;
+import org.apache.oozie.WorkflowJobBean;
+import org.apache.oozie.client.OozieClient;
+import org.apache.oozie.client.WorkflowAction;
+import org.apache.oozie.service.HadoopAccessorService;
+import org.apache.oozie.service.Services;
+import org.apache.oozie.service.WorkflowAppService;
+import org.apache.oozie.util.ClassUtils;
+import org.apache.oozie.util.IOUtils;
+import org.apache.oozie.util.XConfiguration;
+import org.apache.oozie.util.XmlUtils;
+import org.jdom.Element;
+import org.jdom.Namespace;
+
+public class TestHiveActionExecutor extends ActionExecutorTestCase {
+
+    private static final String NEW_LINE =
+        System.getProperty("line.separator", "\n");
+
+    private static final String SAMPLE_DATA_TEXT =
+        "3\n4\n6\n1\n2\n7\n9\n0\n8\n";
+
+    private static final String HIVE_SCRIPT_FILENAME = "script.q";
+
+    private static final String INPUT_DIRNAME = "input";
+    private static final String OUTPUT_DIRNAME = "output";
+    private static final String DATA_FILENAME = "data.txt";
+
+    protected void setSystemProps() {
+        super.setSystemProps();
+        setSystemProperty("oozie.service.ActionService.executor.classes",
+                HiveActionExecutor.class.getName());
+    }
+
+    public void testVariableSubstitutionSimple() throws Exception {
+        String key = "test.random.key";
+        String keyExpression = "${" + key + "}";
+        Map<String, String> varMap = new HashMap<String, String>();
+        varMap.put(key, keyExpression);
+        String value = HiveMain.substitute(varMap, keyExpression);
+        assertTrue("Unexpected value " + value, value.equals(keyExpression));
+    }
+
+    public void testSetupMethods() throws Exception {
+        HiveActionExecutor ae = new HiveActionExecutor();
+        assertEquals("hive", ae.getType());
+    }
+
+    public void testLauncherJar() throws Exception {
+        HiveActionExecutor ae = new HiveActionExecutor();
+        Path jar = new Path(ae.getOozieRuntimeDir(), ae.getLauncherJarName());
+        assertTrue(new File(jar.toString()).exists());
+    }
+
+    private String getHiveScript(String inputPath, String outputPath) {
+        StringBuilder buffer = new StringBuilder(NEW_LINE);
+        buffer.append("set -v;").append(NEW_LINE);
+        buffer.append("CREATE EXTERNAL TABLE test (a INT) STORED AS");
+        buffer.append(NEW_LINE).append("TEXTFILE LOCATION '");
+        buffer.append(inputPath).append("';").append(NEW_LINE);
+        buffer.append("INSERT OVERWRITE DIRECTORY '");
+        buffer.append(outputPath).append("'").append(NEW_LINE);
+        buffer.append("SELECT (a-1) FROM test;").append(NEW_LINE);
+
+        return buffer.toString();
+    }
+
+    private String getActionXml() {
+        String script = "<hive xmlns=''uri:oozie:hive-action:0.2''>" +
+        "<job-tracker>{0}</job-tracker>" +
+        "<name-node>{1}</name-node>" +
+        "<configuration>" +
+        "<property>" +
+        "<name>javax.jdo.option.ConnectionURL</name>" +
+        "<value>jdbc:derby:db;create=true</value>" +
+        "</property>" +
+        "<property>" +
+        "<name>javax.jdo.option.ConnectionDriverName</name>" +
+        "<value>org.apache.derby.jdbc.EmbeddedDriver</value>" +
+        "</property>" +
+        "<property>" +
+        "<name>javax.jdo.option.ConnectionUserName</name>" +
+        "<value>sa</value>" +
+        "</property>" +
+        "<property>" +
+        "<name>javax.jdo.option.ConnectionPassword</name>" +
+        "<value> </value>" +
+        "</property>" +
+        "<property>" +
+        "<name>oozie.hive.log.level</name>" +
+        "<value>DEBUG</value>" +
+        "</property>" +
+        "<property>" +
+        "<name>oozie.hive.defaults</name>" +
+        "<value>user-hive-default.xml</value>" +
+        "</property>" +
+        "</configuration>" +
+        "<script>" + HIVE_SCRIPT_FILENAME + "</script>" +
+        "</hive>";
+        return MessageFormat.format(script, getJobTrackerUri(), getNameNodeUri());
+    }
+
+    public void testHiveAction() throws Exception {
+        Path inputDir = new Path(getFsTestCaseDir(), INPUT_DIRNAME);
+        Path outputDir = new Path(getFsTestCaseDir(), OUTPUT_DIRNAME);
+
+        FileSystem fs = getFileSystem();
+        Path script = new Path(getAppPath(), HIVE_SCRIPT_FILENAME);
+        Writer scriptWriter = new OutputStreamWriter(fs.create(script));
+        scriptWriter.write(getHiveScript(inputDir.toString(), outputDir.toString()));
+        scriptWriter.close();
+
+        Writer dataWriter = new OutputStreamWriter(fs.create(new Path(inputDir, DATA_FILENAME)));
+        dataWriter.write(SAMPLE_DATA_TEXT);
+        dataWriter.close();
+
+        InputStream is = IOUtils.getResourceAsStream("user-hive-default.xml", -1);
+        OutputStream os = fs.create(new Path(getAppPath(), "user-hive-default.xml"));
+        IOUtils.copyStream(is, os);
+
+        Context context = createContext(getActionXml());
+        final RunningJob launcherJob = submitAction(context);
+        String launcherId = context.getAction().getExternalId();
+        waitFor(200 * 1000, new Predicate() {
+            public boolean evaluate() throws Exception {
+                return launcherJob.isComplete();
+            }
+        });
+        assertTrue(launcherJob.isSuccessful());
+
+        assertFalse(LauncherMapper.hasIdSwap(launcherJob));
+
+        HiveActionExecutor ae = new HiveActionExecutor();
+        ae.check(context, context.getAction());
+        assertTrue(launcherId.equals(context.getAction().getExternalId()));
+        assertEquals("SUCCEEDED", context.getAction().getExternalStatus());
+        assertNotNull(context.getAction().getData());
+        ae.end(context, context.getAction());
+        assertEquals(WorkflowAction.Status.OK, context.getAction().getStatus());
+
+        assertNotNull(context.getAction().getData());
+        Properties outputData = new Properties();
+        outputData.load(new StringReader(context.getAction().getData()));
+        assertTrue(outputData.containsKey(LauncherMain.HADOOP_JOBS));
+        //while this works in a real cluster, it does not with miniMR
+        //assertTrue(outputData.getProperty(LauncherMain.HADOOP_JOBS).trim().length() > 0);
+
+        assertTrue(fs.exists(outputDir));
+        assertTrue(fs.isDirectory(outputDir));
+    }
+
+    private RunningJob submitAction(Context context) throws Exception {
+        HiveActionExecutor ae = new HiveActionExecutor();
+
+        WorkflowAction action = context.getAction();
+
+        ae.prepareActionDir(getFileSystem(), context);
+        ae.submitLauncher(getFileSystem(), context, action);
+
+        String jobId = action.getExternalId();
+        String jobTracker = action.getTrackerUri();
+        String consoleUrl = action.getConsoleUrl();
+        assertNotNull(jobId);
+        assertNotNull(jobTracker);
+        assertNotNull(consoleUrl);
+        Element e = XmlUtils.parseXml(action.getConf());
+        Namespace ns = Namespace.getNamespace("uri:oozie:hive-action:0.2");
+        XConfiguration conf =
+                new XConfiguration(new StringReader(XmlUtils.prettyPrint(e.getChild("configuration", ns)).toString()));
+        conf.set("mapred.job.tracker", e.getChildTextTrim("job-tracker", ns));
+        conf.set("fs.default.name", e.getChildTextTrim("name-node", ns));
+        conf.set("user.name", context.getProtoActionConf().get("user.name"));
+        conf.set("group.name", getTestGroup());
+        injectKerberosInfo(conf);
+        JobConf jobConf = new JobConf();
+        XConfiguration.copy(conf, jobConf);
+        String user = jobConf.get("user.name");
+        String group = jobConf.get("group.name");
+        JobClient jobClient = Services.get().get(HadoopAccessorService.class).createJobClient(user, group, jobConf);
+        final RunningJob runningJob = jobClient.getJob(JobID.forName(jobId));
+        assertNotNull(runningJob);
+        return runningJob;
+    }
+
+    private String copyJar(String targetFile, Class<?> anyContainedClass)
+            throws Exception {
+        String file = ClassUtils.findContainingJar(anyContainedClass);
+        System.out.println("[copy-jar] class: " + anyContainedClass
+                + ", local jar ==> " + file);
+        Path targetPath = new Path(getAppPath(), targetFile);
+        FileSystem fs = getFileSystem();
+        InputStream is = new FileInputStream(file);
+        OutputStream os = fs.create(new Path(getAppPath(), targetPath));
+        IOUtils.copyStream(is, os);
+        return targetPath.toString();
+    }
+
+    private Context createContext(String actionXml) throws Exception {
+        HiveActionExecutor ae = new HiveActionExecutor();
+
+        XConfiguration protoConf = new XConfiguration();
+        protoConf.set(WorkflowAppService.HADOOP_USER, getTestUser());
+        protoConf.set(WorkflowAppService.HADOOP_UGI, getTestUser() + "," + getTestGroup());
+        protoConf.set(OozieClient.GROUP_NAME, getTestGroup());
+        injectKerberosInfo(protoConf);
+        SharelibUtils.addToDistributedCache("hive", getFileSystem(), getFsTestCaseDir(), protoConf);
+
+        WorkflowJobBean wf = createBaseWorkflow(protoConf, "hive-action");
+        WorkflowActionBean action = (WorkflowActionBean) wf.getActions().get(0);
+        action.setType(ae.getType());
+        action.setConf(actionXml);
+
+        return new Context(wf, action);
+    }
+
+}
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestHiveMain.java core/src/test/java/org/apache/oozie/action/hadoop/TestHiveMain.java
new file mode 100644
index 0000000..d8f7491
--- /dev/null
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestHiveMain.java
@@ -0,0 +1,160 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.action.hadoop;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.oozie.util.IOUtils;
+import org.apache.oozie.util.XConfiguration;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.FileWriter;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.Writer;
+import java.net.URL;
+
+public class TestHiveMain extends MainTestCase {
+    private SecurityManager SECURITY_MANAGER;
+
+    protected void setUp() throws Exception {
+        super.setUp();
+        SECURITY_MANAGER = System.getSecurityManager();
+    }
+
+    protected void tearDown() throws Exception {
+        System.setSecurityManager(SECURITY_MANAGER);
+        super.tearDown();
+    }
+
+    private static final String NEW_LINE =
+        System.getProperty("line.separator", "\n");
+
+    private String getHiveScript(String inputPath, String outputPath) {
+        StringBuilder buffer = new StringBuilder(NEW_LINE);
+        buffer.append("set -v;").append(NEW_LINE);
+        buffer.append("CREATE EXTERNAL TABLE test (a INT) STORED AS");
+        buffer.append(NEW_LINE).append("TEXTFILE LOCATION '");
+        buffer.append(inputPath).append("';").append(NEW_LINE);
+        buffer.append("INSERT OVERWRITE DIRECTORY '");
+        buffer.append(outputPath).append("'").append(NEW_LINE);
+        buffer.append("SELECT (a-1) FROM test;").append(NEW_LINE);
+
+        return buffer.toString();
+    }
+
+    public Void call() throws Exception {
+        if (System.getenv("HADOOP_HOME") == null) {
+            System.out.println("WARNING: 'HADOOP_HOME' env var not defined, TestHiveMain test is not running");
+        }
+        else {
+            FileSystem fs = getFileSystem();
+
+            Path inputDir = new Path(getFsTestCaseDir(), "input");
+            fs.mkdirs(inputDir);
+            Writer writer = new OutputStreamWriter(fs.create(new Path(inputDir, "data.txt")));
+            writer.write("3\n4\n6\n1\n2\n7\n9\n0\n8\n");
+            writer.close();
+
+            Path outputDir = new Path(getFsTestCaseDir(), "output");
+
+            Path script = new Path(getTestCaseDir(), "script.q");
+            Writer w = new FileWriter(script.toString());
+            w.write(getHiveScript("${IN}", "${OUT}"));
+            w.close();
+
+            XConfiguration jobConf = new XConfiguration();
+
+            jobConf.set("oozie.hive.log.level", "DEBUG");
+
+            jobConf.set("user.name", getTestUser());
+            jobConf.set("group.name", getTestGroup());
+            jobConf.setInt("mapred.map.tasks", 1);
+            jobConf.setInt("mapred.map.max.attempts", 1);
+            jobConf.setInt("mapred.reduce.max.attempts", 1);
+            jobConf.set("mapred.job.tracker", getJobTrackerUri());
+            jobConf.set("fs.default.name", getNameNodeUri());
+            jobConf.set("javax.jdo.option.ConnectionURL", "jdbc:derby:db;create=true");
+            jobConf.set("javax.jdo.option.ConnectionDriverName", "org.apache.derby.jdbc.EmbeddedDriver");
+            jobConf.set("javax.jdo.option.ConnectionUserName", "sa");
+            jobConf.set("javax.jdo.option.ConnectionPassword", " ");
+            injectKerberosInfo(jobConf);
+            SharelibUtils.addToDistributedCache("hive", fs, getFsTestCaseDir(), jobConf);
+
+            HiveMain.setHiveScript(jobConf, script.toString(), new String[]{"IN=" + inputDir.toUri().getPath(),
+                    "OUT=" + outputDir.toUri().getPath()});
+
+            File actionXml = new File(getTestCaseDir(), "action.xml");
+            OutputStream os = new FileOutputStream(actionXml);
+            jobConf.writeXml(os);
+            os.close();
+
+            //needed in the testcase classpath
+            URL url = Thread.currentThread().getContextClassLoader().getResource("PigMain.txt");
+            File classPathDir = new File(url.getPath()).getParentFile();
+            assertTrue(classPathDir.exists());
+            File hiveSite = new File(classPathDir, "hive-site.xml");
+
+            InputStream is = IOUtils.getResourceAsStream("user-hive-default.xml", -1);
+            os = new FileOutputStream(new File(classPathDir, "hive-default.xml"));
+            IOUtils.copyStream(is, os);
+
+            File outputDataFile = new File(getTestCaseDir(), "outputdata.properties");
+
+            setSystemProperty("oozie.launcher.job.id", "" + System.currentTimeMillis());
+            setSystemProperty("oozie.action.conf.xml", actionXml.getAbsolutePath());
+            setSystemProperty("oozie.action.output.properties", outputDataFile.getAbsolutePath());
+
+            new LauncherSecurityManager();
+            String user = System.getProperty("user.name");
+            try {
+                os = new FileOutputStream(hiveSite);
+                jobConf.writeXml(os);
+                os.close();
+                HiveMain.main(null);
+            }
+            catch (SecurityException ex) {
+                if (LauncherSecurityManager.getExitInvoked()) {
+                    System.out.println("Intercepting System.exit(" + LauncherSecurityManager.getExitCode() + ")");
+                    System.err.println("Intercepting System.exit(" + LauncherSecurityManager.getExitCode() + ")");
+                    if (LauncherSecurityManager.getExitCode() != 0) {
+                        fail();
+                    }
+                }
+                else {
+                    throw ex;
+                }
+            }
+            finally {
+                System.setProperty("user.name", user);
+                hiveSite.delete();
+            }
+
+            assertTrue(outputDataFile.exists());
+
+//TODO: I cannot figure out why when log file is not created in this testcase, it works when running in Launcher
+//            Properties props = new Properties();
+//            props.load(new FileReader(outputDataFile));
+//            assertTrue(props.containsKey(LauncherMain.HADOOP_JOBS));
+//            assertTrue(props.getProperty(LauncherMain.HADOOP_JOBS).trim().length() > 0);
+        }
+        return null;
+    }
+
+}
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestJavaActionExecutor.java core/src/test/java/org/apache/oozie/action/hadoop/TestJavaActionExecutor.java
index 18c66ce..3c967ea 100644
--- core/src/test/java/org/apache/oozie/action/hadoop/TestJavaActionExecutor.java
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestJavaActionExecutor.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -47,6 +47,7 @@ import org.apache.oozie.action.ActionExecutorException;
 import org.apache.oozie.client.OozieClient;
 import org.apache.oozie.client.WorkflowAction;
 import org.apache.oozie.client.WorkflowJob;
+import org.apache.oozie.service.HadoopAccessorService;
 import org.apache.oozie.service.Services;
 import org.apache.oozie.service.UUIDService;
 import org.apache.oozie.service.WorkflowAppService;
@@ -88,6 +89,8 @@ public class TestJavaActionExecutor extends ActionExecutorTestCase {
         classes.add(LauncherSecurityManager.class);
         classes.add(LauncherException.class);
         classes.add(LauncherMainException.class);
+        classes.add(ActionStats.class);
+        classes.add(ActionType.class);
         assertEquals(classes, ae.getLauncherClasses());
 
         Configuration conf = new XConfiguration();
@@ -332,7 +335,8 @@ public class TestJavaActionExecutor extends ActionExecutorTestCase {
         JobConf jobConf = new JobConf();
         jobConf.set("mapred.job.tracker", jobTracker);
         injectKerberosInfo(jobConf);
-        JobClient jobClient = new JobClient(jobConf);
+        JobClient jobClient =
+            Services.get().get(HadoopAccessorService.class).createJobClient(getTestUser(), getTestGroup(), jobConf);
         final RunningJob runningJob = jobClient.getJob(JobID.forName(jobId));
         assertNotNull(runningJob);
         return runningJob;
@@ -347,6 +351,7 @@ public class TestJavaActionExecutor extends ActionExecutorTestCase {
         Context context = createContext(actionXml);
         final RunningJob runningJob = submitAction(context);
         waitFor(60 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 return runningJob.isComplete();
             }
@@ -372,6 +377,7 @@ public class TestJavaActionExecutor extends ActionExecutorTestCase {
         Context context = createContext(actionXml);
         final RunningJob runningJob = submitAction(context);
         waitFor(60 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 return runningJob.isComplete();
             }
@@ -402,6 +408,7 @@ public class TestJavaActionExecutor extends ActionExecutorTestCase {
         Context context = createContext(actionXml);
         final RunningJob runningJob = submitAction(context);
         waitFor(60 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 return runningJob.isComplete();
             }
@@ -438,6 +445,7 @@ public class TestJavaActionExecutor extends ActionExecutorTestCase {
         ActionExecutor ae = new JavaActionExecutor();
         assertFalse(ae.isCompleted(context.getAction().getExternalStatus()));
         waitFor(60 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 return runningJob.isComplete();
             }
@@ -462,6 +470,7 @@ public class TestJavaActionExecutor extends ActionExecutorTestCase {
         Context context = createContext(actionXml);
         final RunningJob runningJob = submitAction(context);
         waitFor(60 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 return runningJob.isComplete();
             }
@@ -488,6 +497,7 @@ public class TestJavaActionExecutor extends ActionExecutorTestCase {
         Context context = createContext(actionXml);
         final RunningJob runningJob = submitAction(context);
         waitFor(60 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 return runningJob.isComplete();
             }
@@ -516,6 +526,7 @@ public class TestJavaActionExecutor extends ActionExecutorTestCase {
         Context context = createContext(actionXml);
         final RunningJob runningJob = submitAction(context);
         waitFor(60 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 return runningJob.isComplete();
             }
@@ -548,6 +559,7 @@ public class TestJavaActionExecutor extends ActionExecutorTestCase {
         assertTrue(ae.isCompleted(context.getAction().getExternalStatus()));
 
         waitFor(60 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 return runningJob.isComplete();
             }
@@ -567,6 +579,7 @@ public class TestJavaActionExecutor extends ActionExecutorTestCase {
         String launcherId = context.getAction().getExternalId();
 
         waitFor(60 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 JavaActionExecutor ae = new JavaActionExecutor();
                 Configuration conf = ae.createBaseHadoopConf(context, XmlUtils.parseXml(actionXml));
@@ -580,6 +593,7 @@ public class TestJavaActionExecutor extends ActionExecutorTestCase {
         assertEquals(launcherId, context.getAction().getExternalId());
 
         waitFor(60 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 return runningJob2.isComplete();
             }
@@ -681,6 +695,7 @@ public class TestJavaActionExecutor extends ActionExecutorTestCase {
         Context context = createContext(actionXml);
         final RunningJob runningJob = submitAction(context);
         waitFor(60 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 return runningJob.isComplete();
             }
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestLauncher.java core/src/test/java/org/apache/oozie/action/hadoop/TestLauncher.java
index f5b0808..4d82e56 100644
--- core/src/test/java/org/apache/oozie/action/hadoop/TestLauncher.java
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestLauncher.java
@@ -49,6 +49,7 @@ public class TestLauncher extends XFsTestCase {
         Path actionDir = getFsTestCaseDir();
 
         File jar = IOUtils.createJar(new File(getTestCaseDir()), "launcher.jar", LauncherMapper.class,
+                                     LauncherMainException.class,
                                      LauncherSecurityManager.class, LauncherException.class, LauncherMainTester.class);
 
         FileSystem fs = getFileSystem();
@@ -57,12 +58,14 @@ public class TestLauncher extends XFsTestCase {
         fs.copyFromLocalFile(new Path(jar.toString()), launcherJar);
 
         JobConf jobConf = new JobConf();
+//        jobConf.setJar(jar.getAbsolutePath());
         jobConf.set("user.name", getTestUser());
         jobConf.set("group.name", getTestGroup());
         jobConf.setInt("mapred.map.tasks", 1);
         jobConf.setInt("mapred.map.max.attempts", 1);
         jobConf.setInt("mapred.reduce.max.attempts", 1);
 
+        jobConf.set("mapreduce.framework.name", "yarn");
         jobConf.set("mapred.job.tracker", getJobTrackerUri());
         jobConf.set("fs.default.name", getNameNodeUri());
         injectKerberosInfo(jobConf);
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestMapReduceActionError.java core/src/test/java/org/apache/oozie/action/hadoop/TestMapReduceActionError.java
index 68b78cd..44296eb 100644
--- core/src/test/java/org/apache/oozie/action/hadoop/TestMapReduceActionError.java
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestMapReduceActionError.java
@@ -72,13 +72,14 @@ public class TestMapReduceActionError extends ActionExecutorTestCase {
         classes.add(LauncherSecurityManager.class);
         classes.add(LauncherException.class);
         classes.add(LauncherMainException.class);
+        classes.add(ActionStats.class);
+        classes.add(ActionType.class);
         classes.add(LauncherMain.class);
         classes.add(MapReduceMain.class);
         classes.add(StreamingMain.class);
         classes.add(PipesMain.class);
         assertEquals(classes, ae.getLauncherClasses());
 
-
         Element actionXml = XmlUtils.parseXml("<map-reduce>" +
                 "<job-tracker>" + getJobTrackerUri() + "</job-tracker>" +
                 "<name-node>" + getNameNodeUri() + "</name-node>" +
@@ -204,7 +205,9 @@ public class TestMapReduceActionError extends ActionExecutorTestCase {
         conf.set("user.name", context.getProtoActionConf().get("user.name"));
         conf.set("group.name", getTestGroup());
         injectKerberosInfo(conf);
-        JobConf jobConf = new JobConf(conf);
+        conf.set("mapreduce.framework.name", "yarn");
+        JobConf jobConf = new JobConf();
+        XConfiguration.copy(conf, jobConf);
         String user = jobConf.get("user.name");
         String group = jobConf.get("group.name");
         JobClient jobClient = Services.get().get(HadoopAccessorService.class).createJobClient(user, group, jobConf);
@@ -230,8 +233,9 @@ public class TestMapReduceActionError extends ActionExecutorTestCase {
         Configuration conf = ae.createBaseHadoopConf(context, XmlUtils.parseXml(actionXml));
         String user = conf.get("user.name");
         String group = conf.get("group.name");
-        JobClient jobClient = Services.get().get(HadoopAccessorService.class).createJobClient(user, group,
-                                                                                              new JobConf(conf));
+        JobConf jobConf = new JobConf();
+        XConfiguration.copy(conf, jobConf);
+        JobClient jobClient = Services.get().get(HadoopAccessorService.class).createJobClient(user, group, jobConf);
         final RunningJob mrJob = jobClient.getJob(JobID.forName(context.getAction().getExternalId()));
 
         waitFor(60 * 1000, new Predicate() {
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestMapReduceActionExecutor.java core/src/test/java/org/apache/oozie/action/hadoop/TestMapReduceActionExecutor.java
index 4eb4815..490ecbc 100644
--- core/src/test/java/org/apache/oozie/action/hadoop/TestMapReduceActionExecutor.java
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestMapReduceActionExecutor.java
@@ -75,6 +75,8 @@ public class TestMapReduceActionExecutor extends ActionExecutorTestCase {
         classes.add(LauncherSecurityManager.class);
         classes.add(LauncherException.class);
         classes.add(LauncherMainException.class);
+        classes.add(ActionStats.class);
+        classes.add(ActionType.class);
         classes.add(LauncherMain.class);
         classes.add(MapReduceMain.class);
         classes.add(StreamingMain.class);
@@ -213,7 +215,9 @@ public class TestMapReduceActionExecutor extends ActionExecutorTestCase {
         conf.set("user.name", context.getProtoActionConf().get("user.name"));
         conf.set("group.name", getTestGroup());
         injectKerberosInfo(conf);
-        JobConf jobConf = new JobConf(conf);
+        conf.set("mapreduce.framework.name", "yarn");
+        JobConf jobConf = new JobConf();
+        XConfiguration.copy(conf, jobConf);
         String user = jobConf.get("user.name");
         String group = jobConf.get("group.name");
         JobClient jobClient = Services.get().get(HadoopAccessorService.class).createJobClient(user, group, jobConf);
@@ -227,7 +231,7 @@ public class TestMapReduceActionExecutor extends ActionExecutorTestCase {
         Context context = createContext(name, actionXml);
         final RunningJob launcherJob = submitAction(context);
         String launcherId = context.getAction().getExternalId();
-        waitFor(120 * 1000, new Predicate() {
+        waitFor(120 * 2000, new Predicate() {
             public boolean evaluate() throws Exception {
                 return launcherJob.isComplete();
             }
@@ -243,8 +247,10 @@ public class TestMapReduceActionExecutor extends ActionExecutorTestCase {
         Configuration conf = ae.createBaseHadoopConf(context, XmlUtils.parseXml(actionXml));
         String user = conf.get("user.name");
         String group = conf.get("group.name");
+        JobConf jobConf = new JobConf();
+        XConfiguration.copy(conf, jobConf);
         JobClient jobClient = Services.get().get(HadoopAccessorService.class).createJobClient(user, group,
-                new JobConf(conf));
+                jobConf);
         final RunningJob mrJob = jobClient.getJob(JobID.forName(context.getAction().getExternalId()));
 
         waitFor(120 * 1000, new Predicate() {
@@ -261,9 +267,13 @@ public class TestMapReduceActionExecutor extends ActionExecutorTestCase {
         ae.end(context, context.getAction());
         assertEquals(WorkflowAction.Status.OK, context.getAction().getStatus());
 
+        //hadoop.counters will always be set in case of MR action.
         assertNotNull(context.getVar("hadoop.counters"));
         String counters = context.getVar("hadoop.counters");
-        assertTrue(counters.contains("Task$Counter"));
+        assertTrue(counters.contains("Counter"));
+
+        //External Child IDs will always be null in case of MR action.
+        assertNull(context.getExternalChildIDs());
     }
 
     private void _testSubmitWithCredentials(String name, String actionXml) throws Exception {
@@ -287,8 +297,10 @@ public class TestMapReduceActionExecutor extends ActionExecutorTestCase {
         Configuration conf = ae.createBaseHadoopConf(context, XmlUtils.parseXml(actionXml));
         String user = conf.get("user.name");
         String group = conf.get("group.name");
+        JobConf jobConf = new JobConf();
+        XConfiguration.copy(conf, jobConf);
         JobClient jobClient = Services.get().get(HadoopAccessorService.class).createJobClient(user, group,
-                new JobConf(conf));
+                jobConf);
         final RunningJob mrJob = jobClient.getJob(JobID.forName(context.getAction().getExternalId()));
 
         waitFor(120 * 1000, new Predicate() {
@@ -401,6 +413,15 @@ public class TestMapReduceActionExecutor extends ActionExecutorTestCase {
         return conf;
     }
 
+    private XConfiguration getOozieActionExternalStatsWriteProperty(String inputDir, String outputDir,
+            String oozieProperty) {
+        XConfiguration conf = new XConfiguration();
+        conf.set("mapred.input.dir", inputDir);
+        conf.set("mapred.output.dir", outputDir);
+        conf.set("oozie.action.external.stats.write", oozieProperty);
+        return conf;
+    }
+
     public void testPipes() throws Exception {
         if (Boolean.parseBoolean(System.getProperty("oozie.test.hadoop.pipes", "false"))) {
             String wordCountBinary = TestPipesMain.getProgramName(this);
@@ -429,4 +450,157 @@ public class TestMapReduceActionExecutor extends ActionExecutorTestCase {
         }
     }
 
+    // Test to assert that executionStats is set when user has specified stats
+    // write property as true.
+    public void testSetExecutionStats_when_user_has_specified_stats_write_TRUE() throws Exception {
+        FileSystem fs = getFileSystem();
+
+        Path inputDir = new Path(getFsTestCaseDir(), "input");
+        Path outputDir = new Path(getFsTestCaseDir(), "output");
+
+        Writer w = new OutputStreamWriter(fs.create(new Path(inputDir, "data.txt")));
+        w.write("dummy\n");
+        w.write("dummy\n");
+        w.close();
+
+        // set user stats write property as true explicitly in the
+        // configuration.
+        String actionXml = "<map-reduce>"
+                + "<job-tracker>"
+                + getJobTrackerUri()
+                + "</job-tracker>"
+                + "<name-node>"
+                + getNameNodeUri()
+                + "</name-node>"
+                + getOozieActionExternalStatsWriteProperty(inputDir.toString(), outputDir.toString(), "true")
+                        .toXmlString(false) + "</map-reduce>";
+
+        Context context = createContext("map-reduce", actionXml);
+        final RunningJob launcherJob = submitAction(context);
+        String launcherId = context.getAction().getExternalId();
+        waitFor(120 * 1000, new Predicate() {
+            public boolean evaluate() throws Exception {
+                return launcherJob.isComplete();
+            }
+        });
+        assertTrue(launcherJob.isSuccessful());
+
+        assertTrue(LauncherMapper.hasIdSwap(launcherJob));
+
+        MapReduceActionExecutor ae = new MapReduceActionExecutor();
+        ae.check(context, context.getAction());
+        assertFalse(launcherId.equals(context.getAction().getExternalId()));
+
+        Configuration conf = ae.createBaseHadoopConf(context, XmlUtils.parseXml(actionXml));
+        String user = conf.get("user.name");
+        String group = conf.get("group.name");
+        JobConf jobConf = new JobConf();
+        XConfiguration.copy(conf, jobConf);
+        JobClient jobClient = Services.get().get(HadoopAccessorService.class)
+                .createJobClient(user, group, jobConf);
+        final RunningJob mrJob = jobClient.getJob(JobID.forName(context.getAction().getExternalId()));
+
+        waitFor(120 * 1000, new Predicate() {
+            public boolean evaluate() throws Exception {
+                return mrJob.isComplete();
+            }
+        });
+        assertTrue(mrJob.isSuccessful());
+        ae.check(context, context.getAction());
+
+        assertEquals("SUCCEEDED", context.getAction().getExternalStatus());
+        assertNull(context.getAction().getData());
+
+        ae.end(context, context.getAction());
+        assertEquals(WorkflowAction.Status.OK, context.getAction().getStatus());
+
+        // Assert for stats info stored in the context.
+        assertNotNull(context.getExecutionStats());
+        assertTrue(context.getExecutionStats().contains("ACTION_TYPE"));
+        assertTrue(context.getExecutionStats().contains("Counter"));
+
+        // External Child IDs will always be null in case of MR action.
+        assertNull(context.getExternalChildIDs());
+
+        // hadoop.counters will always be set in case of MR action.
+        assertNotNull(context.getVar("hadoop.counters"));
+        String counters = context.getVar("hadoop.counters");
+        assertTrue(counters.contains("Counter"));
+    }
+
+    // Test to assert that executionStats is not set when user has specified
+    // stats write property as false.
+    public void testSetExecutionStats_when_user_has_specified_stats_write_FALSE() throws Exception {
+        FileSystem fs = getFileSystem();
+
+        Path inputDir = new Path(getFsTestCaseDir(), "input");
+        Path outputDir = new Path(getFsTestCaseDir(), "output");
+
+        Writer w = new OutputStreamWriter(fs.create(new Path(inputDir, "data.txt")));
+        w.write("dummy\n");
+        w.write("dummy\n");
+        w.close();
+
+        // set user stats write property as false explicitly in the
+        // configuration.
+        String actionXml = "<map-reduce>"
+                + "<job-tracker>"
+                + getJobTrackerUri()
+                + "</job-tracker>"
+                + "<name-node>"
+                + getNameNodeUri()
+                + "</name-node>"
+                + getOozieActionExternalStatsWriteProperty(inputDir.toString(), outputDir.toString(), "false")
+                        .toXmlString(false) + "</map-reduce>";
+
+        Context context = createContext("map-reduce", actionXml);
+        final RunningJob launcherJob = submitAction(context);
+        String launcherId = context.getAction().getExternalId();
+        waitFor(120 * 2000, new Predicate() {
+            public boolean evaluate() throws Exception {
+                return launcherJob.isComplete();
+            }
+        });
+        assertTrue(launcherJob.isSuccessful());
+
+        assertTrue(LauncherMapper.hasIdSwap(launcherJob));
+
+        MapReduceActionExecutor ae = new MapReduceActionExecutor();
+        ae.check(context, context.getAction());
+        assertFalse(launcherId.equals(context.getAction().getExternalId()));
+
+        Configuration conf = ae.createBaseHadoopConf(context, XmlUtils.parseXml(actionXml));
+        String user = conf.get("user.name");
+        String group = conf.get("group.name");
+        JobConf jobConf = new JobConf();
+        XConfiguration.copy(conf, jobConf);
+        JobClient jobClient = Services.get().get(HadoopAccessorService.class)
+                .createJobClient(user, group, jobConf);
+        final RunningJob mrJob = jobClient.getJob(JobID.forName(context.getAction().getExternalId()));
+
+        waitFor(120 * 1000, new Predicate() {
+            public boolean evaluate() throws Exception {
+                return mrJob.isComplete();
+            }
+        });
+        assertTrue(mrJob.isSuccessful());
+        ae.check(context, context.getAction());
+
+        assertEquals("SUCCEEDED", context.getAction().getExternalStatus());
+        assertNull(context.getAction().getData());
+
+        ae.end(context, context.getAction());
+        assertEquals(WorkflowAction.Status.OK, context.getAction().getStatus());
+
+        // Assert for stats info stored in the context.
+        assertNull(context.getExecutionStats());
+
+        // External Child IDs will always be null in case of MR action.
+        assertNull(context.getExternalChildIDs());
+
+        // hadoop.counters will always be set in case of MR action.
+        assertNotNull(context.getVar("hadoop.counters"));
+        String counters = context.getVar("hadoop.counters");
+        assertTrue(counters.contains("Counter"));
+    }
 }
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestMapReduceMain.java core/src/test/java/org/apache/oozie/action/hadoop/TestMapReduceMain.java
index 3cd7829..bc28446 100644
--- core/src/test/java/org/apache/oozie/action/hadoop/TestMapReduceMain.java
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestMapReduceMain.java
@@ -44,6 +44,9 @@ public class TestMapReduceMain extends MainTestCase {
         Path outputDir = new Path(getFsTestCaseDir(), "output");
 
         XConfiguration jobConf = new XConfiguration();
+
+        jobConf.set("mapreduce.framework.name", "yarn");
+
         jobConf.setInt("mapred.map.tasks", 1);
         jobConf.setInt("mapred.map.max.attempts", 1);
         jobConf.setInt("mapred.reduce.max.attempts", 1);
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestPigActionExecutor.java core/src/test/java/org/apache/oozie/action/hadoop/TestPigActionExecutor.java
index b7842f6..0391763 100644
--- core/src/test/java/org/apache/oozie/action/hadoop/TestPigActionExecutor.java
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestPigActionExecutor.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -34,11 +34,11 @@ import org.apache.oozie.service.HadoopAccessorService;
 import org.apache.oozie.util.XConfiguration;
 import org.apache.oozie.util.XmlUtils;
 import org.apache.oozie.util.IOUtils;
-import org.apache.oozie.util.ClassUtils;
-import org.apache.pig.Main;
 import org.jdom.Element;
+import org.json.simple.JSONValue;
 
 import java.io.File;
+import java.io.IOException;
 import java.io.OutputStream;
 import java.io.InputStream;
 import java.io.FileInputStream;
@@ -47,12 +47,26 @@ import java.io.OutputStreamWriter;
 import java.io.StringReader;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Properties;
-
-import jline.ConsoleReaderInputStream;
+import java.util.Map;
 
 public class TestPigActionExecutor extends ActionExecutorTestCase {
 
+    private static final String PIG_SCRIPT = "set job.name 'test'\n" + "set debug on\n" +
+            "A = load '$IN' using PigStorage(':');\n" +
+            "B = foreach A generate $0 as id;\n" +
+            "store B into '$OUT' USING PigStorage();\n";
+
+    private static final String ERROR_PIG_SCRIPT = "set job.name 'test'\n" + "set debug on\n" +
+            "A = load '$IN' using PigStorage(':');\n" +
+            "ERROR @#$@#$;\n";
+
+    private static final String UDF_PIG_SCRIPT = "register udf.jar\n" +
+            "set job.name 'test'\n" + "set debug on\n" +
+            "A = load '$IN' using PigStorage(':');\n" +
+            "B = foreach A generate" +
+            "       org.apache.oozie.action.hadoop.UDFTester($0) as id;\n" +
+            "store B into '$OUT' USING PigStorage();\n";
+
     @Override
     protected void setSystemProps() {
         super.setSystemProps();
@@ -77,12 +91,14 @@ public class TestPigActionExecutor extends ActionExecutorTestCase {
         classes.add(LauncherSecurityManager.class);
         classes.add(LauncherException.class);
         classes.add(LauncherMainException.class);
+        classes.add(ActionStats.class);
+        classes.add(ActionType.class);
         classes.add(LauncherMain.class);
         classes.add(MapReduceMain.class);
         classes.add(PigMain.class);
+        classes.add(OoziePigStats.class);
         assertEquals(classes, ae.getLauncherClasses());
 
-
         Element actionXml = XmlUtils.parseXml("<pig>" +
                 "<job-tracker>" + getJobTrackerUri() + "</job-tracker>" +
                 "<name-node>" + getNameNodeUri() + "</name-node>" +
@@ -116,23 +132,13 @@ public class TestPigActionExecutor extends ActionExecutorTestCase {
 
         FileSystem fs = getFileSystem();
 
-        Path pigJar = new Path(getAppPath(), "lib/pig.jar");
-        InputStream is = new FileInputStream(ClassUtils.findContainingJar(Main.class));
-        OutputStream os = fs.create(new Path(getAppPath(), pigJar));
-        IOUtils.copyStream(is, os);
-
-        Path jLineJar = new Path(getAppPath(), "lib/jline.jar");
-        is = new FileInputStream(ClassUtils.findContainingJar(ConsoleReaderInputStream.class));
-        os = fs.create(new Path(getAppPath(), jLineJar));
-        IOUtils.copyStream(is, os);
-
-
         XConfiguration protoConf = new XConfiguration();
         protoConf.set(WorkflowAppService.HADOOP_USER, getTestUser());
         protoConf.set(WorkflowAppService.HADOOP_UGI, getTestUser() + "," + getTestGroup());
         protoConf.set(OozieClient.GROUP_NAME, getTestGroup());
         injectKerberosInfo(protoConf);
-        protoConf.setStrings(WorkflowAppService.APP_LIB_PATH_LIST, pigJar.toString(), jLineJar.toString());
+
+        SharelibUtils.addToDistributedCache("pig", fs, getFsTestCaseDir(), protoConf);
 
         WorkflowJobBean wf = createBaseWorkflow(protoConf, "pig-action");
         WorkflowActionBean action = (WorkflowActionBean) wf.getActions().get(0);
@@ -162,10 +168,12 @@ public class TestPigActionExecutor extends ActionExecutorTestCase {
                 new XConfiguration(new StringReader(XmlUtils.prettyPrint(e.getChild("configuration")).toString()));
         conf.set("mapred.job.tracker", e.getChildTextTrim("job-tracker"));
         conf.set("fs.default.name", e.getChildTextTrim("name-node"));
+        conf.set("mapreduce.framework.name", "yarn");
         conf.set("user.name", context.getProtoActionConf().get("user.name"));
         conf.set("group.name", getTestGroup());
         injectKerberosInfo(conf);
-        JobConf jobConf = new JobConf(conf);
+        JobConf jobConf = new JobConf();
+        XConfiguration.copy(conf, jobConf);
         String user = jobConf.get("user.name");
         String group = jobConf.get("group.name");
         JobClient jobClient = Services.get().get(HadoopAccessorService.class).createJobClient(user, group, jobConf);
@@ -179,18 +187,13 @@ public class TestPigActionExecutor extends ActionExecutorTestCase {
         Context context = createContext(actionXml);
         final RunningJob launcherJob = submitAction(context);
         String launcherId = context.getAction().getExternalId();
-        waitFor(180 * 1000, new Predicate() {
-            public boolean evaluate() throws Exception {
-                return launcherJob.isComplete();
-            }
-        });
-        Thread.sleep(2000);
+        evaluateLauncherJob(launcherJob);
         assertTrue(launcherJob.isSuccessful());
 
         Thread.sleep(2000);
         assertFalse(LauncherMapper.hasIdSwap(launcherJob));
         if (checkForSuccess) {
-            assertTrue(LauncherMapper.hasOutputData(launcherJob));
+            assertTrue(LauncherMapper.hasStatsData(launcherJob));
         }
 
         PigActionExecutor ae = new PigActionExecutor();
@@ -199,11 +202,8 @@ public class TestPigActionExecutor extends ActionExecutorTestCase {
         assertTrue(launcherId.equals(context.getAction().getExternalId()));
         if (checkForSuccess) {
             assertEquals("SUCCEEDED", context.getAction().getExternalStatus());
-            assertNotNull(context.getAction().getData());
-            Properties outputData = new Properties();
-            outputData.load(new StringReader(context.getAction().getData()));
-            assertTrue(outputData.containsKey("hadoopJobs"));
-            assertNotSame("", outputData.getProperty("hadoopJobs"));
+            assertNull(context.getAction().getData());
+
         }
         else {
             assertEquals("FAILED/KILLED", context.getAction().getExternalStatus());
@@ -217,54 +217,148 @@ public class TestPigActionExecutor extends ActionExecutorTestCase {
         }
     }
 
-    private static final String PIG_SCRIPT = "set job.name 'test'\n" + "set debug on\n" +
-            "A = load '$IN' using PigStorage(':');\n" +
-            "B = foreach A generate $0 as id;\n" +
-            "store B into '$OUT' USING PigStorage();\n";
+    /*
+     * Test the stats retrieved from a Pig job
+     */
+    public void testExecutionStats() throws Exception {
+        // Set the action xml with the option for retrieving stats to true
+        String actionXml = setPigActionXml(PIG_SCRIPT, true);
+        Context context = createContext(actionXml);
+        final RunningJob launcherJob = submitAction(context);
+        evaluateLauncherJob(launcherJob);
+        assertTrue(launcherJob.isSuccessful());
+        assertTrue(LauncherMapper.hasStatsData(launcherJob));
 
-    protected XConfiguration getPigConfig() {
-        XConfiguration conf = new XConfiguration();
-        conf.set("oozie.pig.log.level", "INFO");
-        return conf;
+        PigActionExecutor ae = new PigActionExecutor();
+        WorkflowAction wfAction = context.getAction();
+        ae.check(context, wfAction);
+        ae.end(context, wfAction);
+
+        assertEquals("SUCCEEDED", wfAction.getExternalStatus());
+        String stats = wfAction.getStats();
+        assertNotNull(stats);
+        // check for some of the expected key values in the stats
+        Map m = (Map)JSONValue.parse(stats);
+        // check for expected 1st level JSON keys
+        assertTrue(m.containsKey("PIG_VERSION"));
+
+        String expectedChildIDs = wfAction.getExternalChildIDs();
+        String[] childIDs = expectedChildIDs.split(",");
+        assertTrue(m.containsKey(childIDs[0]));
+
+        Map q = (Map)m.get(childIDs[0]);
+        // check for expected 2nd level JSON keys
+        assertTrue(q.containsKey("HADOOP_COUNTERS"));
     }
 
-    public void testPig() throws Exception {
-        FileSystem fs = getFileSystem();
 
-        Path script = new Path(getAppPath(), "script.pig");
-        Writer w = new OutputStreamWriter(fs.create(script));
-        w.write(PIG_SCRIPT);
-        w.close();
+    /*
+     * Test the Hadoop IDs obtained from the Pig job
+     */
+    public void testExternalChildIds() throws Exception {
+        // Set the action xml with the option for retrieving stats to false
+        String actionXml = setPigActionXml(PIG_SCRIPT, false);
+        Context context = createContext(actionXml);
+        final RunningJob launcherJob = submitAction(context);
+        evaluateLauncherJob(launcherJob);
+        assertTrue(launcherJob.isSuccessful());
 
-        Path inputDir = new Path(getFsTestCaseDir(), "input");
-        Path outputDir = new Path(getFsTestCaseDir(), "output");
+        PigActionExecutor ae = new PigActionExecutor();
+        WorkflowAction wfAction = context.getAction();
+        ae.check(context, wfAction);
+        ae.end(context, wfAction);
 
-        w = new OutputStreamWriter(fs.create(new Path(inputDir, "data.txt")));
-        w.write("dummy\n");
-        w.write("dummy\n");
-        w.close();
+        assertEquals("SUCCEEDED", wfAction.getExternalStatus());
+        String externalIds = wfAction.getExternalChildIDs();
+        assertNotNull(externalIds);
+        assertNotSame("", externalIds);
+        // check for the expected prefix of hadoop jobIDs
+        assertTrue(externalIds.contains("job_"));
 
-        String actionXml = "<pig>" +
-                "<job-tracker>" + getJobTrackerUri() + "</job-tracker>" +
-                "<name-node>" + getNameNodeUri() + "</name-node>" +
-                getPigConfig().toXmlString(false) +
-                "<script>" + script.getName() + "</script>" +
-                "<param>IN=" + inputDir.toUri().getPath() + "</param>" +
-                "<param>OUT=" + outputDir.toUri().getPath() + "</param>" +
-                "</pig>";
-        _testSubmit(actionXml, true);
     }
 
-    private static final String ERROR_PIG_SCRIPT = "set job.name 'test'\n" + "set debug on\n" +
-            "A = load '$IN' using PigStorage(':');\n" +
-            "ERROR @#$@#$;\n";
+    /*
+     * Test the stats after setting the maximum allowed size of stats to a small
+     * value
+     */
+    public void testExecutionStatsWithMaxStatsSizeLimit() throws Exception {
+        // Set a very small value for max size of stats
+        setSystemProperty(JavaActionExecutor.MAX_EXTERNAL_STATS_SIZE, new String("1"));
+        new Services().init();
+        // Set the action xml with the option for retrieving stats to true
+        String actionXml = setPigActionXml(PIG_SCRIPT, true);
+        Context context = createContext(actionXml);
+        final RunningJob launcherJob = submitAction(context);
+        evaluateLauncherJob(launcherJob);
+        assertTrue(launcherJob.isSuccessful());
+
+        PigActionExecutor ae = new PigActionExecutor();
+        WorkflowAction wfAction = context.getAction();
+        ae.check(context, wfAction);
+        ae.end(context, wfAction);
+
+        // action should fail as the size of pig stats will always be greater
+        // than 1 byte
+        assertEquals("FAILED/KILLED", wfAction.getExternalStatus());
+        assertNull(wfAction.getStats());
+    }
+
+    /*
+     * Test the stats with retrieve stats option set to false
+     */
+    public void testExecutionStatsWithRetrieveStatsFalse() throws Exception {
+        // Set the action xml with the option for retrieving stats to false
+        String actionXml = setPigActionXml(PIG_SCRIPT, false);
+        Context context = createContext(actionXml);
+        final RunningJob launcherJob = submitAction(context);
+        evaluateLauncherJob(launcherJob);
+        assertTrue(launcherJob.isSuccessful());
+        assertFalse(LauncherMapper.hasStatsData(launcherJob));
+
+        PigActionExecutor ae = new PigActionExecutor();
+        WorkflowAction wfAction = context.getAction();
+        ae.check(context, wfAction);
+        ae.end(context, wfAction);
+
+        assertEquals("SUCCEEDED", wfAction.getExternalStatus());
+        assertNotNull(wfAction.getExternalChildIDs());
+    }
+
+    private void evaluateLauncherJob(final RunningJob launcherJob) throws Exception{
+        waitFor(180 * 1000, new Predicate() {
+            @Override
+            public boolean evaluate() throws Exception {
+                return launcherJob.isComplete();
+            }
+        });
+        Thread.sleep(2000);
+    }
+
+    protected XConfiguration setPigConfig(boolean writeStats) {
+        XConfiguration conf = new XConfiguration();
+        conf.set("oozie.pig.log.level", "INFO");
+        conf.set(PigMain.EXTERNAL_STATS_WRITE, String.valueOf(writeStats));
+        return conf;
+    }
+
+    public void testPig() throws Exception {
+        // Set the action xml with the option for retrieving stats to true
+        String actionXml = setPigActionXml(PIG_SCRIPT, true);
+        _testSubmit(actionXml, true);
+    }
 
     public void testPigError() throws Exception {
+        // Set the action xml with the option for retrieving stats to true
+        String actionXml = setPigActionXml(ERROR_PIG_SCRIPT, true);
+        _testSubmit(actionXml, false);
+    }
+
+    private String setPigActionXml(String pigScript, boolean writeStats) throws IOException{
         FileSystem fs = getFileSystem();
 
         Path script = new Path(getAppPath(), "script.pig");
         Writer w = new OutputStreamWriter(fs.create(script));
-        w.write(ERROR_PIG_SCRIPT);
+        w.write(pigScript);
         w.close();
 
         Path inputDir = new Path(getFsTestCaseDir(), "input");
@@ -278,20 +372,14 @@ public class TestPigActionExecutor extends ActionExecutorTestCase {
         String actionXml = "<pig>" +
                 "<job-tracker>" + getJobTrackerUri() + "</job-tracker>" +
                 "<name-node>" + getNameNodeUri() + "</name-node>" +
-                getPigConfig().toXmlString(false) +
+                setPigConfig(writeStats).toXmlString(false) +
                 "<script>" + script.getName() + "</script>" +
                 "<param>IN=" + inputDir.toUri().getPath() + "</param>" +
                 "<param>OUT=" + outputDir.toUri().getPath() + "</param>" +
                 "</pig>";
-        _testSubmit(actionXml, false);
-    }
 
-    private static final String UDF_PIG_SCRIPT = "register udf.jar\n" +
-            "set job.name 'test'\n" + "set debug on\n" +
-            "A = load '$IN' using PigStorage(':');\n" +
-            "B = foreach A generate" +
-            "       org.apache.oozie.action.hadoop.UDFTester($0) as id;\n" +
-            "store B into '$OUT' USING PigStorage();\n";
+        return actionXml;
+    }
 
     public void testUdfPig() throws Exception {
         FileSystem fs = getFileSystem();
@@ -302,7 +390,6 @@ public class TestPigActionExecutor extends ActionExecutorTestCase {
         OutputStream os = getFileSystem().create(udfJar);
         IOUtils.copyStream(is, os);
 
-
         Path script = new Path(getAppPath(), "script.pig");
         Writer w = new OutputStreamWriter(fs.create(script));
         w.write(UDF_PIG_SCRIPT);
@@ -319,7 +406,7 @@ public class TestPigActionExecutor extends ActionExecutorTestCase {
         String actionXml = "<pig>" +
                 "<job-tracker>" + getJobTrackerUri() + "</job-tracker>" +
                 "<name-node>" + getNameNodeUri() + "</name-node>" +
-                getPigConfig().toXmlString(false) +
+                setPigConfig(true).toXmlString(false) +
                 "<script>" + script.getName() + "</script>" +
                 "<param>IN=" + inputDir.toUri().getPath() + "</param>" +
                 "<param>OUT=" + outputDir.toUri().getPath() + "</param>" +
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestPigMain.java core/src/test/java/org/apache/oozie/action/hadoop/TestPigMain.java
index 383102d..c9ea552 100644
--- core/src/test/java/org/apache/oozie/action/hadoop/TestPigMain.java
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestPigMain.java
@@ -19,26 +19,21 @@ package org.apache.oozie.action.hadoop;
 
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.filecache.DistributedCache;
 import org.apache.oozie.util.XConfiguration;
-import org.apache.oozie.util.ClassUtils;
 import org.apache.oozie.util.IOUtils;
-import org.apache.pig.Main;
+import org.json.simple.JSONValue;
 
 import java.io.File;
-import java.io.FileInputStream;
 import java.io.FileOutputStream;
-import java.io.InputStream;
 import java.io.OutputStream;
 import java.io.OutputStreamWriter;
 import java.io.Writer;
 import java.io.FileWriter;
 import java.io.FileReader;
+import java.util.Map;
 import java.util.Properties;
 import java.net.URL;
 
-import jline.ConsoleReaderInputStream;
-
 public class TestPigMain extends PigTestCase {
     private SecurityManager SECURITY_MANAGER;
 
@@ -58,16 +53,6 @@ public class TestPigMain extends PigTestCase {
     public Void call() throws Exception {
         FileSystem fs = getFileSystem();
 
-        Path pigJar = new Path(getFsTestCaseDir(), "pig.jar");
-        InputStream is = new FileInputStream(ClassUtils.findContainingJar(Main.class));
-        OutputStream os = fs.create(pigJar);
-        IOUtils.copyStream(is, os);
-
-        Path jlineJar = new Path(getFsTestCaseDir(), "jline.jar");
-        is = new FileInputStream(ClassUtils.findContainingJar(ConsoleReaderInputStream.class));
-        os = fs.create(jlineJar);
-        IOUtils.copyStream(is, os);
-
         Path script = new Path(getTestCaseDir(), "script.pig");
         Writer w = new FileWriter(script.toString());
         w.write(pigScript);
@@ -90,24 +75,33 @@ public class TestPigMain extends PigTestCase {
         jobConf.setInt("mapred.reduce.max.attempts", 1);
         jobConf.set("mapred.job.tracker", getJobTrackerUri());
         jobConf.set("fs.default.name", getNameNodeUri());
+
+        jobConf.set("mapreduce.framework.name", "yarn");
+
+        // option to specify whether stats should be stored or not
+        jobConf.set("oozie.action.external.stats.write", Boolean.toString(writeStats));
+
         injectKerberosInfo(jobConf);
 
-        DistributedCache.addFileToClassPath(new Path(pigJar.toUri().getPath()), getFileSystem().getConf());
-        DistributedCache.addFileToClassPath(new Path(jlineJar.toUri().getPath()), getFileSystem().getConf());
+        SharelibUtils.addToDistributedCache("pig", fs, getFsTestCaseDir(), jobConf);
 
         PigMain.setPigScript(jobConf, script.toString(), new String[] { "IN=" + inputDir.toUri().getPath(),
                 "OUT=" + outputDir.toUri().getPath() }, new String[] { "-v" });
 
         File actionXml = new File(getTestCaseDir(), "action.xml");
-        os = new FileOutputStream(actionXml);
+        OutputStream os = new FileOutputStream(actionXml);
         jobConf.writeXml(os);
         os.close();
 
-        File outputDataFile = new File(getTestCaseDir(), "outputdata.properties");
+        File statsDataFile = new File(getTestCaseDir(), "statsdata.properties");
+
+        File hadoopIdsFile = new File(getTestCaseDir(), "hadoopIds.properties");
 
         setSystemProperty("oozie.launcher.job.id", "" + System.currentTimeMillis());
         setSystemProperty("oozie.action.conf.xml", actionXml.getAbsolutePath());
-        setSystemProperty("oozie.action.output.properties", outputDataFile.getAbsolutePath());
+        setSystemProperty("oozie.action.stats.properties", statsDataFile.getAbsolutePath());
+        setSystemProperty("oozie.action.externalChildIDs.properties", hadoopIdsFile.getAbsolutePath());
+
 
         URL url = Thread.currentThread().getContextClassLoader().getResource("PigMain.txt");
         File classPathDir = new File(url.getPath()).getParentFile();
@@ -141,12 +135,21 @@ public class TestPigMain extends PigTestCase {
             System.setProperty("user.name", user);
         }
 
-        assertTrue(outputDataFile.exists());
-        props = new Properties();
-        props.load(new FileReader(outputDataFile));
-        assertTrue(props.containsKey("hadoopJobs"));
-        assertNotSame("", props.getProperty("hadoopJobs"));
-
+        // Stats should be stored only if option to write stats is set to true
+        if (writeStats) {
+            assertTrue(statsDataFile.exists());
+            String stats = IOUtils.getReaderAsString(new FileReader(statsDataFile), -1);
+            // check for some of the expected key values in the stats
+            Map m = (Map) JSONValue.parse(stats);
+            // check for expected 1st level JSON keys
+            assertTrue(m.containsKey("PIG_VERSION"));
+        } else {
+            assertFalse(statsDataFile.exists());
+        }
+        // HadoopIds should always be stored
+        assertTrue(hadoopIdsFile.exists());
+        String externalChildIds = IOUtils.getReaderAsString(new FileReader(hadoopIdsFile), -1);
+        assertTrue(externalChildIds.contains("job_"));
         return null;
     }
 
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestPipesMain.java core/src/test/java/org/apache/oozie/action/hadoop/TestPipesMain.java
index a9f542f..c8c2c0e 100644
--- core/src/test/java/org/apache/oozie/action/hadoop/TestPipesMain.java
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestPipesMain.java
@@ -37,18 +37,8 @@ import java.net.URI;
 public class TestPipesMain extends MainTestCase {
 
     public static String getProgramName(XTestCase tc) {
-        String hadoopVersion = tc.getHadoopVersion();
-
-        String hadoopSuffix = null;
         String osSuffix = null;
 
-        if (hadoopVersion.startsWith("0.20")) {
-            hadoopSuffix = "h20";
-        }
-        else {
-            fail();
-        }
-
         if (System.getProperty("os.name").startsWith("Mac") && System.getProperty("os.arch").equals("x86_64")) {
             osSuffix = "Mac_OS_X-x86_64-64";
         }
@@ -62,7 +52,7 @@ public class TestPipesMain extends MainTestCase {
             fail();
         }
 
-        return "wordcount-simple" + "_" + osSuffix + "_" + hadoopSuffix;
+        return "wordcount-simple" + "_" + osSuffix + "_h20";
     }
 
     public Void call() throws Exception {
@@ -95,6 +85,7 @@ public class TestPipesMain extends MainTestCase {
 
         jobConf.set("mapred.job.tracker", getJobTrackerUri());
         jobConf.set("fs.default.name", getNameNodeUri());
+        jobConf.set("mapreduce.framework.name", "yarn");
 
         jobConf.set("mapred.input.dir", inputDir.toString());
         jobConf.set("mapred.output.dir", outputDir.toString());
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestShellActionExecutor.java core/src/test/java/org/apache/oozie/action/hadoop/TestShellActionExecutor.java
new file mode 100644
index 0000000..95e0efa
--- /dev/null
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestShellActionExecutor.java
@@ -0,0 +1,300 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.action.hadoop;
+
+import java.io.File;
+import java.io.OutputStreamWriter;
+import java.io.Writer;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.JobID;
+import org.apache.hadoop.mapred.RunningJob;
+import org.apache.oozie.WorkflowActionBean;
+import org.apache.oozie.WorkflowJobBean;
+import org.apache.oozie.client.OozieClient;
+import org.apache.oozie.client.WorkflowAction;
+import org.apache.oozie.service.HadoopAccessorService;
+import org.apache.oozie.service.Services;
+import org.apache.oozie.service.WorkflowAppService;
+import org.apache.oozie.util.PropertiesUtils;
+import org.apache.oozie.util.XConfiguration;
+import org.apache.oozie.util.XmlUtils;
+import org.jdom.Element;
+
+public class TestShellActionExecutor extends ActionExecutorTestCase {
+
+    private static final String SHELL_SCRIPT_CONTENT = "ls -ltr\necho $1 $2\necho $PATH\npwd\ntype sh";
+    private static final String SHELL_SCRIPT_CONTENT_ERROR = "ls -ltr\necho $1 $2\nexit 1";
+    private static final String PERL_SCRIPT_CONTENT = "print \"MY_VAR=TESTING\";";
+
+    @Override
+    protected void setSystemProps() {
+        super.setSystemProps();
+        setSystemProperty("oozie.service.ActionService.executor.classes", ShellActionExecutor.class.getName());
+    }
+
+    /**
+     * Verify if the launcher jar is created.
+     *
+     * @throws Exception
+     */
+    public void testLauncherJar() throws Exception {
+        ShellActionExecutor ae = new ShellActionExecutor();
+        Path jar = new Path(ae.getOozieRuntimeDir(), ae.getLauncherJarName());
+        assertTrue(new File(jar.toString()).exists());
+    }
+
+    /**
+     * Verify if the ShellActionExecutor indeed setups the basic stuffs
+     *
+     * @throws Exception
+     */
+    public void testSetupMethods() throws Exception {
+        ShellActionExecutor ae = new ShellActionExecutor();
+
+        assertEquals("shell", ae.getType());// ActionExcutor type is 'shell'
+        // Verify the launcher jar filename
+        assertEquals("shell-launcher.jar", ae.getLauncherJarName());
+
+        List<Class> classes = new ArrayList<Class>();
+        classes.add(LauncherMapper.class);
+        classes.add(LauncherSecurityManager.class);
+        classes.add(LauncherException.class);
+        classes.add(LauncherMainException.class);
+        classes.add(ActionStats.class);
+        classes.add(ActionType.class);
+        classes.add(LauncherMain.class);
+        classes.add(MapReduceMain.class);
+        classes.add(ShellMain.class);
+        classes.add(ShellMain.OutputWriteThread.class);
+        assertEquals(classes, ae.getLauncherClasses());// Verify the class
+
+        Element actionXml = XmlUtils.parseXml("<shell>" + "<job-tracker>" + getJobTrackerUri() + "</job-tracker>"
+                + "<name-node>" + getNameNodeUri() + "</name-node>" + "<exec>SCRIPT</exec>"
+                + "<argument>a=A</argument>" + "<argument>b=B</argument>" + "</shell>");
+
+        XConfiguration protoConf = new XConfiguration();
+        protoConf.set(WorkflowAppService.HADOOP_USER, getTestUser());
+        protoConf.set(WorkflowAppService.HADOOP_UGI, getTestUser() + "," + getTestGroup());
+        protoConf.set(OozieClient.GROUP_NAME, getTestGroup());
+        injectKerberosInfo(protoConf);
+
+        WorkflowJobBean wf = createBaseWorkflow(protoConf, "pig-action");
+        WorkflowActionBean action = (WorkflowActionBean) wf.getActions().get(0);
+        action.setType(ae.getType());
+
+        Context context = new Context(wf, action);
+
+        Configuration conf = ae.createBaseHadoopConf(context, actionXml);
+        ae.setupActionConf(conf, context, actionXml, getFsTestCaseDir());
+        assertEquals("SCRIPT", conf.get("oozie.shell.exec"));
+        assertEquals("2", conf.get("oozie.shell.args.size"));
+        assertEquals("a=A", conf.get("oozie.shell.args.0"));
+        assertEquals("b=B", conf.get("oozie.shell.args.1"));
+    }
+
+    /**
+     * test if a sample shell script could run successfully
+     *
+     * @throws Exception
+     */
+    public void testShellScript() throws Exception {
+        FileSystem fs = getFileSystem();
+        // Create the script file with canned shell command
+        Path script = new Path(getAppPath(), "script.sh");
+        Writer w = new OutputStreamWriter(fs.create(script));
+        w.write(SHELL_SCRIPT_CONTENT);
+        w.close();
+
+        // Create sample Shell action xml
+        String actionXml = "<shell>" + "<job-tracker>" + getJobTrackerUri() + "</job-tracker>" + "<name-node>"
+                + getNameNodeUri() + "</name-node>" + "<exec>sh</exec>" + "<argument>-c</argument>"
+                + "<argument>script.sh</argument>" + "<argument>A</argument>" + "<argument>B</argument>"
+                + "<env-var>var1=val1</env-var>" + "<env-var>var2=val2</env-var>" + "<file>" + script.toString()
+                + "#" + script.getName() + "</file>" + "</shell>";
+        // Submit and verify the job's status
+        _testSubmit(actionXml, true, "");
+    }
+
+    /**
+     * test if a sample shell script could run with error when the script return
+     * non-zero exit code
+     *
+     * @throws Exception
+     */
+    public void testShellScriptError() throws Exception {
+        FileSystem fs = getFileSystem();
+        // Create the script file with canned shell command
+        Path script = new Path(getAppPath(), "script.sh");
+        Writer w = new OutputStreamWriter(fs.create(script));
+        w.write(SHELL_SCRIPT_CONTENT_ERROR);
+        w.close();
+
+        // Create sample shell action xml
+        String actionXml = "<shell>" + "<job-tracker>" + getJobTrackerUri() + "</job-tracker>" + "<name-node>"
+                + getNameNodeUri() + "</name-node>" + "<exec>sh</exec>" + "<argument>-c</argument>"
+                + "<argument>script.sh</argument>" + "<argument>A</argument>" + "<argument>B</argument>" + "<file>"
+                + script.toString() + "#" + script.getName() + "</file>" + "</shell>";
+        // Submit and verify the job's status
+        _testSubmit(actionXml, false, "");
+    }
+
+    /**
+     * test if a perl script could run successfully
+     *
+     * @throws Exception
+     */
+    public void testPerlScript() throws Exception {
+        FileSystem fs = getFileSystem();
+        // Create a sample perl script
+        Path script = new Path(getAppPath(), "script.pl");
+        Writer w = new OutputStreamWriter(fs.create(script));
+        w.write(PERL_SCRIPT_CONTENT);
+        w.close();
+        // Create a Sample Shell action using the perl script
+        String actionXml = "<shell>" + "<job-tracker>" + getJobTrackerUri() + "</job-tracker>" + "<name-node>"
+                + getNameNodeUri() + "</name-node>" + "<exec>perl</exec>" + "<argument>script.pl</argument>"
+                + "<argument>A</argument>" + "<argument>B</argument>" + "<env-var>my_var1=my_val1</env-var>" + "<file>"
+                + script.toString() + "#" + script.getName() + "</file>" + "<capture-output/>" + "</shell>";
+        _testSubmit(actionXml, true, "TESTING");
+    }
+
+    /**
+     * Submit the WF with a Shell action and very of the job succeeds
+     *
+     * @param actionXml
+     * @param checkForSuccess
+     * @throws Exception
+     */
+    private void _testSubmit(String actionXml, boolean checkForSuccess, String capture_output) throws Exception {
+
+        Context context = createContext(actionXml);
+        final RunningJob launcherJob = submitAction(context);// Submit the
+        // action
+        String launcherId = context.getAction().getExternalId(); // Get LM id
+        waitFor(180 * 1000, new Predicate() { // Wait for the external job to
+                    // finish
+                    public boolean evaluate() throws Exception {
+                        return launcherJob.isComplete();
+                    }
+                });
+        // Thread.sleep(2000);
+        assertTrue(launcherJob.isSuccessful());
+
+        Thread.sleep(2000);// Wait more to make sure no ID swap happens
+        assertFalse(LauncherMapper.hasIdSwap(launcherJob));
+
+        ShellActionExecutor ae = new ShellActionExecutor();
+        ae.check(context, context.getAction());
+        ae.end(context, context.getAction());
+        assertTrue(launcherId.equals(context.getAction().getExternalId()));
+
+        if (checkForSuccess) { // Postive test cases
+            assertEquals("SUCCEEDED", context.getAction().getExternalStatus());
+            // Testing capture output
+            if (capture_output != null && capture_output.length() > 0) {
+                assertEquals(capture_output, PropertiesUtils.stringToProperties(context.getAction().getData())
+                        .getProperty("MY_VAR"));
+            }
+        }
+        else { // Negative test cases
+            assertEquals("FAILED/KILLED", context.getAction().getExternalStatus());
+            assertNotNull(context.getAction().getErrorMessage());
+        }
+        if (checkForSuccess) { // Positive test cases
+            assertEquals(WorkflowAction.Status.OK, context.getAction().getStatus());
+        }
+        else {// Negative test cases
+            assertEquals(WorkflowAction.Status.ERROR, context.getAction().getStatus());
+        }
+    }
+
+    /**
+     * Create WF using the Shell action and return the corresponding contest
+     * structure
+     *
+     * @param actionXml :Shell action
+     * @return :Context
+     * @throws Exception
+     */
+    private Context createContext(String actionXml) throws Exception {
+        ShellActionExecutor ae = new ShellActionExecutor();
+
+        XConfiguration protoConf = new XConfiguration();
+        protoConf.set(WorkflowAppService.HADOOP_USER, getTestUser());
+        protoConf.set(WorkflowAppService.HADOOP_UGI, getTestUser() + "," + getTestGroup());
+        protoConf.set(OozieClient.GROUP_NAME, getTestGroup());
+        // Make sure Kerbores prinicpal is in the conf
+        injectKerberosInfo(protoConf);
+
+        WorkflowJobBean wf = createBaseWorkflow(protoConf, "shell-action");
+        WorkflowActionBean action = (WorkflowActionBean) wf.getActions().get(0);
+        action.setType(ae.getType());
+        action.setConf(actionXml);
+
+        return new Context(wf, action);
+    }
+
+    /**
+     * Submit the Shell action using Shell ActionExecutor
+     *
+     * @param context
+     * @return The RunningJob of the Launcher Mapper
+     * @throws Exception
+     */
+    private RunningJob submitAction(Context context) throws Exception {
+        ShellActionExecutor ae = new ShellActionExecutor();
+
+        WorkflowAction action = context.getAction();
+
+        ae.prepareActionDir(getFileSystem(), context);
+        ae.submitLauncher(getFileSystem(), context, action); // Submit the
+        // Launcher Mapper
+
+        String jobId = action.getExternalId();
+        String jobTracker = action.getTrackerUri();
+        String consoleUrl = action.getConsoleUrl();
+
+        assertNotNull(jobId);
+        assertNotNull(jobTracker);
+        assertNotNull(consoleUrl);
+
+        Element e = XmlUtils.parseXml(action.getConf());
+        XConfiguration conf = new XConfiguration();
+        conf.set("mapred.job.tracker", e.getChildTextTrim("job-tracker"));
+        conf.set("fs.default.name", e.getChildTextTrim("name-node"));
+        conf.set("user.name", context.getProtoActionConf().get("user.name"));
+        conf.set("group.name", getTestGroup());
+        injectKerberosInfo(conf);
+        JobConf jobConf = new JobConf();
+        XConfiguration.copy(conf, jobConf);
+        String user = jobConf.get("user.name");
+        String group = jobConf.get("group.name");
+        JobClient jobClient = Services.get().get(HadoopAccessorService.class).createJobClient(user, group, jobConf);
+        final RunningJob runningJob = jobClient.getJob(JobID.forName(jobId));
+        assertNotNull(runningJob);
+        return runningJob;
+    }
+
+}
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestShellMain.java core/src/test/java/org/apache/oozie/action/hadoop/TestShellMain.java
new file mode 100644
index 0000000..0f31fce
--- /dev/null
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestShellMain.java
@@ -0,0 +1,89 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.oozie.action.hadoop;
+
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.FileWriter;
+import java.io.Writer;
+import java.util.Properties;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.oozie.util.XConfiguration;
+
+//Test cases are mainly implemented in the Base class
+
+public class TestShellMain extends ShellTestCase {
+
+    @Override
+    public Void call() throws Exception {
+        Path script = new Path(getTestCaseDir(), scriptName);
+        Writer w = new FileWriter(script.toString());
+        w.write(scriptContent);
+        w.close();
+        File fl = new File(getTestCaseDir(), scriptName);
+        fl.setExecutable(true);
+
+        XConfiguration jobConf = new XConfiguration();
+
+        jobConf.set("user.name", getTestUser());
+        jobConf.set("group.name", getTestGroup());
+        jobConf.setInt("mapred.map.tasks", 1);
+        jobConf.setInt("mapred.map.max.attempts", 1);
+        jobConf.setInt("mapred.reduce.max.attempts", 1);
+        jobConf.set("mapred.job.tracker", getJobTrackerUri());
+        jobConf.set("fs.default.name", getNameNodeUri());
+        injectKerberosInfo(jobConf);
+
+        jobConf.set(ShellMain.CONF_OOZIE_SHELL_EXEC, "sh");
+        MapReduceMain.setStrings(jobConf, ShellMain.CONF_OOZIE_SHELL_ARGS, new String[] { "-c",
+                getTestCaseDir() + "/" + scriptName, "A", "B" });
+        MapReduceMain.setStrings(jobConf, ShellMain.CONF_OOZIE_SHELL_ENVS,
+                new String[] { "var1=value1", "var2=value2" });
+
+        File actionXml = new File(getTestCaseDir(), "action.xml");
+        FileOutputStream os = new FileOutputStream(actionXml);
+        jobConf.writeXml(os);
+        os.close();
+
+        setSystemProperty("oozie.launcher.job.id", "" + System.currentTimeMillis());
+        setSystemProperty("oozie.action.conf.xml", actionXml.getAbsolutePath());
+
+        Properties props = jobConf.toProperties();
+        // Test arguments count
+        assertEquals(props.getProperty(ShellMain.CONF_OOZIE_SHELL_ARGS + ".size"), "4");
+        // Test environment count
+        assertEquals(props.getProperty(ShellMain.CONF_OOZIE_SHELL_ENVS + ".size"), "2");
+
+        try {
+            ShellMain.main(null);
+            if (expectedSuccess == false) {
+                fail("Expected to succeed");
+            }
+        }
+        catch (LauncherMainException le) {
+            if (expectedSuccess == true) {
+                fail("Should be succeesfull");
+            }
+        }
+
+        return null;
+    }
+
+}
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestSqoopActionExecutor.java core/src/test/java/org/apache/oozie/action/hadoop/TestSqoopActionExecutor.java
new file mode 100644
index 0000000..5ab829e
--- /dev/null
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestSqoopActionExecutor.java
@@ -0,0 +1,321 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.action.hadoop;
+
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.JobID;
+import org.apache.hadoop.mapred.RunningJob;
+import org.apache.oozie.WorkflowActionBean;
+import org.apache.oozie.WorkflowJobBean;
+import org.apache.oozie.client.OozieClient;
+import org.apache.oozie.client.WorkflowAction;
+import org.apache.oozie.service.HadoopAccessorService;
+import org.apache.oozie.service.Services;
+import org.apache.oozie.service.WorkflowAppService;
+import org.apache.oozie.util.IOUtils;
+import org.apache.oozie.util.XConfiguration;
+import org.apache.oozie.util.XmlUtils;
+import org.jdom.Element;
+import org.jdom.Namespace;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.io.OutputStream;
+import java.io.StringReader;
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.Statement;
+import java.text.MessageFormat;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Properties;
+
+public class TestSqoopActionExecutor extends ActionExecutorTestCase {
+
+    private static final String SQOOP_COMMAND = "import --connect {0} --table TT --target-dir {1} -m 1";
+
+    private static final String SQOOP_ACTION_COMMAND_XML =
+            "<sqoop xmlns=\"uri:oozie:sqoop-action:0.1\">" +
+            "<job-tracker>{0}</job-tracker>" +
+            "<name-node>{1}</name-node>" +
+            "<configuration>" +
+            "<property>" +
+            "<name>{2}</name>" +
+            "<value>{3}</value>" +
+            "</property>" +
+            "</configuration>" +
+            "<command>{4}</command>" +
+            "</sqoop>";
+
+    private static final String SQOOP_ACTION_ARGS_XML =
+            "<sqoop xmlns=\"uri:oozie:sqoop-action:0.1\">" +
+            "<job-tracker>{0}</job-tracker>" +
+            "<name-node>{1}</name-node>" +
+            "<configuration>" +
+            "<property>" +
+            "<name>oozie.sqoop.log.level</name>" +
+            "<value>INFO</value>" +
+            "</property>" +
+            "</configuration>" +
+            "<arg>import</arg>" +
+            "<arg>--connect</arg>" +
+            "<arg>{2}</arg>" +
+            "<arg>--username</arg>" +
+            "<arg>sa</arg>" +
+            "<arg>--password</arg>" +
+            "<arg></arg>" +
+            "<arg>--verbose</arg>" +
+            "<arg>--query</arg>" +
+            "<arg>{3}</arg>" +
+            "<arg>--target-dir</arg>" +
+            "<arg>{4}</arg>" +
+            "<arg>--split-by</arg>" +
+            "<arg>I</arg>" +
+            "</sqoop>";
+
+    protected void setSystemProps() {
+        super.setSystemProps();
+        setSystemProperty("oozie.service.ActionService.executor.ext.classes", SqoopActionExecutor.class.getName());
+    }
+
+    public void testSetupMethods() throws Exception {
+        SqoopActionExecutor ae = new SqoopActionExecutor();
+        assertEquals("sqoop", ae.getType());
+    }
+
+    public void testLauncherJar() throws Exception {
+        SqoopActionExecutor ae = new SqoopActionExecutor();
+        Path jar = new Path(ae.getOozieRuntimeDir(), ae.getLauncherJarName());
+        assertTrue(new File(jar.toString()).exists());
+    }
+
+    private String getDbFile() {
+        return "db.hsqldb";
+    }
+
+    private String getDbPath() {
+        return new File(getTestCaseDir(), getDbFile()).getAbsolutePath();
+    }
+
+    private String getLocalJdbcUri() {
+        return "jdbc:hsqldb:file:" + getDbPath() + ";shutdown=true";
+    }
+
+    private String getActionJdbcUri() {
+        return "jdbc:hsqldb:file:" + getDbFile();
+    }
+
+    private String getSqoopOutputDir() {
+        return new Path(getFsTestCaseDir(), "output").toString();
+    }
+
+    private String getActionXml() {
+        String command = MessageFormat.format(SQOOP_COMMAND, getActionJdbcUri(), getSqoopOutputDir());
+        return MessageFormat.format(SQOOP_ACTION_COMMAND_XML, getJobTrackerUri(), getNameNodeUri(),
+                                    "dummy", "dummyValue", command);
+    }
+
+    private String getActionXmlFreeFromQuery() {
+        String query = "select TT.I, TT.S from TT where $CONDITIONS";
+        return MessageFormat.format(SQOOP_ACTION_ARGS_XML, getJobTrackerUri(), getNameNodeUri(),
+                                    getActionJdbcUri(), query, getSqoopOutputDir());
+    }
+
+    private void createDB() throws Exception {
+        Class.forName("org.hsqldb.jdbcDriver");
+        Connection conn = DriverManager.getConnection(getLocalJdbcUri(), "sa", "");
+        Statement st = conn.createStatement();
+        st.executeUpdate("CREATE TABLE TT (I INTEGER PRIMARY KEY, S VARCHAR(256))");
+        st.executeUpdate("INSERT INTO TT (I, S) VALUES (1, 'a')");
+        st.executeUpdate("INSERT INTO TT (I, S) VALUES (2, 'a')");
+        st.executeUpdate("INSERT INTO TT (I, S) VALUES (3, 'a')");
+        st.close();
+        conn.close();
+    }
+
+    public void testSqoopAction() throws Exception {
+        createDB();
+
+        Context context = createContext(getActionXml());
+        final RunningJob launcherJob = submitAction(context);
+        String launcherId = context.getAction().getExternalId();
+        waitFor(120 * 1000, new Predicate() {
+            public boolean evaluate() throws Exception {
+                return launcherJob.isComplete();
+            }
+        });
+        assertTrue(launcherJob.isSuccessful());
+
+        assertFalse(LauncherMapper.hasIdSwap(launcherJob));
+
+        SqoopActionExecutor ae = new SqoopActionExecutor();
+        ae.check(context, context.getAction());
+        assertTrue(launcherId.equals(context.getAction().getExternalId()));
+        assertEquals("SUCCEEDED", context.getAction().getExternalStatus());
+        assertNotNull(context.getAction().getData());
+        ae.end(context, context.getAction());
+        assertEquals(WorkflowAction.Status.OK, context.getAction().getStatus());
+        FileSystem fs = getFileSystem();
+        BufferedReader br = new BufferedReader(new InputStreamReader(fs.open(new Path(getSqoopOutputDir(), "part-m-00000"))));
+        int count = 0;
+        String line = br.readLine();
+        while (line != null) {
+            assertTrue(line.contains("a"));
+            count++;
+            line = br.readLine();
+        }
+        br.close();
+        assertEquals(3, count);
+
+        assertNotNull(context.getAction().getData());
+        Properties outputData = new Properties();
+        outputData.load(new StringReader(context.getAction().getData()));
+        assertTrue(outputData.containsKey(LauncherMain.HADOOP_JOBS));
+        assertTrue(outputData.getProperty(LauncherMain.HADOOP_JOBS).trim().length() > 0);
+    }
+
+    public void testSqoopActionFreeFormQuery() throws Exception {
+        createDB();
+
+        Context context = createContext(getActionXmlFreeFromQuery());
+        final RunningJob launcherJob = submitAction(context);
+        String launcherId = context.getAction().getExternalId();
+        waitFor(120 * 1000, new Predicate() {
+            public boolean evaluate() throws Exception {
+                return launcherJob.isComplete();
+            }
+        });
+        assertTrue(launcherJob.isSuccessful());
+
+        assertFalse(LauncherMapper.hasIdSwap(launcherJob));
+
+        SqoopActionExecutor ae = new SqoopActionExecutor();
+        ae.check(context, context.getAction());
+        assertTrue(launcherId.equals(context.getAction().getExternalId()));
+        assertEquals("SUCCEEDED", context.getAction().getExternalStatus());
+        assertNotNull(context.getAction().getData());
+        ae.end(context, context.getAction());
+        assertEquals(WorkflowAction.Status.OK, context.getAction().getStatus());
+        FileSystem fs = getFileSystem();
+        FileStatus[] parts = fs.listStatus(new Path(getSqoopOutputDir()), new PathFilter() {
+            @Override
+            public boolean accept(Path path) {
+                return path.getName().startsWith("part-");
+            }
+        });
+        int count = 0;
+        for (FileStatus part : parts) {
+            BufferedReader br = new BufferedReader(new InputStreamReader(fs.open(part.getPath())));
+            String line = br.readLine();
+            while (line != null) {
+                assertTrue(line.contains("a"));
+                count++;
+                line = br.readLine();
+            }
+            br.close();
+        }
+        assertEquals(3, count);
+
+        assertNotNull(context.getAction().getData());
+        Properties outputData = new Properties();
+        outputData.load(new StringReader(context.getAction().getData()));
+        assertTrue(outputData.containsKey(LauncherMain.HADOOP_JOBS));
+        assertTrue(outputData.getProperty(LauncherMain.HADOOP_JOBS).trim().length() > 0);
+    }
+
+
+    private RunningJob submitAction(Context context) throws Exception {
+        SqoopActionExecutor ae = new SqoopActionExecutor();
+
+        WorkflowAction action = context.getAction();
+
+        ae.prepareActionDir(getFileSystem(), context);
+        ae.submitLauncher(getFileSystem(), context, action);
+
+        String jobId = action.getExternalId();
+        String jobTracker = action.getTrackerUri();
+        String consoleUrl = action.getConsoleUrl();
+        assertNotNull(jobId);
+        assertNotNull(jobTracker);
+        assertNotNull(consoleUrl);
+        Element e = XmlUtils.parseXml(action.getConf());
+        Namespace ns = Namespace.getNamespace("uri:oozie:sqoop-action:0.1");
+        XConfiguration conf = new XConfiguration(
+                new StringReader(XmlUtils.prettyPrint(e.getChild("configuration", ns)).toString()));
+        conf.set("mapred.job.tracker", e.getChildTextTrim("job-tracker", ns));
+        conf.set("fs.default.name", e.getChildTextTrim("name-node", ns));
+        conf.set("user.name", context.getProtoActionConf().get("user.name"));
+        conf.set("mapreduce.framework.name", "yarn");
+        conf.set("group.name", getTestGroup());
+        injectKerberosInfo(conf);
+        JobConf jobConf = new JobConf();
+        XConfiguration.copy(conf, jobConf);
+        String user = jobConf.get("user.name");
+        String group = jobConf.get("group.name");
+        JobClient jobClient = Services.get().get(HadoopAccessorService.class).createJobClient(user, group, jobConf);
+        final RunningJob runningJob = jobClient.getJob(JobID.forName(jobId));
+        assertNotNull(runningJob);
+        return runningJob;
+    }
+
+    private Context createContext(String actionXml) throws Exception {
+        SqoopActionExecutor ae = new SqoopActionExecutor();
+
+        XConfiguration protoConf = new XConfiguration();
+        protoConf.set(WorkflowAppService.HADOOP_USER, getTestUser());
+        protoConf.set(WorkflowAppService.HADOOP_UGI, getTestUser() + "," + getTestGroup());
+        protoConf.set(OozieClient.GROUP_NAME, getTestGroup());
+        injectKerberosInfo(protoConf);
+
+        FileSystem fs = getFileSystem();
+        SharelibUtils.addToDistributedCache("sqoop", fs, getFsTestCaseDir(), protoConf);
+
+        protoConf.setStrings(WorkflowAppService.APP_LIB_PATH_LIST, copyDbToHdfs());
+
+        WorkflowJobBean wf = createBaseWorkflow(protoConf, "sqoop-action");
+        WorkflowActionBean action = (WorkflowActionBean) wf.getActions().get(0);
+        action.setType(ae.getType());
+        action.setConf(actionXml);
+
+        return new Context(wf, action);
+    }
+
+    private String[] copyDbToHdfs() throws Exception {
+        List<String> files = new ArrayList<String>();
+        String[] exts = {".script", ".properties"};
+        for (String ext : exts) {
+            String file = getDbPath() + ext;
+            String name = getDbFile() + ext;
+            Path targetPath = new Path(getAppPath(), name);
+            FileSystem fs = getFileSystem();
+            InputStream is = new FileInputStream(file);
+            OutputStream os = fs.create(new Path(getAppPath(), targetPath));
+            IOUtils.copyStream(is, os);
+            files.add(targetPath.toString() + "#" + name);
+        }
+        return files.toArray(new String[files.size()]);
+    }
+}
diff --git core/src/test/java/org/apache/oozie/action/hadoop/TestStreamingMain.java core/src/test/java/org/apache/oozie/action/hadoop/TestStreamingMain.java
index feb4dd6..2185c34 100644
--- core/src/test/java/org/apache/oozie/action/hadoop/TestStreamingMain.java
+++ core/src/test/java/org/apache/oozie/action/hadoop/TestStreamingMain.java
@@ -19,11 +19,7 @@ package org.apache.oozie.action.hadoop;
 
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.streaming.StreamJob;
-import org.apache.hadoop.filecache.DistributedCache;
 import org.apache.oozie.util.XConfiguration;
-import org.apache.oozie.util.ClassUtils;
-import org.apache.oozie.util.IOUtils;
 
 import java.io.File;
 import java.io.FileInputStream;
@@ -39,11 +35,6 @@ public class TestStreamingMain extends MainTestCase {
     public Void call() throws Exception {
         FileSystem fs = getFileSystem();
 
-        Path streamingJar = new Path(getFsTestCaseDir(), "hadoop-streaming.jar");
-        InputStream is = new FileInputStream(ClassUtils.findContainingJar(StreamJob.class));
-        OutputStream os = fs.create(streamingJar);
-        IOUtils.copyStream(is, os);
-
         Path inputDir = new Path(getFsTestCaseDir(), "input");
         fs.mkdirs(inputDir);
         Writer writer = new OutputStreamWriter(fs.create(new Path(inputDir, "data.txt")));
@@ -59,11 +50,12 @@ public class TestStreamingMain extends MainTestCase {
         jobConf.setInt("mapred.reduce.max.attempts", 1);
         jobConf.set("mapred.job.tracker", getJobTrackerUri());
         jobConf.set("fs.default.name", getNameNodeUri());
+        jobConf.set("mapreduce.framework.name", "yarn");
         injectKerberosInfo(jobConf);
         jobConf.set("user.name", getTestUser());
         jobConf.set("hadoop.job.ugi", getTestUser() + "," + getTestGroup());
 
-        DistributedCache.addFileToClassPath(new Path(streamingJar.toUri().getPath()), fs.getConf());
+        SharelibUtils.addToDistributedCache("streaming", fs, getFsTestCaseDir(), jobConf);
 
         StreamingMain.setStreaming(jobConf, "cat", "wc", null, null, null);
 
@@ -71,7 +63,7 @@ public class TestStreamingMain extends MainTestCase {
         jobConf.set("mapred.output.dir", outputDir.toString());
 
         File actionXml = new File(getTestCaseDir(), "action.xml");
-        os = new FileOutputStream(actionXml);
+        OutputStream os = new FileOutputStream(actionXml);
         jobConf.writeXml(os);
         os.close();
 
@@ -84,7 +76,7 @@ public class TestStreamingMain extends MainTestCase {
 
         assertTrue(newIdProperties.exists());
 
-        is = new FileInputStream(newIdProperties);
+        InputStream is = new FileInputStream(newIdProperties);
         Properties props = new Properties();
         props.load(is);
         is.close();
diff --git core/src/test/java/org/apache/oozie/action/ssh/TestSshActionExecutor.java core/src/test/java/org/apache/oozie/action/ssh/TestSshActionExecutor.java
index 75d556a..76b58e1 100644
--- core/src/test/java/org/apache/oozie/action/ssh/TestSshActionExecutor.java
+++ core/src/test/java/org/apache/oozie/action/ssh/TestSshActionExecutor.java
@@ -40,6 +40,7 @@ import org.apache.oozie.service.UUIDService;
 import org.apache.oozie.service.UUIDService.ApplicationType;
 import org.apache.oozie.test.XFsTestCase;
 import org.apache.oozie.util.ELEvaluator;
+import org.apache.oozie.util.PropertiesUtils;
 import org.apache.oozie.util.XConfiguration;
 
 public class TestSshActionExecutor extends XFsTestCase {
@@ -101,6 +102,16 @@ public class TestSshActionExecutor extends XFsTestCase {
         }
 
         @Override
+        public void setExecutionStats(String jsonStats){
+            action.setExecutionStats(jsonStats);
+        }
+
+        @Override
+        public void setExternalChildIDs(String externalChildIDs){
+            action.setExternalChildIDs(externalChildIDs);
+        }
+
+        @Override
         public void setStartData(String externalId, String trackerUri, String consoleUrl) {
             action.setStartData(externalId, trackerUri, consoleUrl);
         }
@@ -147,7 +158,10 @@ public class TestSshActionExecutor extends XFsTestCase {
         fs.delete(path, true);
     }
 
-/*
+    protected String getActionXMLSchema() {
+        return "uri:oozie-workflow:0.1";
+    }
+
     public void testJobStart() throws ActionExecutorException {
         String baseDir = getTestCaseDir();
         Path appPath = new Path(getNameNodeUri(), baseDir);
@@ -168,7 +182,7 @@ public class TestSshActionExecutor extends XFsTestCase {
 
         final WorkflowActionBean action = new WorkflowActionBean();
         action.setId("actionId");
-        action.setConf("<ssh xmlns='uri:oozie-workflow:0.1'>" +
+        action.setConf("<ssh xmlns='" + getActionXMLSchema() + "'>" +
                        "<host>localhost</host>" +
                        "<command>echo</command>" +
                        "<capture-output/>" +
@@ -189,9 +203,7 @@ public class TestSshActionExecutor extends XFsTestCase {
         assertEquals(Status.OK, action.getStatus());
         assertEquals("something", PropertiesUtils.stringToProperties(action.getData()).getProperty("prop1"));
     }
-*/
 
-/*
     public void testJobRecover() throws ActionExecutorException, InterruptedException {
         String baseDir = getTestCaseDir();
         Path appPath = new Path(getNameNodeUri(), baseDir);
@@ -212,7 +224,7 @@ public class TestSshActionExecutor extends XFsTestCase {
 
         final WorkflowActionBean action = new WorkflowActionBean();
         action.setId("actionId");
-        action.setConf("<ssh xmlns='uri:oozie-workflow:0.1'>" +
+        action.setConf("<ssh xmlns='" + getActionXMLSchema() + "'>" +
                        "<host>localhost</host>" +
                        "<command>echo</command>" +
                        "<capture-output/>" +
@@ -226,7 +238,7 @@ public class TestSshActionExecutor extends XFsTestCase {
         Thread.sleep(200);
         final WorkflowActionBean action1 = new WorkflowActionBean();
         action1.setId("actionId");
-        action1.setConf("<ssh xmlns='uri:oozie-workflow:0.1'>" +
+        action1.setConf("<ssh xmlns='" + getActionXMLSchema() + "'>" +
                        "<host>localhost</host>" +
                        "<command>echo</command>" +
                        "<capture-output/>" +
@@ -249,7 +261,7 @@ public class TestSshActionExecutor extends XFsTestCase {
         assertEquals(Status.OK, action1.getStatus());
         assertEquals("something", PropertiesUtils.stringToProperties(action1.getData()).getProperty("prop1"));
     }
-*/
+
 
     // TODO Move this test case over to a new class. Conflict between this one
     // and testConnectionErrors. The property to replace the ssh user cannot be
@@ -275,7 +287,7 @@ public class TestSshActionExecutor extends XFsTestCase {
 //
 //        final WorkflowActionBean action = new WorkflowActionBean();
 //        action.setId("actionId_" + System.currentTimeMillis());
-//        action.setConf("<ssh xmlns='uri:oozie-workflow:0.1'>" +
+//        action.setConf("<ssh xmlns='" + getActionXMLSchema() + "'>" +
 //                       "<host>invalid@localhost</host>" +
 //                       "<command>echo</command>" +
 //                       "<capture-output/>" +
@@ -315,7 +327,7 @@ public class TestSshActionExecutor extends XFsTestCase {
 
         final WorkflowActionBean action = new WorkflowActionBean();
         action.setId("actionId");
-        action.setConf("<ssh xmlns='uri:oozie-workflow:0.1'>" +
+        action.setConf("<ssh xmlns='" + getActionXMLSchema() + "'>" +
                 "<host>blabla</host>" +
                 "<command>echo</command>" +
                 "<args>\"prop1=something\"</args>" +
@@ -332,7 +344,7 @@ public class TestSshActionExecutor extends XFsTestCase {
             assertEquals("COULD_NOT_RESOLVE_HOST", ex.getErrorCode());
             assertEquals(ActionExecutorException.ErrorType.TRANSIENT, ex.getErrorType());
         }
-        action.setConf("<ssh xmlns='uri:oozie-workflow:0.1'>" +
+        action.setConf("<ssh xmlns='" + getActionXMLSchema() + "'>" +
                 "<host>11.11.11.11</host>" +
                 "<command>echo</command>" +
                 "<args>\"prop1=something\"</args>" +
@@ -346,7 +358,7 @@ public class TestSshActionExecutor extends XFsTestCase {
             assertEquals("COULD_NOT_CONNECT", ex.getErrorCode());
             assertEquals(ActionExecutorException.ErrorType.TRANSIENT, ex.getErrorType());
         }
-        action.setConf("<ssh xmlns='uri:oozie-workflow:0.1'>" +
+        action.setConf("<ssh xmlns='" + getActionXMLSchema() + "'>" +
                 "<host>y@localhost</host>" +
                 "<command>echo</command>" +
                 "<args>\"prop1=something\"</args>" +
diff --git core/src/test/java/org/apache/oozie/action/ssh/TestSshActionExecutorAsExtension.java core/src/test/java/org/apache/oozie/action/ssh/TestSshActionExecutorAsExtension.java
new file mode 100644
index 0000000..1915d2d
--- /dev/null
+++ core/src/test/java/org/apache/oozie/action/ssh/TestSshActionExecutorAsExtension.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.action.ssh;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.oozie.WorkflowActionBean;
+import org.apache.oozie.WorkflowJobBean;
+import org.apache.oozie.action.ActionExecutor;
+import org.apache.oozie.action.ActionExecutorException;
+import org.apache.oozie.client.OozieClient;
+import org.apache.oozie.client.WorkflowAction.Status;
+import org.apache.oozie.client.WorkflowJob;
+import org.apache.oozie.service.CallbackService;
+import org.apache.oozie.service.Services;
+import org.apache.oozie.service.UUIDService;
+import org.apache.oozie.service.UUIDService.ApplicationType;
+import org.apache.oozie.service.WorkflowAppService;
+import org.apache.oozie.test.XFsTestCase;
+import org.apache.oozie.util.ELEvaluator;
+import org.apache.oozie.util.PropertiesUtils;
+import org.apache.oozie.util.XConfiguration;
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.net.URISyntaxException;
+import java.util.Properties;
+
+public class TestSshActionExecutorAsExtension extends TestSshActionExecutor {
+
+    @Override
+    protected String getActionXMLSchema() {
+        return "uri:oozie:ssh-action:0.1";
+    }
+
+}
diff --git core/src/test/java/org/apache/oozie/client/TestOozieCLI.java core/src/test/java/org/apache/oozie/client/TestOozieCLI.java
index 8ba88d6..ae3430f 100644
--- core/src/test/java/org/apache/oozie/client/TestOozieCLI.java
+++ core/src/test/java/org/apache/oozie/client/TestOozieCLI.java
@@ -62,7 +62,6 @@ public class TestOozieCLI extends DagServletTestCase {
     private String createConfigFile(String appPath) throws Exception {
         String path = getTestCaseDir() + "/" + getName() + ".xml";
         Configuration conf = new Configuration(false);
-        conf.set(OozieClient.USER_NAME, getTestUser());
         conf.set(OozieClient.GROUP_NAME, getTestGroup());
         conf.set(OozieClient.APP_PATH, appPath);
         conf.set(OozieClient.RERUN_SKIP_NODES, "node");
@@ -145,6 +144,25 @@ public class TestOozieCLI extends DagServletTestCase {
         });
     }
 
+    public void testSubmitDoAs() throws Exception {
+        runTest(END_POINTS, SERVLET_CLASSES, IS_SECURITY_ENABLED, new Callable<Void>() {
+            public Void call() throws Exception {
+                String oozieUrl = getContextURL();
+
+                Path appPath = new Path(getFsTestCaseDir(), "app");
+                getFileSystem().mkdirs(appPath);
+                getFileSystem().create(new Path(appPath, "workflow.xml")).close();
+
+                String[] args = new String[]{"job", "-submit", "-oozie", oozieUrl, "-config",
+                    createConfigFile(appPath.toString()), "-doas", getTestUser2() };
+                assertEquals(0, new OozieCLI().run(args));
+                assertEquals("submit", MockDagEngineService.did);
+                assertEquals(getTestUser2(), MockDagEngineService.user);
+                return null;
+            }
+        });
+    }
+
     public void testSubmitWithPropertyArguments() throws Exception {
         runTest(END_POINTS, SERVLET_CLASSES, IS_SECURITY_ENABLED, new Callable<Void>() {
             public Void call() throws Exception {
diff --git core/src/test/java/org/apache/oozie/client/rest/TestJsonWorkflowAction.java core/src/test/java/org/apache/oozie/client/rest/TestJsonWorkflowAction.java
index 7400c1e..561dbc2 100644
--- core/src/test/java/org/apache/oozie/client/rest/TestJsonWorkflowAction.java
+++ core/src/test/java/org/apache/oozie/client/rest/TestJsonWorkflowAction.java
@@ -37,6 +37,8 @@ public class TestJsonWorkflowAction extends TestCase {
         action.setEndTime(JsonUtils.parseDateRfc822(END_TIME));
         action.setTransition("e");
         action.setData("ee");
+        action.setStats("stats");
+        action.setExternalChildIDs("extChIDs");
         action.setExternalId("f");
         action.setExternalStatus("g");
         action.setTrackerUri("h");
@@ -57,6 +59,8 @@ public class TestJsonWorkflowAction extends TestCase {
         Assert.assertEquals(JsonUtils.parseDateRfc822(END_TIME), action.getEndTime());
         Assert.assertEquals("e", action.getTransition());
         Assert.assertEquals("ee", action.getData());
+        Assert.assertEquals("stats", action.getStats());
+        Assert.assertEquals("extChIDs", action.getExternalChildIDs());
         Assert.assertEquals("f", action.getExternalId());
         Assert.assertEquals("g", action.getExternalStatus());
         Assert.assertEquals("h", action.getTrackerUri());
diff --git core/src/test/java/org/apache/oozie/command/coord/TestCoordActionInputCheckXCommand.java core/src/test/java/org/apache/oozie/command/coord/TestCoordActionInputCheckXCommand.java
index 6dbaf94..bb42dcf 100644
--- core/src/test/java/org/apache/oozie/command/coord/TestCoordActionInputCheckXCommand.java
+++ core/src/test/java/org/apache/oozie/command/coord/TestCoordActionInputCheckXCommand.java
@@ -141,8 +141,36 @@ public class TestCoordActionInputCheckXCommand extends XDataTestCase {
         checkCoordAction(job.getId() + "@1");
     }
 
-    protected CoordinatorJobBean addRecordToCoordJobTableForWaiting(String testFileName, CoordinatorJob.Status status, Date start, Date end,
-            boolean pending, boolean doneMatd, int lastActionNum) throws Exception {
+    /**
+     * This test case verifies if getCoordInputCheckRequeueInterval picks up the
+     * overridden value. In reality, the value could be overridden in
+     * oozie-site.xml.
+     *
+     * @throws Exception
+     */
+    public void testRequeueInterval() throws Exception {
+        /*
+         * Create a dummy Coordinator Job to pass to
+         * CoordActionInputCheckXCommand constructor.
+         */
+        String jobId = "0000000-" + new Date().getTime() + "-TestCoordActionInputCheckXCommand-C";
+        Date startTime = DateUtils.parseDateUTC("2009-02-01T23:59Z");
+        Date endTime = DateUtils.parseDateUTC("2009-02-02T23:59Z");
+        CoordinatorJobBean job = addRecordToCoordJobTable(jobId, startTime, endTime);
+        /* Override the property value for testing purpose only. */
+        long testedValue = 12000;
+        Services.get().getConf().setLong(CoordActionInputCheckXCommand.CONF_COORD_INPUT_CHECK_REQUEUE_INTERVAL,
+                testedValue);
+
+        CoordActionInputCheckXCommand caicc = new CoordActionInputCheckXCommand(job.getId() + "@1");
+
+        long effectiveValue = caicc.getCoordInputCheckRequeueInterval();
+        // Verify if two values are same.
+        assertEquals(testedValue, effectiveValue);
+    }
+
+    protected CoordinatorJobBean addRecordToCoordJobTableForWaiting(String testFileName, CoordinatorJob.Status status,
+            Date start, Date end, boolean pending, boolean doneMatd, int lastActionNum) throws Exception {
 
         String testDir = getTestCaseDir();
         CoordinatorJobBean coordJob = createCoordJob(testFileName, status, start, end, pending, doneMatd, lastActionNum);
@@ -172,7 +200,7 @@ public class TestCoordActionInputCheckXCommand extends XDataTestCase {
             return appXml;
         }
         catch (IOException ioe) {
-            throw new RuntimeException(XLog.format("Could not get "+ testFileName, ioe));
+            throw new RuntimeException(XLog.format("Could not get " + testFileName, ioe));
         }
     }
 
diff --git core/src/test/java/org/apache/oozie/command/coord/TestCoordRerunXCommand.java core/src/test/java/org/apache/oozie/command/coord/TestCoordRerunXCommand.java
index 25aa4a6..459d473 100644
--- core/src/test/java/org/apache/oozie/command/coord/TestCoordRerunXCommand.java
+++ core/src/test/java/org/apache/oozie/command/coord/TestCoordRerunXCommand.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -554,6 +554,7 @@ public class TestCoordRerunXCommand extends XDataTestCase {
         store2.closeTrx();
 
         waitFor(120 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 CoordinatorAction bean = coordClient.getCoordActionInfo(actionId);
                 return (bean.getStatus() == CoordinatorAction.Status.READY || bean.getStatus() == CoordinatorAction.Status.SUBMITTED);
@@ -623,6 +624,7 @@ public class TestCoordRerunXCommand extends XDataTestCase {
         store2.closeTrx();
 
         waitFor(120 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 CoordinatorAction bean = coordClient.getCoordActionInfo(actionId);
                 return (bean.getStatus() == CoordinatorAction.Status.WAITING || bean.getStatus() == CoordinatorAction.Status.READY);
@@ -667,6 +669,7 @@ public class TestCoordRerunXCommand extends XDataTestCase {
         store2.commitTrx();
         store2.closeTrx();
         waitFor(120 * 1000, new Predicate() {
+            @Override
             public boolean evaluate() throws Exception {
                 CoordinatorAction bean = coordClient.getCoordActionInfo(actionId);
                 return (bean.getStatus() == CoordinatorAction.Status.WAITING || bean.getStatus() == CoordinatorAction.Status.READY);
diff --git core/src/test/java/org/apache/oozie/command/coord/TestCoordSubmitXCommand.java core/src/test/java/org/apache/oozie/command/coord/TestCoordSubmitXCommand.java
index 45634eb..1c15f36 100644
--- core/src/test/java/org/apache/oozie/command/coord/TestCoordSubmitXCommand.java
+++ core/src/test/java/org/apache/oozie/command/coord/TestCoordSubmitXCommand.java
@@ -21,6 +21,8 @@ import java.io.File;
 import java.io.FileWriter;
 import java.io.IOException;
 import java.io.PrintWriter;
+import java.io.Reader;
+import java.io.Writer;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.oozie.CoordinatorJobBean;
@@ -33,6 +35,7 @@ import org.apache.oozie.executor.jpa.JPAExecutorException;
 import org.apache.oozie.service.JPAService;
 import org.apache.oozie.service.Services;
 import org.apache.oozie.test.XDataTestCase;
+import org.apache.oozie.util.IOUtils;
 import org.apache.oozie.util.XConfiguration;
 
 public class TestCoordSubmitXCommand extends XDataTestCase {
@@ -92,6 +95,128 @@ public class TestCoordSubmitXCommand extends XDataTestCase {
     }
 
     /**
+     * Testing for when user tries to submit a coordinator application having data-in events
+     * that erroneously specify multiple input data instances inside a single <instance> tag.
+     * Job gives submission error and indicates appropriate correction
+     * Testing both negative(error) and well as positive(success) cases
+     */
+    public void testBasicSubmitWithMultipleInstancesInputEvent() throws Exception {
+        Configuration conf = new XConfiguration();
+        String appPath = getTestCaseDir() + File.separator + "coordinator.xml";
+
+        // CASE 1: Failure case i.e. multiple data-in instances
+        Reader reader = IOUtils.getResourceAsReader("coord-multiple-input-instance1.xml", -1);
+        Writer writer = new FileWriter(appPath);
+        IOUtils.copyCharStream(reader, writer);
+        conf.set(OozieClient.COORDINATOR_APP_PATH, appPath);
+        conf.set(OozieClient.USER_NAME, getTestUser());
+        conf.set(OozieClient.GROUP_NAME, "other");
+        CoordSubmitXCommand sc = new CoordSubmitXCommand(conf, "UNIT_TESTING");
+
+        try {
+            sc.call();
+            fail("Expected to catch errors due to incorrectly specified input data set instances");
+        }
+        catch (CommandException e) {
+            assertEquals(sc.getJob().getStatus(), Job.Status.FAILED);
+            assertEquals(e.getErrorCode(), ErrorCode.E1021);
+            assertTrue(e.getMessage().contains(sc.COORD_INPUT_EVENTS) && e.getMessage().contains("per data-in instance"));
+        }
+
+        // CASE 2: Multiple data-in instances specified as separate <instance> tags, but one or more tags are empty. Check works for whitespace in the tags too
+        reader = IOUtils.getResourceAsReader("coord-multiple-input-instance2.xml", -1);
+        writer = new FileWriter(appPath);
+        IOUtils.copyCharStream(reader, writer);
+        sc = new CoordSubmitXCommand(conf, "UNIT_TESTING");
+
+        try {
+            sc.call();
+            fail("Expected to catch errors due to incorrectly specified input data set instances");
+        }
+        catch (CommandException e) {
+            assertEquals(sc.getJob().getStatus(), Job.Status.FAILED);
+            assertEquals(e.getErrorCode(), ErrorCode.E1021);
+            assertTrue(e.getMessage().contains(sc.COORD_INPUT_EVENTS) && e.getMessage().contains("is empty"));
+        }
+
+        // CASE 3: Success case i.e. Multiple data-in instances specified correctly as separate <instance> tags
+        reader = IOUtils.getResourceAsReader("coord-multiple-input-instance3.xml", -1);
+        writer = new FileWriter(appPath);
+        IOUtils.copyCharStream(reader, writer);
+        sc = new CoordSubmitXCommand(conf, "UNIT_TESTING");
+
+        try {
+            sc.call();
+        }
+        catch (CommandException e) {
+            fail("Unexpected failure: " + e);
+        }
+    }
+
+    /**
+     * Testing for when user tries to submit a coordinator application having data-out events
+     * that erroneously specify multiple output data instances inside a single <instance> tag.
+     * Job gives submission error and indicates appropriate correction
+     * Testing negative(error) cases. Positive(success) case is covered in another test case "testBasicSubmit".
+     */
+    public void testBasicSubmitWithMultipleInstancesOutputEvent() throws Exception {
+        Configuration conf = new XConfiguration();
+        String appPath = getTestCaseDir() + File.separator + "coordinator.xml";
+
+        // CASE 1: Failure case i.e. multiple data-out instances
+        Reader reader = IOUtils.getResourceAsReader("coord-multiple-output-instance1.xml", -1);
+        Writer writer = new FileWriter(appPath);
+        IOUtils.copyCharStream(reader, writer);
+
+        conf.set(OozieClient.COORDINATOR_APP_PATH, appPath);
+        conf.set(OozieClient.USER_NAME, getTestUser());
+        conf.set(OozieClient.GROUP_NAME, "other");
+        CoordSubmitXCommand sc = new CoordSubmitXCommand(conf, "UNIT_TESTING");
+
+        try {
+            sc.call();
+            fail("Expected to catch errors due to incorrectly specified output data set instances");
+        }
+        catch (CommandException e) {
+            assertEquals(sc.getJob().getStatus(), Job.Status.FAILED);
+            assertEquals(e.getErrorCode(), ErrorCode.E1021);
+            assertTrue(e.getMessage().contains(sc.COORD_OUTPUT_EVENTS) && e.getMessage().contains("per data-out instance"));
+        }
+
+        // CASE 2: Data-out instance tag is empty. Check works for whitespace in the tag too
+        reader = IOUtils.getResourceAsReader("coord-multiple-output-instance2.xml", -1);
+        writer = new FileWriter(appPath);
+        IOUtils.copyCharStream(reader, writer);
+        sc = new CoordSubmitXCommand(conf, "UNIT_TESTING");
+
+        try {
+            sc.call();
+            fail("Expected to catch errors due to incorrectly specified output data set instances");
+        }
+        catch (CommandException e) {
+            assertEquals(sc.getJob().getStatus(), Job.Status.FAILED);
+            assertEquals(e.getErrorCode(), ErrorCode.E1021);
+            assertTrue(e.getMessage().contains(sc.COORD_OUTPUT_EVENTS) && e.getMessage().contains("is empty"));
+        }
+
+        // CASE 3: Multiple <instance> tags within data-out should fail coordinator schema validation - different error than above is expected
+        reader = IOUtils.getResourceAsReader("coord-multiple-output-instance3.xml", -1);
+        writer = new FileWriter(appPath);
+        IOUtils.copyCharStream(reader, writer);
+        sc = new CoordSubmitXCommand(conf, "UNIT_TESTING");
+
+        try {
+            sc.call();
+            fail("Expected to catch errors due to incorrectly specified output data set instances");
+        }
+        catch (CommandException e) {
+            assertEquals(sc.getJob().getStatus(), Job.Status.FAILED);
+            assertEquals(e.getErrorCode(), ErrorCode.E0701);
+            assertTrue(e.getMessage().contains("No child element is expected at this point"));
+        }
+    }
+
+    /**
      * Basic coordinator submit test with bundleId
      *
      * @throws Exception
@@ -488,6 +613,33 @@ public class TestCoordSubmitXCommand extends XDataTestCase {
         }
     }
 
+    /**
+     * Checking that any dataset initial-instance is not set to a date earlier than the server default Jan 01, 1970 00:00Z UTC
+     * @throws Exception
+     */
+    public void testSubmitDatasetInitialInstance() throws Exception {
+        Configuration conf = new XConfiguration();
+        String appPath = getTestCaseDir() + File.separator + "coordinator.xml";
+        Reader reader = IOUtils.getResourceAsReader("coord-dataset-initial-instance.xml", -1);
+        Writer writer = new FileWriter(appPath);
+        IOUtils.copyCharStream(reader, writer);
+
+        conf.set(OozieClient.COORDINATOR_APP_PATH, appPath);
+        conf.set(OozieClient.USER_NAME, getTestUser());
+        conf.set(OozieClient.GROUP_NAME, "other");
+        CoordSubmitXCommand sc = new CoordSubmitXCommand(conf, "UNIT_TESTING");
+        try {
+            sc.call();
+            fail("Expected to catch errors due to invalid dataset initial instance");
+        }
+        catch(CommandException cx) {
+            assertEquals(sc.getJob().getStatus(), Job.Status.FAILED);
+            assertEquals(cx.getErrorCode(), ErrorCode.E1021);
+            if(!(cx.getMessage().contains("earlier than the default initial instance"))) {
+                fail("Unexpected failure - " + cx.getMessage());
+            }
+        }
+    }
 
     private void _testConfigDefaults(boolean withDefaults) throws Exception {
         Configuration conf = new XConfiguration();
diff --git core/src/test/java/org/apache/oozie/command/wf/TestReRunXCommand.java core/src/test/java/org/apache/oozie/command/wf/TestReRunXCommand.java
index f75e9e2..cbe4edb 100644
--- core/src/test/java/org/apache/oozie/command/wf/TestReRunXCommand.java
+++ core/src/test/java/org/apache/oozie/command/wf/TestReRunXCommand.java
@@ -19,6 +19,7 @@ package org.apache.oozie.command.wf;
 
 import java.util.Properties;
 import java.io.File;
+import java.io.FileReader;
 import java.io.FileWriter;
 import java.io.IOException;
 import java.io.Reader;
@@ -101,6 +102,50 @@ public class TestReRunXCommand extends XFsTestCase {
         assertEquals(WorkflowJob.Status.SUCCEEDED, wfClient.getJobInfo(jobId1).getStatus());
     }
 
+    /*
+     * Test to ensure parameterized configuration variables get resolved in workflow rerun
+     */
+    public void testRerunVariableSub() throws IOException, OozieClientException {
+        Reader reader = IOUtils.getResourceAsReader("rerun-varsub-wf.xml", -1);
+        Writer writer = new FileWriter(getTestCaseDir() + "/workflow.xml");
+        IOUtils.copyCharStream(reader, writer);
+
+        Path path = getFsTestCaseDir();
+
+        final OozieClient wfClient = LocalOozie.getClient();
+        Properties conf = wfClient.createConfiguration();
+        conf.setProperty(OozieClient.APP_PATH, getTestCaseDir() + File.separator + "workflow.xml");
+        conf.setProperty(OozieClient.USER_NAME, getTestUser());
+        injectKerberosInfo(conf);
+
+        conf.setProperty("nnbase", path.toString());
+        conf.setProperty("base", conf.getProperty("nnbase"));
+        // setting the variables "srcDir" and "dstDir", used as a file paths in the workflow, to parameterized expressions to test resolution.
+        conf.setProperty("srcDir", "${base}/p1");
+        conf.setProperty("dstDir", "${base}/p2");
+
+        final String jobId1 = wfClient.submit(conf);
+        wfClient.start(jobId1);
+
+        wfClient.kill(jobId1);
+
+        assertEquals(WorkflowJob.Status.KILLED, wfClient.getJobInfo(jobId1).getStatus());
+
+        // Skip executed nodes
+        getFileSystem().delete(new Path(path, "p2"), true);
+        conf.setProperty(OozieClient.RERUN_FAIL_NODES, "false");
+
+        wfClient.reRun(jobId1, conf);
+        waitFor(15 * 1000, new Predicate() {
+            public boolean evaluate() throws Exception {
+                return wfClient.getJobInfo(jobId1).getStatus() == WorkflowJob.Status.SUCCEEDED;
+            }
+        });
+
+        // workflow success reflects that rerun configuration contained correctly resolved variable values.
+        assertEquals(WorkflowJob.Status.SUCCEEDED, wfClient.getJobInfo(jobId1).getStatus());
+    }
+
     public void testRerunFromFailNodes() throws IOException, OozieClientException {
         Reader reader = IOUtils.getResourceAsReader("rerun-wf.xml", -1);
         Writer writer = new FileWriter(getTestCaseDir() + "/workflow.xml");
diff --git core/src/test/java/org/apache/oozie/command/wf/TestSubmitCommand.java core/src/test/java/org/apache/oozie/command/wf/TestSubmitCommand.java
index ab85e1b..cc8c4a2 100644
--- core/src/test/java/org/apache/oozie/command/wf/TestSubmitCommand.java
+++ core/src/test/java/org/apache/oozie/command/wf/TestSubmitCommand.java
@@ -57,6 +57,7 @@ public class TestSubmitCommand extends XFsTestCase {
         conf.set(OozieClient.USER_NAME, getTestUser());
         conf.set(OozieClient.GROUP_NAME, "other");
         conf.set("GB", "5");
+        injectKerberosInfo(conf);
         SubmitCommand sc = new SubmitCommand(conf, "UNIT_TESTING");
 
         try {
@@ -126,6 +127,7 @@ public class TestSubmitCommand extends XFsTestCase {
         conf.set(OozieClient.APP_PATH, appPath+"/does_not_exist.xml");
         conf.set(OozieClient.USER_NAME, getTestUser());
         conf.set(OozieClient.GROUP_NAME, "other");
+        injectKerberosInfo(conf);
         SubmitCommand sc = new SubmitCommand(conf, "UNIT_TESTING");
 
         try {
diff --git core/src/test/java/org/apache/oozie/coord/TestCoordELFunctions.java core/src/test/java/org/apache/oozie/coord/TestCoordELFunctions.java
index bc15d94..9227a01 100644
--- core/src/test/java/org/apache/oozie/coord/TestCoordELFunctions.java
+++ core/src/test/java/org/apache/oozie/coord/TestCoordELFunctions.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -359,6 +359,9 @@ public class TestCoordELFunctions extends XTestCase {
 
         expr = "${coord:dateOffset(\"2009-09-08T23:59Z\", -1, \"DAY\")}";
         assertEquals("2009-09-07T23:59Z", CoordELFunctions.evalAndWrap(eval, expr));
+
+        expr = "${coord:dateOffset(\"2009-09-08T23:59Z\", 1, \"YEAR\")}";
+        assertEquals("2010-09-08T23:59Z", CoordELFunctions.evalAndWrap(eval, expr));
     }
 
     public void testCurrent() throws Exception {
diff --git core/src/test/java/org/apache/oozie/coord/TestCoordUtils.java core/src/test/java/org/apache/oozie/coord/TestCoordUtils.java
new file mode 100644
index 0000000..a89f98c
--- /dev/null
+++ core/src/test/java/org/apache/oozie/coord/TestCoordUtils.java
@@ -0,0 +1,105 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.oozie.coord;
+
+import java.util.List;
+import org.apache.oozie.CoordinatorActionBean;
+import org.apache.oozie.CoordinatorJobBean;
+import org.apache.oozie.client.CoordinatorAction;
+import org.apache.oozie.client.CoordinatorJob;
+import org.apache.oozie.local.LocalOozie;
+import org.apache.oozie.service.Services;
+import org.apache.oozie.test.XDataTestCase;
+
+public class TestCoordUtils extends  XDataTestCase{
+    private Services services;
+
+    @Override
+    protected void setUp() throws Exception {
+        super.setUp();
+        services = new Services();
+        services.init();
+        cleanUpDBTables();
+        LocalOozie.start();
+    }
+
+    @Override
+    protected void tearDown() throws Exception {
+        LocalOozie.stop();
+        services.destroy();
+        super.tearDown();
+    }
+
+    // test retrieval of single action (action 1)
+    public void testGetCoordActionsFromIds() throws Exception {
+        int actionNum = 1;
+        CoordinatorJobBean job = addRecordToCoordJobTable(CoordinatorJob.Status.SUCCEEDED, false, false);
+        String jobId = job.getId();
+        CoordinatorActionBean action1 = addRecordToCoordActionTable(jobId, actionNum,
+                CoordinatorAction.Status.SUCCEEDED, "coord-action-get.xml", 0);
+        List<CoordinatorActionBean> coordActions = CoordUtils
+                .getCoordActionsFromIds(jobId, Integer.toString(actionNum));
+        //check for the expected size of actions list
+        assertEquals(1, coordActions.size());
+        //check for the expected action
+        assertEquals(action1, coordActions.get(0));
+    }
+
+    // test retrieval of range of actions (action 1-2)
+    public void testGetCoordActionsFromIdsRange() throws Exception {
+        int actionNum1 = 1;
+        int actionNum2 = 2;
+        CoordinatorJobBean job = addRecordToCoordJobTable(CoordinatorJob.Status.SUCCEEDED, false, false);
+        String jobId = job.getId();
+        addRecordToCoordActionTable(jobId, actionNum1, CoordinatorAction.Status.SUCCEEDED, "coord-action-get.xml", 0);
+        addRecordToCoordActionTable(jobId, actionNum2, CoordinatorAction.Status.SUCCEEDED, "coord-action-get.xml", 0);
+
+        String rerunScope = Integer.toString(actionNum1) + "-" + Integer.toString(actionNum2);
+        List<CoordinatorActionBean> coordActions = CoordUtils.getCoordActionsFromIds(jobId, rerunScope);
+
+        assertEquals(2, coordActions.size());
+    }
+
+    // test retrieval of action corresponding to single date (date1)
+    public void testGetCoordActionsFromDate() throws Exception{
+        int actionNum = 1;
+        CoordinatorJobBean job = addRecordToCoordJobTable(CoordinatorJob.Status.SUCCEEDED, false, false);
+        String jobId = job.getId();
+        CoordinatorActionBean action1 = addRecordToCoordActionTable(jobId, actionNum,
+                CoordinatorAction.Status.SUCCEEDED, "coord-rerun-action1.xml", 0);
+        List<CoordinatorActionBean> coordActions = CoordUtils.getCoordActionsFromDates(jobId, "2009-12-15T01:00Z");
+
+        assertEquals(1, coordActions.size());
+        assertEquals(action1, coordActions.get(0));
+    }
+
+    //test retrieval of action corresponding to range of dates (date1::date2);
+    public void testGetCoordActionsFromDateRange() throws Exception{
+        int actionNum1 = 1;
+        int actionNum2 = 2;
+        CoordinatorJobBean job = addRecordToCoordJobTable(CoordinatorJob.Status.SUCCEEDED, false, false);
+        String jobId = job.getId();
+        addRecordToCoordActionTable(jobId, actionNum1, CoordinatorAction.Status.SUCCEEDED, "coord-rerun-action1.xml", 0);
+        addRecordToCoordActionTable(jobId, actionNum2, CoordinatorAction.Status.SUCCEEDED, "coord-rerun-action2.xml", 0);
+        String rerunScope = "2009-12-15T01:00Z" + "::" + "2009-12-16T01:00Z";
+        List<CoordinatorActionBean> coordActions = CoordUtils.getCoordActionsFromDates(jobId, rerunScope);
+
+        assertEquals(2, coordActions.size());
+    }
+}
diff --git core/src/test/java/org/apache/oozie/executor/jpa/TestCoordJobGetActionsSubsetJPAExecutor.java core/src/test/java/org/apache/oozie/executor/jpa/TestCoordJobGetActionsSubsetJPAExecutor.java
index d249ac5..bb2ba8c 100644
--- core/src/test/java/org/apache/oozie/executor/jpa/TestCoordJobGetActionsSubsetJPAExecutor.java
+++ core/src/test/java/org/apache/oozie/executor/jpa/TestCoordJobGetActionsSubsetJPAExecutor.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -64,4 +64,25 @@ public class TestCoordJobGetActionsSubsetJPAExecutor extends XDataTestCase {
         assertEquals(actions.get(0).getId(), actionId);
     }
 
+    // Check the ordering of actions by nominal time
+    public void testCoordActionOrderBy() throws Exception {
+        CoordinatorJobBean job = addRecordToCoordJobTable(CoordinatorJob.Status.RUNNING, false, false);
+        // Add Coordinator action with nominal time: 2009-12-15T01:00Z
+        CoordinatorActionBean action = addRecordToCoordActionTable(job.getId(), 1, CoordinatorAction.Status.WAITING, "coord-action-get.xml", 0);
+        // Add Coordinator action with nominal time: 2009-02-01T23:59Z
+        CoordinatorActionBean action1 = addRecordToCoordActionTable(job.getId(), 2, CoordinatorAction.Status.WAITING, "coord-action-for-action-input-check.xml", 0);
+        // test for the expected action number
+        _testGetActionsSubsetOrderBy(job.getId(), 2, 1, 2);
+    }
+
+   private void _testGetActionsSubsetOrderBy(String jobId, int actionNum, int start, int len) throws Exception {
+        JPAService jpaService = Services.get().get(JPAService.class);
+        assertNotNull(jpaService);
+        CoordJobGetActionsSubsetJPAExecutor actionGetCmd = new CoordJobGetActionsSubsetJPAExecutor(jobId, start, len);
+        List<CoordinatorActionBean> actions = jpaService.execute(actionGetCmd);
+        assertEquals(actions.size(), 2);
+        // As actions are sorted by nominal time, the first action should be with action number 2
+        assertEquals(actions.get(0).getActionNumber(), actionNum);
+    }
+
 }
diff --git core/src/test/java/org/apache/oozie/executor/jpa/TestCoordJobInfoGetJPAExecutor.java core/src/test/java/org/apache/oozie/executor/jpa/TestCoordJobInfoGetJPAExecutor.java
index 7f8fc7f..06f4b67 100644
--- core/src/test/java/org/apache/oozie/executor/jpa/TestCoordJobInfoGetJPAExecutor.java
+++ core/src/test/java/org/apache/oozie/executor/jpa/TestCoordJobInfoGetJPAExecutor.java
@@ -61,6 +61,7 @@ public class TestCoordJobInfoGetJPAExecutor extends XDataTestCase {
         _testGetJobInfoForUserAndStatus();
         _testGetJobInfoForFrequency();
         _testGetJobInfoForId(coordinatorJob1.getId());
+        _testGetJobInfoForFrequencyAndUnit();
     }
 
     private void _testGetJobInfoForStatus() throws Exception {
@@ -165,4 +166,51 @@ public class TestCoordJobInfoGetJPAExecutor extends XDataTestCase {
         assertEquals(ret.getCoordJobs().size(), 1);
     }
 
+    /**
+     * Test to verify various combinations of frequency and time unit filters for jobs
+     *
+     * @throws Exception
+     */
+    private void _testGetJobInfoForFrequencyAndUnit() throws Exception {
+        JPAService jpaService = Services.get().get(JPAService.class);
+        assertNotNull(jpaService);
+
+        // Test specifying frequency value as 1 minute
+        Map<String, List<String>> filter = new HashMap<String, List<String>>();
+        List<String> unitList = new ArrayList<String>();
+        List<String> frequencyList = new ArrayList<String>();
+        unitList.add("MINUTE");
+        filter.put(OozieClient.FILTER_UNIT, unitList);
+        frequencyList = new ArrayList<String>();
+        frequencyList.add("1");
+        filter.put(OozieClient.FILTER_FREQUENCY, frequencyList);
+        CoordJobInfoGetJPAExecutor coordInfoGetCmd = new CoordJobInfoGetJPAExecutor(filter, 1, 20);
+        CoordinatorJobInfo ret = jpaService.execute(coordInfoGetCmd);
+        assertNotNull(ret);
+        assertEquals(ret.getCoordJobs().size(), 0);
+        frequencyList.remove(0);
+        unitList.remove(0);
+
+        // Test specifying frequency value as 3 days
+        unitList.add("DAY");
+        filter.put(OozieClient.FILTER_UNIT, unitList);
+        frequencyList.add("3");
+        filter.put(OozieClient.FILTER_FREQUENCY, frequencyList);
+        coordInfoGetCmd = new CoordJobInfoGetJPAExecutor(filter, 1, 20);
+        ret = jpaService.execute(coordInfoGetCmd);
+        assertNotNull(ret);
+        assertEquals(ret.getCoordJobs().size(), 0);
+        frequencyList.remove(0);
+        unitList.remove(0);
+
+        // Test specifying frequency value as 1 day
+        unitList.add("DAY");
+        filter.put(OozieClient.FILTER_UNIT, unitList);
+        frequencyList.add("1");
+        filter.put(OozieClient.FILTER_FREQUENCY, frequencyList);
+        coordInfoGetCmd = new CoordJobInfoGetJPAExecutor(filter, 1, 20);
+        ret = jpaService.execute(coordInfoGetCmd);
+        assertNotNull(ret);
+        assertEquals(ret.getCoordJobs().size(), 3);
+    }
 }
diff --git core/src/test/java/org/apache/oozie/service/TestGroupsService.java core/src/test/java/org/apache/oozie/service/TestGroupsService.java
new file mode 100644
index 0000000..34655a4
--- /dev/null
+++ core/src/test/java/org/apache/oozie/service/TestGroupsService.java
@@ -0,0 +1,63 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.service;
+
+import junit.framework.Assert;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.oozie.test.XTestCase;
+
+import java.util.Arrays;
+import java.util.List;
+
+public class TestGroupsService extends XTestCase {
+
+    public void testService() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName())));
+        services.init();
+        try {
+            GroupsService groups = services.get(GroupsService.class);
+            Assert.assertNotNull(groups);
+            List<String> g = groups.getGroups(System.getProperty("user.name"));
+            Assert.assertNotSame(g.size(), 0);
+        }
+        finally {
+            services.destroy();
+        }
+    }
+
+    public void testInvalidGroupsMapping() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName())));
+        conf.set("oozie.service.GroupsService.hadoop.security.group.mapping", String.class.getName());
+        try {
+            services.init();
+            fail();
+        }
+        catch (ServiceException ex) {
+        }
+        catch (Exception ex) {
+            fail(ex.toString());
+        }
+    }
+
+}
+
diff --git core/src/test/java/org/apache/oozie/service/TestLiteWorkflowAppService.java core/src/test/java/org/apache/oozie/service/TestLiteWorkflowAppService.java
index e26b180..0fdda52 100644
--- core/src/test/java/org/apache/oozie/service/TestLiteWorkflowAppService.java
+++ core/src/test/java/org/apache/oozie/service/TestLiteWorkflowAppService.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -84,9 +84,11 @@ public class TestLiteWorkflowAppService extends XTestCase {
             Writer writer = new FileWriter(getTestCaseDir() + "/workflow.xml");
             IOUtils.copyCharStream(reader, writer);
 
+            Configuration conf = new XConfiguration();
+            injectKerberosInfo(conf);
             WorkflowAppService wps = services.get(WorkflowAppService.class);
-            String wfDef = wps.readDefinition("file://" + getTestCaseDir() + File.separator + "workflow.xml", getTestUser(), "group",
-                                              "authToken");
+            String wfDef = wps.readDefinition("file://" + getTestCaseDir() + File.separator + "workflow.xml",
+                                              getTestUser(), "group", "authToken", conf);
             assertNotNull(reader.toString(), wfDef);
         }
         finally {
@@ -203,6 +205,7 @@ public class TestLiteWorkflowAppService extends XTestCase {
             jobConf.set(OozieClient.APP_PATH, "file://" + getTestCaseDir() + File.separator + "workflow.xml");
             jobConf.set(OozieClient.USER_NAME, getTestUser());
             jobConf.set(OozieClient.GROUP_NAME, "group");
+            injectKerberosInfo(jobConf);
 
             try {
                 LiteWorkflowApp app = (LiteWorkflowApp) wps.parseDef(jobConf, "authToken");
@@ -241,21 +244,21 @@ public class TestLiteWorkflowAppService extends XTestCase {
             assertEquals("a", app.getNode("::start::").getTransitions().get(0));
             assertEquals("b", app.getNode("a").getTransitions().get(0));
             assertEquals("c", app.getNode("a").getTransitions().get(1));
-            assertEquals("d", app.getNode("a").getTransitions().get(2));
+            assertEquals("c", app.getNode("a").getTransitions().get(2));
             assertTrue(app.getNode("b").getConf().contains("kill"));
             assertEquals("d", app.getNode("c").getTransitions().get(0));
             assertEquals("e", app.getNode("c").getTransitions().get(1));
             assertEquals(2, app.getNode("c").getTransitions().size());
 
-            assertEquals("e", app.getNode("d").getTransitions().get(0));
+            assertEquals("f", app.getNode("d").getTransitions().get(0));
             assertEquals("b", app.getNode("d").getTransitions().get(1));
             assertTrue(app.getNode("d").getConf().startsWith("<map-reduce"));
 
-            assertEquals("z", app.getNode("e").getTransitions().get(0));
+            assertEquals("f", app.getNode("e").getTransitions().get(0));
             assertEquals("b", app.getNode("e").getTransitions().get(1));
             assertTrue(app.getNode("e").getConf().startsWith("<pig"));
 
-            assertEquals("g", app.getNode("f").getTransitions().get(0));
+            assertEquals("z", app.getNode("f").getTransitions().get(0));
 
             assertNotNull(app.getNode("z"));
         }
@@ -354,7 +357,7 @@ public class TestLiteWorkflowAppService extends XTestCase {
             services.destroy();
         }
     }
-    
+
     public void testCreateprotoConfWithMulipleLibPath() throws Exception {
         Services services = new Services();
         try {
@@ -373,7 +376,7 @@ public class TestLiteWorkflowAppService extends XTestCase {
             createTestCaseSubDir("libx");
             writer = new FileWriter(getTestCaseDir() + "/libx/maputil_x.jar");
             writer.write("bla bla");
-            writer.close();            
+            writer.close();
             createTestCaseSubDir("liby");
             writer = new FileWriter(getTestCaseDir() + "/liby/maputil_y1.jar");
             writer.write("bla bla");
@@ -422,135 +425,6 @@ public class TestLiteWorkflowAppService extends XTestCase {
         }
     }
 
-    public void testCreateprotoConfWithSystemLibPath() throws Exception {
-        String systemLibPath = createTestCaseSubDir("syslib");
-        setSystemProperty(WorkflowAppService.SYSTEM_LIB_PATH, systemLibPath);
-        Services services = new Services();
-        try {
-            services.init();
-            Writer writer;
-            writer = new FileWriter(systemLibPath + "/maputilsys.jar");
-            writer.write("bla bla");
-            writer.close();
-            new Services().init();
-
-            Reader reader = IOUtils.getResourceAsReader("wf-schema-valid.xml", -1);
-            writer = new FileWriter(getTestCaseDir() + "/workflow.xml");
-            IOUtils.copyCharStream(reader, writer);
-
-            createTestCaseSubDir("lib");
-            writer = new FileWriter(getTestCaseDir() + "/lib/maputil.jar");
-            writer.write("bla bla");
-            writer.close();
-            writer = new FileWriter(getTestCaseDir() + "/lib/reduceutil.so");
-            writer.write("bla bla");
-            writer.close();
-            createTestCaseSubDir("libx");
-            writer = new FileWriter(getTestCaseDir() + "/libx/maputilx.jar");
-            writer.write("bla bla");
-            writer.close();
-
-            // without using system libpath
-            WorkflowAppService wps = Services.get().get(WorkflowAppService.class);
-            Configuration jobConf = new XConfiguration();
-            jobConf.set(OozieClient.APP_PATH, "file://" + getTestCaseDir() + "/workflow.xml");
-            jobConf.set(OozieClient.LIBPATH, "file://" + getTestCaseDir() + "/libx");
-            jobConf.set(OozieClient.USER_NAME, getTestUser());
-            jobConf.set(OozieClient.GROUP_NAME, getTestGroup());
-            injectKerberosInfo(jobConf);
-            Configuration protoConf = wps.createProtoActionConf(jobConf, "authToken", true);
-            assertEquals(getTestUser(), protoConf.get(OozieClient.USER_NAME));
-            assertEquals(getTestGroup(), protoConf.get(OozieClient.GROUP_NAME));
-
-            assertEquals(3, protoConf.getStrings(WorkflowAppService.APP_LIB_PATH_LIST).length);
-            List<String> found = new ArrayList<String>();
-            found.add(protoConf.getStrings(WorkflowAppService.APP_LIB_PATH_LIST)[0]);
-            found.add(protoConf.getStrings(WorkflowAppService.APP_LIB_PATH_LIST)[1]);
-            found.add(protoConf.getStrings(WorkflowAppService.APP_LIB_PATH_LIST)[2]);
-            List<String> expected = new ArrayList<String>();
-            expected.add(getTestCaseDir() + "/lib/reduceutil.so");
-            expected.add(getTestCaseDir() + "/lib/maputil.jar");
-            expected.add(getTestCaseDir() + "/libx/maputilx.jar");
-            Collections.sort(found);
-            Collections.sort(expected);
-            assertEquals(expected, found);
-
-            // using system libpath
-            wps = Services.get().get(WorkflowAppService.class);
-            jobConf = new XConfiguration();
-            jobConf.set(OozieClient.APP_PATH, "file://" + getTestCaseDir() + "/workflow.xml");
-            jobConf.set(OozieClient.LIBPATH, "file://" + getTestCaseDir() + "/libx");
-            jobConf.set(OozieClient.USER_NAME, getTestUser());
-            jobConf.set(OozieClient.GROUP_NAME, getTestGroup());
-            jobConf.setBoolean(OozieClient.USE_SYSTEM_LIBPATH, true);
-            injectKerberosInfo(jobConf);
-            protoConf = wps.createProtoActionConf(jobConf, "authToken", true);
-            assertEquals(getTestUser(), protoConf.get(OozieClient.USER_NAME));
-            assertEquals(getTestGroup(), protoConf.get(OozieClient.GROUP_NAME));
-
-            assertEquals(4, protoConf.getStrings(WorkflowAppService.APP_LIB_PATH_LIST).length);
-            found = new ArrayList<String>();
-            found.add(protoConf.getStrings(WorkflowAppService.APP_LIB_PATH_LIST)[0]);
-            found.add(protoConf.getStrings(WorkflowAppService.APP_LIB_PATH_LIST)[1]);
-            found.add(protoConf.getStrings(WorkflowAppService.APP_LIB_PATH_LIST)[2]);
-            found.add(protoConf.getStrings(WorkflowAppService.APP_LIB_PATH_LIST)[3]);
-            expected = new ArrayList<String>();
-            expected.add(getTestCaseDir() + "/lib/reduceutil.so");
-            expected.add(getTestCaseDir() + "/lib/maputil.jar");
-            expected.add(getTestCaseDir() + "/libx/maputilx.jar");
-            expected.add(getTestCaseDir() + "/syslib/maputilsys.jar");
-            Collections.sort(found);
-            Collections.sort(expected);
-            assertEquals(expected, found);
-        }
-        finally {
-            services.destroy();
-        }
-    }
-
-    public void testCreateprotoConfWithMissingSystemLibPath() throws Exception {
-        setSystemProperty(WorkflowAppService.SYSTEM_LIB_PATH, getTestCaseDir() + "/missingsyslib");
-        Services services = new Services();
-        try {
-            services.init();
-            new Services().init();
-
-            Writer writer;
-            Reader reader = IOUtils.getResourceAsReader("wf-schema-valid.xml", -1);
-            writer = new FileWriter(getTestCaseDir() + "/workflow.xml");
-            IOUtils.copyCharStream(reader, writer);
-
-            createTestCaseSubDir("lib");
-            writer = new FileWriter(getTestCaseDir() + "/lib/maputil.jar");
-            writer.write("bla bla");
-            writer.close();
-
-            // using missing system libpath
-            WorkflowAppService wps = Services.get().get(WorkflowAppService.class);
-            Configuration jobConf = new XConfiguration();
-            jobConf.set(OozieClient.APP_PATH, "file://" + getTestCaseDir() + "/workflow.xml");
-            jobConf.set(OozieClient.USER_NAME, getTestUser());
-            jobConf.set(OozieClient.GROUP_NAME, getTestGroup());
-            jobConf.setBoolean(OozieClient.USE_SYSTEM_LIBPATH, true);
-            injectKerberosInfo(jobConf);
-            Configuration protoConf = wps.createProtoActionConf(jobConf, "authToken", true);
-            assertEquals(getTestUser(), protoConf.get(OozieClient.USER_NAME));
-            assertEquals(getTestGroup(), protoConf.get(OozieClient.GROUP_NAME));
-
-            assertEquals(1, protoConf.getStrings(WorkflowAppService.APP_LIB_PATH_LIST).length);
-            List<String> found = new ArrayList<String>();
-            found.add(protoConf.getStrings(WorkflowAppService.APP_LIB_PATH_LIST)[0]);
-            List<String> expected = new ArrayList<String>();
-            expected.add(getTestCaseDir() + "/lib/maputil.jar");
-            Collections.sort(found);
-            Collections.sort(expected);
-            assertEquals(expected, found);
-        }
-        finally {
-            services.destroy();
-        }
-    }
-
     public void testCreateprotoConfWithSubWorkflow_Case1_ParentWorkflowContainingLibs() throws Exception {
         // When parent workflow has an non-empty lib directory,
         // APP_LIB_PATH_LIST should contain libraries from both parent and
diff --git core/src/test/java/org/apache/oozie/service/TestProxyUserService.java core/src/test/java/org/apache/oozie/service/TestProxyUserService.java
new file mode 100644
index 0000000..d1338e6
--- /dev/null
+++ core/src/test/java/org/apache/oozie/service/TestProxyUserService.java
@@ -0,0 +1,265 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.service;
+
+import junit.framework.Assert;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.oozie.test.XTestCase;
+
+import java.security.AccessControlException;
+import java.util.Arrays;
+import java.util.List;
+
+public class TestProxyUserService extends XTestCase {
+
+    public void testService() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName(),
+                                                                                    ProxyUserService.class.getName())));
+        services.init();
+        try {
+            ProxyUserService proxyUser = services.get(ProxyUserService.class);
+            Assert.assertNotNull(proxyUser);
+        }
+        finally {
+            services.destroy();
+        }
+    }
+
+    public void testWrongConfigGroups() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName(),
+                                                                                    ProxyUserService.class.getName())));
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.hosts", "*");
+        try {
+            services.init();
+            fail();
+        }
+        catch (ServiceException ex) {
+        }
+        catch (Exception ex) {
+            fail();
+        }
+    }
+
+    public void testWrongHost() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName(),
+                                                                                    ProxyUserService.class.getName())));
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.hosts", "otherhost");
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.groups", "*");
+        try {
+            services.init();
+            fail();
+        }
+        catch (ServiceException ex) {
+        }
+        catch (Exception ex) {
+            fail();
+        }
+    }
+
+    public void testWrongConfigHosts() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName(),
+                                                                                    ProxyUserService.class.getName())));
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.groups", "*");
+        try {
+            services.init();
+            fail();
+        }
+        catch (ServiceException ex) {
+        }
+        catch (Exception ex) {
+            fail();
+        }
+    }
+
+    public void testValidateAnyHostAnyUser() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName(),
+                                                                                    ProxyUserService.class.getName())));
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.hosts", "*");
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.groups", "*");
+        services.init();
+        try {
+            ProxyUserService proxyUser = services.get(ProxyUserService.class);
+            Assert.assertNotNull(proxyUser);
+            proxyUser.validate("foo", "localhost", "bar");
+        }
+        finally {
+            services.destroy();
+        }
+    }
+
+    public void testInvalidProxyUser() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName(),
+                                                                                    ProxyUserService.class.getName())));
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.hosts", "*");
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.groups", "*");
+        services.init();
+        try {
+            ProxyUserService proxyUser = services.get(ProxyUserService.class);
+            Assert.assertNotNull(proxyUser);
+            proxyUser.validate("bar", "localhost", "foo");
+            fail();
+        }
+        catch (AccessControlException ex) {
+        }
+        catch (Exception ex) {
+            fail(ex.toString());
+        }
+        finally {
+            services.destroy();
+        }
+    }
+
+    public void testValidateHost() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName(),
+                                                                                    ProxyUserService.class.getName())));
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.hosts", "localhost");
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.groups", "*");
+        services.init();
+        try {
+            ProxyUserService proxyUser = services.get(ProxyUserService.class);
+            Assert.assertNotNull(proxyUser);
+            proxyUser.validate("foo", "localhost", "bar");
+        }
+        finally {
+            services.destroy();
+        }
+    }
+
+    private String getGroup() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName())));
+        conf.set("server.services", StringUtils.join(",", Arrays.asList(GroupsService.class.getName())));
+        services.init();
+        GroupsService groups = services.get(GroupsService.class);
+        List<String> g = groups.getGroups(System.getProperty("user.name"));
+        services.destroy();
+        return g.get(0);
+    }
+
+    public void testValidateGroup() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName(),
+                                                                                    ProxyUserService.class.getName())));
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.hosts", "*");
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.groups", getGroup());
+        services.init();
+        try {
+            ProxyUserService proxyUser = services.get(ProxyUserService.class);
+            Assert.assertNotNull(proxyUser);
+            proxyUser.validate("foo", "localhost", System.getProperty("user.name"));
+        }
+        finally {
+            services.destroy();
+        }
+    }
+
+
+    public void testUnknownHost() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName(),
+                                                                                    ProxyUserService.class.getName())));
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.hosts", "localhost");
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.groups", "*");
+        services.init();
+        try {
+            ProxyUserService proxyUser = services.get(ProxyUserService.class);
+            Assert.assertNotNull(proxyUser);
+            proxyUser.validate("foo", "unknownhost.bar.foo", "bar");
+            fail();
+        }
+        catch (AccessControlException ex) {
+
+        }
+        catch (Exception ex) {
+            fail(ex.toString());
+        }
+        finally {
+            services.destroy();
+        }
+    }
+
+    public void testInvalidHost() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName(),
+                                                                                    ProxyUserService.class.getName())));
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.hosts", "localhost");
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.groups", "*");
+        services.init();
+        try {
+            ProxyUserService proxyUser = services.get(ProxyUserService.class);
+            Assert.assertNotNull(proxyUser);
+            proxyUser.validate("foo", "www.yahoo.com", "bar");
+            fail();
+        }
+        catch (AccessControlException ex) {
+
+        }
+        catch (Exception ex) {
+            fail(ex.toString());
+        }
+        finally {
+            services.destroy();
+        }
+    }
+
+    public void testInvalidGroup() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        conf.set(Services.CONF_SERVICE_CLASSES, StringUtils.join(",", Arrays.asList(GroupsService.class.getName(),
+                                                                                    ProxyUserService.class.getName())));
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.hosts", "localhost");
+        conf.set("oozie.service.ProxyUserService.proxyuser.foo.groups", "nobody");
+        services.init();
+        try {
+            ProxyUserService proxyUser = services.get(ProxyUserService.class);
+            Assert.assertNotNull(proxyUser);
+            proxyUser.validate("foo", "localhost", System.getProperty("user.name"));
+            fail();
+        }
+        catch (AccessControlException ex) {
+
+        }
+        catch (Exception ex) {
+            fail(ex.toString());
+        }
+        finally {
+            services.destroy();
+        }
+    }
+
+}
+
diff --git core/src/test/java/org/apache/oozie/service/TestServices.java core/src/test/java/org/apache/oozie/service/TestServices.java
index fbd2831..497f8b3 100644
--- core/src/test/java/org/apache/oozie/service/TestServices.java
+++ core/src/test/java/org/apache/oozie/service/TestServices.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -41,9 +41,11 @@ public class TestServices extends XTestCase {
     }
 
     public static class S1 implements Service {
+        public static boolean INITED_S1 = false;
 
         @Override
         public void init(Services services) throws ServiceException {
+            INITED_S1 = true;
         }
 
         @Override
@@ -73,6 +75,12 @@ public class TestServices extends XTestCase {
     }
 
     public static class S1Ext extends S1 {
+        public static boolean INITED_S1EXT = false;
+
+        @Override
+        public void init(Services services) throws ServiceException {
+            INITED_S1EXT = true;
+        }
     }
 
     private static final String SERVICES = S1.class.getName() + "," + S2.class.getName();
@@ -88,11 +96,15 @@ public class TestServices extends XTestCase {
     private static final String SERVICES_EXT = S1Ext.class.getName();
 
     public void testServicesExtLoading() throws Exception {
+        S1.INITED_S1 = false;
+        S1Ext.INITED_S1EXT = false;
         setSystemProperty(Services.CONF_SERVICE_CLASSES, SERVICES);
         setSystemProperty(Services.CONF_SERVICE_EXT_CLASSES, SERVICES_EXT);
         Services services = new Services();
         services.init();
         assertEquals(S1Ext.class,  services.get(S1.class).getClass());
         assertEquals(S2.class,  services.get(S2.class).getClass());
+        assertFalse(S1.INITED_S1);
+        assertTrue(S1Ext.INITED_S1EXT);
     }
 }
diff --git core/src/test/java/org/apache/oozie/servlet/DagServletTestCase.java core/src/test/java/org/apache/oozie/servlet/DagServletTestCase.java
index 172c609..eb7c27f 100644
--- core/src/test/java/org/apache/oozie/servlet/DagServletTestCase.java
+++ core/src/test/java/org/apache/oozie/servlet/DagServletTestCase.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -19,6 +19,7 @@ package org.apache.oozie.servlet;
 
 import org.apache.oozie.service.AuthorizationService;
 
+import org.apache.oozie.service.ProxyUserService;
 import org.apache.oozie.service.Services;
 import org.apache.oozie.service.ForTestAuthorizationService;
 import org.apache.oozie.service.ForTestWorkflowStoreService;
@@ -70,6 +71,11 @@ public abstract class DagServletTestCase extends XFsTestCase {
         Services services = new Services();
         this.servletPath = servletPath[0];
         try {
+            String proxyUser = getTestUser();
+            services.getConf().set(ProxyUserService.CONF_PREFIX + proxyUser +
+                                   ProxyUserService.HOSTS, "*");
+            services.getConf().set(ProxyUserService.CONF_PREFIX + proxyUser +
+                                   ProxyUserService.GROUPS, "*");
             services.init();
             services.getConf().setBoolean(AuthorizationService.CONF_SECURITY_ENABLED, securityEnabled);
             Services.get().setService(ForTestAuthorizationService.class);
@@ -80,6 +86,9 @@ public abstract class DagServletTestCase extends XFsTestCase {
             for (int i = 0; i < servletPath.length; i++) {
                 container.addServletEndpoint(servletPath[i], servletClass[i]);
             }
+            container.addFilter("*", HostnameFilter.class);
+            container.addFilter("*", AuthFilter.class);
+            setSystemProperty("user.name", getTestUser());
             container.start();
             assertions.call();
         }
diff --git core/src/test/java/org/apache/oozie/servlet/MockDagEngineService.java core/src/test/java/org/apache/oozie/servlet/MockDagEngineService.java
index 1294516..6566e05 100644
--- core/src/test/java/org/apache/oozie/servlet/MockDagEngineService.java
+++ core/src/test/java/org/apache/oozie/servlet/MockDagEngineService.java
@@ -50,6 +50,7 @@ public class MockDagEngineService extends DagEngineService {
 
     public static Configuration submittedConf;
     public static String did = null;
+    public static String user = null;
     public static Properties properties;
     public static List<WorkflowJob> workflows;
     public static List<Boolean> started;
@@ -60,6 +61,7 @@ public class MockDagEngineService extends DagEngineService {
     }
 
     public static void reset() {
+        user = null;
         did = null;
         properties = null;
         workflows = new ArrayList<WorkflowJob>();
@@ -96,6 +98,7 @@ public class MockDagEngineService extends DagEngineService {
             int idx = workflows.size();
             workflows.add(createDummyWorkflow(idx, XmlUtils.prettyPrint(conf).toString()));
             started.add(startJob);
+            MockDagEngineService.user = getUser();
             return JOB_ID + idx;
         }
 
diff --git core/src/test/java/org/apache/oozie/servlet/TestAuthFilterAuthOozieClient.java core/src/test/java/org/apache/oozie/servlet/TestAuthFilterAuthOozieClient.java
new file mode 100644
index 0000000..a1e2209
--- /dev/null
+++ core/src/test/java/org/apache/oozie/servlet/TestAuthFilterAuthOozieClient.java
@@ -0,0 +1,196 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.servlet;
+
+import org.apache.hadoop.security.authentication.client.AuthenticatedURL;
+import org.apache.hadoop.security.authentication.client.AuthenticationException;
+import org.apache.hadoop.security.authentication.client.PseudoAuthenticator;
+import org.apache.oozie.cli.OozieCLI;
+import org.apache.oozie.client.AuthOozieClient;
+import org.apache.oozie.client.HeaderTestingVersionServlet;
+import org.apache.oozie.client.XOozieClient;
+import org.apache.oozie.service.ForTestAuthorizationService;
+import org.apache.oozie.service.ForTestWorkflowStoreService;
+import org.apache.oozie.service.Services;
+import org.apache.oozie.test.EmbeddedServletContainer;
+import org.apache.oozie.test.XTestCase;
+import org.apache.oozie.util.IOUtils;
+
+import java.io.FileReader;
+import java.io.IOException;
+import java.net.HttpURLConnection;
+import java.net.URL;
+import java.net.URLEncoder;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.Callable;
+
+/**
+ *
+ */
+public class TestAuthFilterAuthOozieClient extends XTestCase {
+    private EmbeddedServletContainer container;
+
+    protected String getContextURL() {
+        return container.getContextURL();
+    }
+
+    protected URL createURL(String servletPath, String resource, Map<String, String> parameters) throws Exception {
+        StringBuilder sb = new StringBuilder();
+        sb.append(container.getServletURL(servletPath));
+        if (resource != null && resource.length() > 0) {
+            sb.append("/").append(resource);
+        }
+        if (parameters.size() > 0) {
+            String separator = "?";
+            for (Map.Entry<String, String> param : parameters.entrySet()) {
+                sb.append(separator).append(URLEncoder.encode(param.getKey(), "UTF-8")).append("=")
+                        .append(URLEncoder.encode(param.getValue(), "UTF-8"));
+                separator = "&";
+            }
+        }
+        return new URL(sb.toString());
+    }
+
+    protected void runTest(Callable<Void> assertions) throws Exception {
+        Services services = new Services();
+        try {
+            services.init();
+            Services.get().setService(ForTestAuthorizationService.class);
+            Services.get().setService(ForTestWorkflowStoreService.class);
+            Services.get().setService(MockDagEngineService.class);
+            Services.get().setService(MockCoordinatorEngineService.class);
+            container = new EmbeddedServletContainer("oozie");
+            container.addServletEndpoint("/versions", HeaderTestingVersionServlet.class);
+            String version = "/v" + XOozieClient.WS_PROTOCOL_VERSION;
+            container.addServletEndpoint(version + "/admin/*", V1AdminServlet.class);
+            container.addFilter("*", HostnameFilter.class);
+            container.addFilter("/*", AuthFilter.class);
+            container.start();
+            assertions.call();
+        }
+        finally {
+            if (container != null) {
+                container.stop();
+            }
+            services.destroy();
+            container = null;
+        }
+    }
+
+    public static class Authenticator4Test extends PseudoAuthenticator {
+
+        private static boolean USED = false;
+
+        @Override
+        public void authenticate(URL url, AuthenticatedURL.Token token) throws IOException, AuthenticationException {
+            USED = true;
+            super.authenticate(url, token);
+        }
+    }
+
+    public void testClientWithAnonymous() throws Exception {
+        setSystemProperty("oozie.authentication.simple.anonymous.allowed", "true");
+        runTest(new Callable<Void>() {
+            public Void call() throws Exception {
+                String oozieUrl = getContextURL();
+                String[] args = new String[]{"admin", "-status", "-oozie", oozieUrl};
+                assertEquals(0, new OozieCLI().run(args));
+                return null;
+            }
+        });
+    }
+
+    public void testClientWithoutAnonymous() throws Exception {
+        setSystemProperty("oozie.authentication.simple.anonymous.allowed", "false");
+        runTest(new Callable<Void>() {
+            public Void call() throws Exception {
+                String oozieUrl = getContextURL();
+                String[] args = new String[]{"admin", "-status", "-oozie", oozieUrl};
+                assertEquals(0, new OozieCLI().run(args));
+                return null;
+            }
+        });
+    }
+
+    public void testClientWithCustomAuthenticator() throws Exception {
+        setSystemProperty("authenticator.class", Authenticator4Test.class.getName());
+        setSystemProperty("oozie.authentication.simple.anonymous.allowed", "false");
+        Authenticator4Test.USED = false;
+        runTest(new Callable<Void>() {
+            public Void call() throws Exception {
+                String oozieUrl = getContextURL();
+                String[] args = new String[]{"admin", "-status", "-oozie", oozieUrl};
+                assertEquals(0, new OozieCLI().run(args));
+                return null;
+            }
+        });
+        assertTrue(Authenticator4Test.USED);
+    }
+
+
+    public void testClientAuthTokenCache() throws Exception {
+        //not using cache
+        setSystemProperty("oozie.authentication.simple.anonymous.allowed", "false");
+        AuthOozieClient.AUTH_TOKEN_CACHE_FILE.delete();
+        assertFalse(AuthOozieClient.AUTH_TOKEN_CACHE_FILE.exists());
+        runTest(new Callable<Void>() {
+            public Void call() throws Exception {
+                String oozieUrl = getContextURL();
+                String[] args = new String[]{"admin", "-status", "-oozie", oozieUrl};
+                assertEquals(0, new OozieCLI().run(args));
+                return null;
+            }
+        });
+        assertFalse(AuthOozieClient.AUTH_TOKEN_CACHE_FILE.exists());
+
+        //using cache
+        setSystemProperty("oozie.auth.token.cache", "true");
+        setSystemProperty("oozie.authentication.simple.anonymous.allowed", "false");
+        setSystemProperty("oozie.authentication.signature.secret", "secret");
+        AuthOozieClient.AUTH_TOKEN_CACHE_FILE.delete();
+        assertFalse(AuthOozieClient.AUTH_TOKEN_CACHE_FILE.exists());
+        runTest(new Callable<Void>() {
+            public Void call() throws Exception {
+                String oozieUrl = getContextURL();
+                String[] args = new String[]{"admin", "-status", "-oozie", oozieUrl};
+                assertEquals(0, new OozieCLI().run(args));
+                return null;
+            }
+        });
+        assertTrue(AuthOozieClient.AUTH_TOKEN_CACHE_FILE.exists());
+        String currentCache = IOUtils.getReaderAsString(new FileReader(AuthOozieClient.AUTH_TOKEN_CACHE_FILE), -1);
+
+        //re-using cache
+        setSystemProperty("oozie.auth.token.cache", "true");
+        setSystemProperty("oozie.authentication.simple.anonymous.allowed", "false");
+        setSystemProperty("oozie.authentication.signature.secret", "secret");
+        runTest(new Callable<Void>() {
+            public Void call() throws Exception {
+                String oozieUrl = getContextURL();
+                String[] args = new String[]{"admin", "-status", "-oozie", oozieUrl};
+                assertEquals(0, new OozieCLI().run(args));
+                return null;
+            }
+        });
+        assertTrue(AuthOozieClient.AUTH_TOKEN_CACHE_FILE.exists());
+        String newCache = IOUtils.getReaderAsString(new FileReader(AuthOozieClient.AUTH_TOKEN_CACHE_FILE), -1);
+        assertEquals(currentCache, newCache);
+    }
+
+}
diff --git core/src/test/java/org/apache/oozie/servlet/TestHostnameFilter.java core/src/test/java/org/apache/oozie/servlet/TestHostnameFilter.java
new file mode 100644
index 0000000..aed470e
--- /dev/null
+++ core/src/test/java/org/apache/oozie/servlet/TestHostnameFilter.java
@@ -0,0 +1,62 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.servlet;
+
+
+import javax.servlet.Filter;
+import javax.servlet.FilterChain;
+import javax.servlet.ServletException;
+import javax.servlet.ServletRequest;
+import javax.servlet.ServletResponse;
+import java.io.IOException;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+
+import junit.framework.Assert;
+import junit.framework.TestCase;
+import org.mockito.Mockito;
+
+public class TestHostnameFilter extends TestCase {
+
+    public void testHostname() throws Exception {
+        ServletRequest request = Mockito.mock(ServletRequest.class);
+        Mockito.when(request.getRemoteAddr()).thenReturn("localhost");
+
+        ServletResponse response = Mockito.mock(ServletResponse.class);
+
+        final AtomicBoolean invoked = new AtomicBoolean();
+
+        FilterChain chain = new FilterChain() {
+            @Override
+            public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse)
+                throws IOException, ServletException {
+                Assert.assertTrue(HostnameFilter.get().contains("localhost"));
+                invoked.set(true);
+            }
+        };
+
+        Filter filter = new HostnameFilter();
+        filter.init(null);
+        Assert.assertNull(HostnameFilter.get());
+        filter.doFilter(request, response, chain);
+        Assert.assertTrue(invoked.get());
+        Assert.assertNull(HostnameFilter.get());
+        filter.destroy();
+    }
+
+}
diff --git core/src/test/java/org/apache/oozie/test/XTestCase.java core/src/test/java/org/apache/oozie/test/XTestCase.java
index b976ecb..19287aa 100644
--- core/src/test/java/org/apache/oozie/test/XTestCase.java
+++ core/src/test/java/org/apache/oozie/test/XTestCase.java
@@ -50,6 +50,7 @@ import org.apache.oozie.CoordinatorJobBean;
 import org.apache.oozie.SLAEventBean;
 import org.apache.oozie.WorkflowActionBean;
 import org.apache.oozie.WorkflowJobBean;
+import org.apache.oozie.service.ConfigurationService;
 import org.apache.oozie.service.Services;
 import org.apache.oozie.service.WorkflowAppService;
 import org.apache.oozie.store.CoordinatorStore;
@@ -81,21 +82,29 @@ public abstract class XTestCase extends TestCase {
     private String hadoopVersion;
     protected XLog log = new XLog(LogFactory.getLog(getClass()));
 
+    private static File OOZIE_SRC_DIR = null;
     private static final String OOZIE_TEST_PROPERTIES = "oozie.test.properties";
 
     public static float WAITFOR_RATIO = Float.parseFloat(System.getProperty("oozie.test.waitfor.ratio", "1"));
 
     static {
         try {
-            // by default uses 'test.properties'
-            String testPropsFile = System.getProperty(OOZIE_TEST_PROPERTIES, "test.properties");
-            File file = new File(testPropsFile);
-            // the testPropsFile is looked at 'oozie-main' project level
-            // this trick here is to work both with Maven (runs the testcases from module directory)
-            // and IDEs (run testcases from 'oozie-main' project directory).
-            if (!file.exists()) {
-                file = new File(file.getAbsoluteFile().getParentFile().getParentFile(), testPropsFile);
+            OOZIE_SRC_DIR = new File("core").getAbsoluteFile();
+            if (!OOZIE_SRC_DIR.exists()) {
+                OOZIE_SRC_DIR = OOZIE_SRC_DIR.getParentFile().getParentFile();
+                OOZIE_SRC_DIR = new File(OOZIE_SRC_DIR, "core");
+            }
+            if (!OOZIE_SRC_DIR.exists()) {
+                System.err.println();
+                System.err.println("Could not determine project root directory");
+                System.err.println();
+                System.exit(-1);
             }
+            OOZIE_SRC_DIR = OOZIE_SRC_DIR.getParentFile();
+
+            String testPropsFile = System.getProperty(OOZIE_TEST_PROPERTIES, "test.properties");
+           File file = (testPropsFile.startsWith("/"))
+                        ? new File(testPropsFile) : new File(OOZIE_SRC_DIR, testPropsFile);
             if (file.exists()) {
                 System.out.println();
                 System.out.println("*********************************************************************************");
@@ -195,8 +204,7 @@ public abstract class XTestCase extends TestCase {
     @Override
     protected void setUp() throws Exception {
         super.setUp();
-        String baseDir = System.getProperty(OOZIE_TEST_DIR, "/tmp");
-        hadoopVersion = System.getProperty(HADOOP_VERSION, "0.20.0");
+        String baseDir = System.getProperty(OOZIE_TEST_DIR, new File("target/test-data").getAbsolutePath());
         String msg = null;
         if (!baseDir.startsWith("/")) {
             msg = XLog.format("System property [{0}]=[{1}] must be set to an absolute path", OOZIE_TEST_DIR, baseDir);
@@ -207,8 +215,18 @@ public abstract class XTestCase extends TestCase {
             }
         }
         if (msg != null) {
-            throw new Error(msg);
+            System.err.println();
+            System.err.println(msg);
+            System.exit(-1);
         }
+        File f = new File(baseDir);
+        f.mkdirs();
+        if (!f.exists() || !f.isDirectory()) {
+            System.err.println();
+            System.err.println(XLog.format("Could not create test dir [{0}]",  baseDir));
+            System.exit(-1);
+        }
+        hadoopVersion = System.getProperty(HADOOP_VERSION, "0.20.0");
         sysProps = new HashMap<String, String>();
         testCaseDir = createTestCaseDir(this, true);
 
@@ -217,27 +235,23 @@ public abstract class XTestCase extends TestCase {
         Services.setOozieHome();
         testCaseConfDir = createTestCaseSubDir("conf");
 
-        //setting up custom Oozie site for testing if avail
-        String customOozieSite = System.getProperty("oozie.test.config.file", "");
-        if (!customOozieSite.equals("")) {
-            if (!customOozieSite.startsWith("/")) {
-                System.err.println();
-                System.err.println(XLog.format(
-                        "Custom configuration file must be an absolute path [{0}]", customOozieSite));
-                System.err.println();
-                System.exit(-1);
-            }
-            File source = new File(customOozieSite);
-            if (!source.exists()) {
-                System.err.println();
-                System.err.println(XLog.format(
-                        "Custom configuration file for testing does no exist [{0}]", customOozieSite));
-                System.err.println();
-                System.exit(-1);
-            }
-            File target = new File(testCaseConfDir, "oozie-site.xml");
-            IOUtils.copyStream(new FileInputStream(source), new FileOutputStream(target));
+        // load test Oozie site
+        String oozieTestDB = System.getProperty("oozie.test.db", "hsqldb");
+        String defaultOozieSize =
+            new File(OOZIE_SRC_DIR, "core/src/test/resources/" + oozieTestDB + "-oozie-site.xml").getAbsolutePath();
+        String customOozieSite = System.getProperty("oozie.test.config.file", defaultOozieSize);
+        File source = (customOozieSite.startsWith("/"))
+                      ? new File(customOozieSite) : new File(OOZIE_SRC_DIR, customOozieSite);
+        source = source.getAbsoluteFile();
+        if (!source.exists()) {
+            System.err.println();
+            System.err.println(XLog.format("Custom configuration file for testing does no exist [{0}]", 
+                                           source.getAbsolutePath()));
+            System.err.println();
+            System.exit(-1);
         }
+        File target = new File(testCaseConfDir, "oozie-site.xml");
+        IOUtils.copyStream(new FileInputStream(source), new FileOutputStream(target));
 
         if (System.getProperty("oozielocal.log") == null) {
             setSystemProperty("oozielocal.log", "/tmp/oozielocal.log");
@@ -252,16 +266,10 @@ public abstract class XTestCase extends TestCase {
             System.setProperty("oozie.services.ext", "org.apache.oozie.service.HadoopAccessorService");
         }
 
-        if (System.getProperty("oozie.test.db", "hsqldb").equals("hsqldb")) {
-            setSystemProperty("oozie.service.JPAService.jdbc.driver", "org.hsqldb.jdbcDriver");
-            setSystemProperty("oozie.service.JPAService.jdbc.url", "jdbc:hsqldb:mem:oozie-db;create=true");
-        }
-        if (System.getProperty("oozie.test.db", "hsqldb").equals("derby")) {
-            delete(new File(baseDir, "oozie-derby"));
-            setSystemProperty("oozie.service.JPAService.jdbc.driver", "org.apache.derby.jdbc.EmbeddedDriver");
-            setSystemProperty("oozie.service.JPAService.jdbc.url", "jdbc:derby:" + baseDir +
-                                                                     "/oozie-derby;create=true");
+        if (System.getProperty("oozie.test.db.host") == null) {
+           System.setProperty("oozie.test.db.host", "localhost");
         }
+        setSystemProperty(ConfigurationService.OOZIE_DATA_DIR, testCaseDir);
     }
 
     /**
@@ -355,8 +363,8 @@ public abstract class XTestCase extends TestCase {
      */
     private String getTestCaseDirInternal(TestCase testCase) {
         ParamChecker.notNull(testCase, "testCase");
-        File dir = new File(System.getProperty(OOZIE_TEST_DIR, "/tmp"));
-        dir = new File(dir, "oozietests");
+        File dir = new File(System.getProperty(OOZIE_TEST_DIR, "target/test-data"));
+        dir = new File(dir, "oozietests").getAbsoluteFile();
         dir = new File(dir, testCase.getClass().getName());
         dir = new File(dir, testCase.getName());
         return dir.getAbsolutePath();
@@ -674,12 +682,17 @@ public abstract class XTestCase extends TestCase {
             UserGroupInformation.createUserForTesting(getTestUser2(), userGroups);
             UserGroupInformation.createUserForTesting(getTestUser3(), new String[] { "users" } );
 
+            conf.set("hadoop.tmp.dir", "/tmp/minicluster");
+
             dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);
             FileSystem fileSystem = dfsCluster.getFileSystem();
             fileSystem.mkdirs(new Path("/tmp"));
+            fileSystem.mkdirs(new Path("/tmp/minicluster/mapred"));
             fileSystem.mkdirs(new Path("/user"));
             fileSystem.mkdirs(new Path("/hadoop/mapred/system"));
             fileSystem.setPermission(new Path("/tmp"), FsPermission.valueOf("-rwxrwxrwx"));
+            fileSystem.setPermission(new Path("/tmp/minicluster"), FsPermission.valueOf("-rwxrwxrwx"));
+            fileSystem.setPermission(new Path("/tmp/minicluster/mapred"), FsPermission.valueOf("-rwxrwxrwx"));
             fileSystem.setPermission(new Path("/user"), FsPermission.valueOf("-rwxrwxrwx"));
             fileSystem.setPermission(new Path("/hadoop/mapred/system"), FsPermission.valueOf("-rwx------"));
             String nnURI = fileSystem.getUri().toString();
diff --git core/src/test/java/org/apache/oozie/util/TestCoordActionsInDateRange.java core/src/test/java/org/apache/oozie/util/TestCoordActionsInDateRange.java
index b06e306..6b799de 100644
--- core/src/test/java/org/apache/oozie/util/TestCoordActionsInDateRange.java
+++ core/src/test/java/org/apache/oozie/util/TestCoordActionsInDateRange.java
@@ -22,6 +22,8 @@ import java.util.Date;
 
 import org.apache.oozie.CoordinatorActionBean;
 import org.apache.oozie.CoordinatorJobBean;
+import org.apache.oozie.ErrorCode;
+import org.apache.oozie.XException;
 import org.apache.oozie.client.CoordinatorAction;
 import org.apache.oozie.client.CoordinatorJob;
 import org.apache.oozie.command.CommandException;
@@ -66,9 +68,45 @@ public class TestCoordActionsInDateRange extends XDataTestCase {
             long nominalTimeMilliseconds = nominalTime.getTime();
             long noOfMillisecondsinOneHour = 3600000;
 
-            // Testing for the number of coordinator actions in a date range that spans from half an hour prior to the nominal time to 1 hour after the nominal time
             String date1 = DateUtils.formatDateUTC(new Date(nominalTimeMilliseconds - (noOfMillisecondsinOneHour / 2)));
             String date2 = DateUtils.formatDateUTC(new Date(nominalTimeMilliseconds + noOfMillisecondsinOneHour));
+
+            // Test a bad date format.
+            try {
+              String badDate = "bad" + date1;
+              CoordActionsInDateRange.getCoordActionsFromDates(
+                  job.getId().toString(),
+                  badDate + "::" + date2);
+              fail("Accepted badly formatted date: " + badDate);
+            } catch (XException e) {
+              // Pass
+              assertEquals(ErrorCode.E0308, e.getErrorCode());
+            }
+
+            // Test a bad scope.
+            try {
+              String badScope = date1 + "0xbad5c09e" + date2;
+              CoordActionsInDateRange.getCoordActionsFromDates(
+                  job.getId().toString(),
+                  badScope);
+              fail("Accepted bad range scope: " + badScope);
+            } catch (XException e) {
+              // Pass
+              assertEquals(ErrorCode.E0308, e.getErrorCode());
+            }
+
+            // Test inverted start and end dates.
+            try {
+              CoordActionsInDateRange.getCoordActionsFromDates(
+                  job.getId().toString(),
+                  date2 + "::" + date1);
+              fail("Accepted inverted dates: [Start::End] = " + date2 + "::" + date1);
+            } catch (XException e) {
+              // Pass
+              assertEquals(ErrorCode.E0308, e.getErrorCode());
+            }
+
+            // Testing for the number of coordinator actions in a date range that spans from half an hour prior to the nominal time to 1 hour after the nominal time
             int noOfActions = CoordActionsInDateRange.getCoordActionsFromDates(job.getId().toString(), date1 + "::" + date2).size();
             assertEquals(1, noOfActions);
 
diff --git core/src/test/java/org/apache/oozie/workflow/lite/TestLiteWorkflowAppParser.java core/src/test/java/org/apache/oozie/workflow/lite/TestLiteWorkflowAppParser.java
index c4086f3..451ceb3 100644
--- core/src/test/java/org/apache/oozie/workflow/lite/TestLiteWorkflowAppParser.java
+++ core/src/test/java/org/apache/oozie/workflow/lite/TestLiteWorkflowAppParser.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *      http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -17,21 +17,34 @@
  */
 package org.apache.oozie.workflow.lite;
 
+
+import java.lang.reflect.Field;
+import java.lang.reflect.Method;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+
+
 import org.apache.oozie.service.LiteWorkflowStoreService;
 import org.apache.oozie.service.Services;
 import org.apache.oozie.service.ActionService;
 import org.apache.oozie.workflow.WorkflowException;
+import org.apache.oozie.workflow.lite.TestLiteWorkflowLib.TestActionNodeHandler;
+import org.apache.oozie.workflow.lite.TestLiteWorkflowLib.TestDecisionNodeHandler;
 import org.apache.oozie.test.XTestCase;
 import org.apache.oozie.util.IOUtils;
 import org.apache.oozie.ErrorCode;
 
 public class TestLiteWorkflowAppParser extends XTestCase {
+    public static String dummyConf = "<java></java>";
 
+    @Override
     protected void setUp() throws Exception {
         super.setUp();
         new Services().init();
     }
 
+    @Override
     protected void tearDown() throws Exception {
         Services.get().destroy();
         super.tearDown();
@@ -89,4 +102,297 @@ public class TestLiteWorkflowAppParser extends XTestCase {
         }
     }
 
+    /*
+     * 1->ok->2
+     * 2->ok->end
+     */
+   public void testWfNoForkJoin() throws WorkflowException  {
+        LiteWorkflowAppParser parser = new LiteWorkflowAppParser(null,
+                LiteWorkflowStoreService.LiteDecisionHandler.class,
+                LiteWorkflowStoreService.LiteActionHandler.class);
+
+        LiteWorkflowApp def = new LiteWorkflowApp("name", "def", new StartNodeDef("one"))
+            .addNode(new ActionNodeDef("one", dummyConf, TestActionNodeHandler.class, "two", "three"))
+            .addNode(new ActionNodeDef("two", dummyConf, TestActionNodeHandler.class, "end", "end"))
+            .addNode(new ActionNodeDef("three", dummyConf, TestActionNodeHandler.class, "end", "end"))
+            .addNode(new EndNodeDef( "end"));
+
+        try {
+            invokeForkJoin(parser, def);
+        } catch (Exception e) {
+            e.printStackTrace();
+            fail();
+        }
+    }
+
+    /*
+    f->(2,3)
+    (2,3)->j
+    */
+    public void testSimpleForkJoin() throws WorkflowException {
+        LiteWorkflowAppParser parser = new LiteWorkflowAppParser(null,
+                LiteWorkflowStoreService.LiteDecisionHandler.class,
+                LiteWorkflowStoreService.LiteActionHandler.class);
+
+        LiteWorkflowApp def = new LiteWorkflowApp("wf", "<worklfow-app/>", new StartNodeDef("one"))
+        .addNode(new ActionNodeDef("one", dummyConf, TestActionNodeHandler.class, "f", "end"))
+        .addNode(new ForkNodeDef("f", Arrays.asList(new String[]{"two", "three"})))
+        .addNode(new ActionNodeDef("two", dummyConf,  TestActionNodeHandler.class, "j", "end"))
+        .addNode(new ActionNodeDef("three", dummyConf, TestActionNodeHandler.class, "j", "end"))
+        .addNode(new JoinNodeDef("j", "four"))
+        .addNode(new ActionNodeDef("four", dummyConf, TestActionNodeHandler.class, "end", "end"))
+        .addNode(new EndNodeDef("end"));
+
+        try {
+            invokeForkJoin(parser, def);
+        } catch (Exception e) {
+            e.printStackTrace();
+            fail();
+        }
+    }
+
+    /*
+     f->(2,3)
+     2->f2
+     3->j
+     f2->(4,5,6)
+     (4,5,6)->j2
+     j2->7
+     7->j
+    */
+    public void testNestedForkJoin() throws WorkflowException{
+        LiteWorkflowAppParser parser = new LiteWorkflowAppParser(null,
+                LiteWorkflowStoreService.LiteDecisionHandler.class,
+                LiteWorkflowStoreService.LiteActionHandler.class);
+
+        LiteWorkflowApp def = new LiteWorkflowApp("testWf", "<worklfow-app/>", new StartNodeDef("one"))
+        .addNode(new ActionNodeDef("one", dummyConf, TestActionNodeHandler.class, "f","end"))
+        .addNode(new ForkNodeDef("f", Arrays.asList(new String[]{"two", "three"})))
+        .addNode(new ActionNodeDef("two", dummyConf, TestActionNodeHandler.class, "f2","end"))
+        .addNode(new ActionNodeDef("three", dummyConf, TestActionNodeHandler.class, "j","end"))
+        .addNode(new ForkNodeDef("f2", Arrays.asList(new String[]{"four", "five", "six"})))
+        .addNode(new ActionNodeDef("four", dummyConf, TestActionNodeHandler.class, "j2","end"))
+        .addNode(new ActionNodeDef("five", dummyConf, TestActionNodeHandler.class, "j2","end"))
+        .addNode(new ActionNodeDef("six", dummyConf, TestActionNodeHandler.class,"j2", "end"))
+        .addNode(new JoinNodeDef("j2", "seven"))
+        .addNode(new ActionNodeDef("seven", dummyConf, TestActionNodeHandler.class, "j","end"))
+        .addNode(new JoinNodeDef("j", "end"))
+        .addNode(new EndNodeDef("end"));
+
+        try {
+            invokeForkJoin(parser, def);
+        } catch (Exception e) {
+            e.printStackTrace();
+            fail();
+        }
+    }
+
+    /*
+      f->(2,3)
+      2->j
+      3->end
+    */
+    public void testForkJoinFailure() throws WorkflowException{
+        LiteWorkflowAppParser parser = new LiteWorkflowAppParser(null,
+                LiteWorkflowStoreService.LiteDecisionHandler.class,
+                LiteWorkflowStoreService.LiteActionHandler.class);
+
+        LiteWorkflowApp def = new LiteWorkflowApp("testWf", "<worklfow-app/>", new StartNodeDef("one"))
+        .addNode(new ActionNodeDef("one", dummyConf, TestActionNodeHandler.class, "f","end"))
+        .addNode(new ForkNodeDef("f", Arrays.asList(new String[]{"two", "three"})))
+        .addNode(new ActionNodeDef("two", dummyConf, TestActionNodeHandler.class, "j","end"))
+        .addNode(new ActionNodeDef("three", dummyConf, TestActionNodeHandler.class, "end","end"))
+        .addNode(new JoinNodeDef("j", "end"))
+        .addNode(new EndNodeDef("end"));
+
+        try {
+            invokeForkJoin(parser, def);
+            fail();
+        } catch (Exception ex) {
+            WorkflowException we = (WorkflowException) ex.getCause();
+            assertEquals(ErrorCode.E0730, we.getErrorCode());
+        }
+    }
+
+    /*
+     f->(2,3,4)
+     2->j
+     3->j
+     4->f2
+     f2->(5,6)
+     5-j2
+     6-j2
+     j-j2
+     j2-end
+    */
+    public void testNestedForkJoinFailure() throws WorkflowException {
+        LiteWorkflowAppParser parser = new LiteWorkflowAppParser(null,
+                LiteWorkflowStoreService.LiteDecisionHandler.class,
+                LiteWorkflowStoreService.LiteActionHandler.class);
+
+        LiteWorkflowApp def = new LiteWorkflowApp("testWf", "<worklfow-app/>", new StartNodeDef("one"))
+            .addNode(new ActionNodeDef("one", dummyConf, TestActionNodeHandler.class, "f","end"))
+            .addNode(new ForkNodeDef("f", Arrays.asList(new String[]{"four", "three", "two"})))
+            .addNode(new ActionNodeDef("two", dummyConf, TestActionNodeHandler.class, "j","end"))
+            .addNode(new ActionNodeDef("three", dummyConf, TestActionNodeHandler.class, "j","end"))
+            .addNode(new ActionNodeDef("four", dummyConf, TestActionNodeHandler.class, "f2","end"))
+            .addNode(new ForkNodeDef("f2", Arrays.asList(new String[]{"five", "six"})))
+            .addNode(new ActionNodeDef("five", dummyConf, TestActionNodeHandler.class, "j2","end"))
+            .addNode(new ActionNodeDef("six", dummyConf, TestActionNodeHandler.class, "j2","end"))
+            .addNode(new JoinNodeDef("j", "j2"))
+            .addNode(new JoinNodeDef("j2", "end"))
+            .addNode(new EndNodeDef("end"));
+
+        try {
+            invokeForkJoin(parser, def);
+            fail();
+        } catch (Exception ex) {
+            WorkflowException we = (WorkflowException) ex.getCause();
+            assertEquals(ErrorCode.E0730, we.getErrorCode());
+        }
+    }
+
+    /*
+     f->(2,3)
+     2->ok->3
+     2->fail->j
+     3->ok->j
+     3->fail->end
+    */
+    public void testTransitionFailure() throws WorkflowException{
+        LiteWorkflowAppParser parser = new LiteWorkflowAppParser(null,
+                LiteWorkflowStoreService.LiteDecisionHandler.class,
+                LiteWorkflowStoreService.LiteActionHandler.class);
+
+        LiteWorkflowApp def = new LiteWorkflowApp("name", "def", new StartNodeDef("one"))
+        .addNode(new ActionNodeDef("one", dummyConf, TestActionNodeHandler.class, "f","end"))
+        .addNode(new ForkNodeDef("f", Arrays.asList(new String[]{"two","three"})))
+        .addNode(new ActionNodeDef("two", dummyConf, TestActionNodeHandler.class, "three", "j"))
+        .addNode(new ActionNodeDef("three", dummyConf, TestActionNodeHandler.class, "j", "end"))
+        .addNode(new JoinNodeDef("j", "end"))
+        .addNode(new EndNodeDef("end"));
+
+        try {
+            invokeForkJoin(parser, def);
+            fail();
+        } catch (Exception ex) {
+            WorkflowException we = (WorkflowException) ex.getCause();
+            assertEquals(ErrorCode.E0734, we.getErrorCode());
+            // Make sure the message contains the nodes involved in the invalid transition
+            assertTrue(we.getMessage().contains("two"));
+            assertTrue(we.getMessage().contains("three"));
+        }
+
+    }
+
+    /*
+    f->(2,3)
+    2->decision node->{4,5,4}
+    4->j
+    5->j
+    3->j
+    */
+    public void testDecisionForkJoin() throws WorkflowException{
+        LiteWorkflowAppParser parser = new LiteWorkflowAppParser(null,
+                LiteWorkflowStoreService.LiteDecisionHandler.class,
+                LiteWorkflowStoreService.LiteActionHandler.class);
+        LiteWorkflowApp def = new LiteWorkflowApp("name", "def", new StartNodeDef("one"))
+        .addNode(new ActionNodeDef("one", dummyConf, TestActionNodeHandler.class, "f","end"))
+        .addNode(new ForkNodeDef("f", Arrays.asList(new String[]{"two","three"})))
+        .addNode(new DecisionNodeDef("two", dummyConf, TestDecisionNodeHandler.class, Arrays.asList(new String[]{"four","five","four"})))
+        .addNode(new ActionNodeDef("four", dummyConf, TestActionNodeHandler.class, "j", "end"))
+        .addNode(new ActionNodeDef("five", dummyConf, TestActionNodeHandler.class, "j", "end"))
+        .addNode(new ActionNodeDef("three", dummyConf, TestActionNodeHandler.class, "j", "end"))
+        .addNode(new JoinNodeDef("j", "end"))
+        .addNode(new EndNodeDef("end"));
+
+        try {
+            invokeForkJoin(parser, def);
+        } catch (Exception e) {
+            e.printStackTrace();
+            fail();
+        }
+    }
+
+    /*
+     *f->(2,3)
+     *2->decision node->{3,4}
+     *3->end
+     *4->end
+     */
+    public void testDecisionForkJoinFailure() throws WorkflowException{
+        LiteWorkflowAppParser parser = new LiteWorkflowAppParser(null,
+                LiteWorkflowStoreService.LiteDecisionHandler.class,
+                LiteWorkflowStoreService.LiteActionHandler.class);
+        LiteWorkflowApp def = new LiteWorkflowApp("name", "def", new StartNodeDef("one"))
+        .addNode(new ActionNodeDef("one", dummyConf, TestActionNodeHandler.class, "f","end"))
+        .addNode(new ForkNodeDef("f", Arrays.asList(new String[]{"two","three"})))
+        .addNode(new DecisionNodeDef("two", dummyConf, TestDecisionNodeHandler.class, Arrays.asList(new String[]{"four","three"})))
+        .addNode(new ActionNodeDef("three", dummyConf, TestActionNodeHandler.class, "j", "end"))
+        .addNode(new ActionNodeDef("four", dummyConf, TestActionNodeHandler.class, "j", "end"))
+        .addNode(new JoinNodeDef("j", "end"))
+        .addNode(new EndNodeDef("end"));
+
+        try {
+            invokeForkJoin(parser, def);
+            fail();
+        } catch (Exception ex) {
+            WorkflowException we = (WorkflowException) ex.getCause();
+            assertEquals(ErrorCode.E0734, we.getErrorCode());
+            // Make sure the message contains the nodes involved in the invalid transition
+            assertTrue(we.getMessage().contains("two"));
+            assertTrue(we.getMessage().contains("three"));
+        }
+    }
+
+    /*
+     * 1->decision node->{f1, f2}
+     * f1->(2,3)
+     * f2->(4,5)
+     * (2,3)->j1
+     * (4,5)->j2
+     * j1->end
+     * j2->end
+     */
+    public void testDecisionMultipleForks() throws WorkflowException{
+        LiteWorkflowAppParser parser = new LiteWorkflowAppParser(null,
+                LiteWorkflowStoreService.LiteDecisionHandler.class,
+                LiteWorkflowStoreService.LiteActionHandler.class);
+        LiteWorkflowApp def = new LiteWorkflowApp("name", "def", new StartNodeDef("one"))
+        .addNode(new DecisionNodeDef("one", dummyConf, TestDecisionNodeHandler.class, Arrays.asList(new String[]{"f1","f2"})))
+        .addNode(new ForkNodeDef("f1", Arrays.asList(new String[]{"two","three"})))
+        .addNode(new ForkNodeDef("f2", Arrays.asList(new String[]{"four","five"})))
+        .addNode(new ActionNodeDef("two", dummyConf, TestActionNodeHandler.class, "j1", "end"))
+        .addNode(new ActionNodeDef("three", dummyConf, TestActionNodeHandler.class, "j1", "end"))
+        .addNode(new ActionNodeDef("four", dummyConf, TestActionNodeHandler.class, "j2", "end"))
+        .addNode(new ActionNodeDef("five", dummyConf, TestActionNodeHandler.class, "j2", "end"))
+        .addNode(new JoinNodeDef("j1", "end"))
+        .addNode(new JoinNodeDef("j2", "end"))
+        .addNode(new EndNodeDef("end"));
+
+        try {
+            invokeForkJoin(parser, def);
+        } catch (Exception e) {
+            e.printStackTrace();
+            fail();
+        }
+    }
+
+    // Invoke private validateForkJoin method using Reflection API
+    private void invokeForkJoin(LiteWorkflowAppParser parser, LiteWorkflowApp def) throws Exception {
+        Class<? extends LiteWorkflowAppParser> c = parser.getClass();
+        Class d = Class.forName("org.apache.oozie.workflow.lite.LiteWorkflowAppParser$VisitStatus");
+        Field f = d.getField("VISITING");
+        Map traversed = new HashMap();
+        traversed.put(def.getNode(StartNodeDef.START).getName(), f);
+        Method validate = c.getDeclaredMethod("validate", LiteWorkflowApp.class, NodeDef.class, Map.class);
+        validate.setAccessible(true);
+        // invoke validate method to populate the fork and join list
+        validate.invoke(parser, def, def.getNode(StartNodeDef.START), traversed);
+        Method validateForkJoin = c.getDeclaredMethod("validateForkJoin", LiteWorkflowApp.class);
+        validateForkJoin.setAccessible(true);
+        // invoke validateForkJoin
+        validateForkJoin.invoke(parser, def);
+    }
+
 }
diff --git core/src/test/resources/coord-dataset-initial-instance.xml core/src/test/resources/coord-dataset-initial-instance.xml
new file mode 100644
index 0000000..2365733
--- /dev/null
+++ core/src/test/resources/coord-dataset-initial-instance.xml
@@ -0,0 +1,57 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<coordinator-app name="NAME" frequency="${coord:days(1)}" start="2009-02-01T01:00Z" end="2009-02-03T23:59Z" timezone="UTC" xmlns="uri:oozie:coordinator:0.2">
+ <controls>
+  <concurrency>2</concurrency>
+  <execution>LIFO</execution>
+ </controls>
+ <datasets>
+	<!-- Note the initial-instance is set to a date older than the default date Jan 01, 1970 00:00Z. -->
+  <dataset name="a" frequency="${coord:days(7)}" initial-instance="1960-02-01T01:00Z" timezone="UTC">
+   <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+  </dataset>
+  <dataset name="local_a" frequency="${coord:days(7)}" initial-instance="2009-02-01T01:00Z" timezone="UTC">
+   <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+  </dataset>
+ </datasets>
+ <input-events>
+   <data-in name="A" dataset="a">
+     <instance>${coord:latest(0)}</instance>
+   </data-in>
+ </input-events>
+ <output-events>
+   <data-out name="LOCAL_A" dataset="local_a">
+     <instance>${coord:current(-1)}</instance>
+   </data-out>
+ </output-events>
+ <action>
+  <workflow>
+   <app-path>hdfs:///tmp/workflows/</app-path>
+   <configuration>
+    <property>
+     <name>inputA</name>
+     <value>${coord:dataIn('A')}</value>
+    </property>
+    <property>
+     <name>inputB</name>
+     <value>${coord:dataOut('LOCAL_A')}</value>
+    </property>
+   </configuration>
+  </workflow>
+ </action>
+</coordinator-app>
diff --git core/src/test/resources/coord-multiple-input-instance1.xml core/src/test/resources/coord-multiple-input-instance1.xml
new file mode 100644
index 0000000..c284b32
--- /dev/null
+++ core/src/test/resources/coord-multiple-input-instance1.xml
@@ -0,0 +1,56 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<coordinator-app xmlns="uri:oozie:coordinator:0.2" name="NAME" frequency="${coord:days(1)}" start="2009-02-01T01:00Z" end="2009-02-03T23:59Z" timezone="UTC">
+  <controls>
+    <concurrency>2</concurrency>
+    <execution>LIFO</execution>
+  </controls>
+  <datasets>
+    <dataset name="a" frequency="${coord:days(7)}" initial-instance="2009-02-01T01:00Z" timezone="UTC">
+        <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+    </dataset>
+    <dataset name="local_a" frequency="${coord:days(7)}" initial-instance="2009-02-01T01:00Z" timezone="UTC">
+        <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+    </dataset>
+  </datasets>
+  <input-events>
+    <data-in name="A" dataset="a">
+    <instance>${coord:latest(0)},${coord:latest(-1)}</instance>
+    </data-in>
+  </input-events>
+  <output-events>
+    <data-out name="LOCAL_A" dataset="local_a">
+      <instance>${coord:current(-1)}</instance>
+    </data-out>
+  </output-events>
+  <action>
+    <workflow>
+      <app-path>hdfs:///tmp/workflows/</app-path>
+      <configuration>
+        <property>
+          <name>inputA</name>
+          <value>${coord:dataIn('A')}</value>
+        </property>
+        <property>
+          <name>inputB</name>
+          <value>${coord:dataOut('LOCAL_A')}</value>
+        </property>
+      </configuration>
+    </workflow>
+  </action>
+</coordinator-app>
\ No newline at end of file
diff --git core/src/test/resources/coord-multiple-input-instance2.xml core/src/test/resources/coord-multiple-input-instance2.xml
new file mode 100644
index 0000000..3ec1d8c
--- /dev/null
+++ core/src/test/resources/coord-multiple-input-instance2.xml
@@ -0,0 +1,57 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<coordinator-app xmlns="uri:oozie:coordinator:0.2" name="NAME" frequency="${coord:days(1)}" start="2009-02-01T01:00Z" end="2009-02-03T23:59Z" timezone="UTC">
+  <controls>
+    <concurrency>2</concurrency>
+    <execution>LIFO</execution>
+  </controls>
+  <datasets>
+    <dataset name="a" frequency="${coord:days(7)}" initial-instance="2009-02-01T01:00Z" timezone="UTC">
+        <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+    </dataset>
+    <dataset name="local_a" frequency="${coord:days(7)}" initial-instance="2009-02-01T01:00Z" timezone="UTC">
+        <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+    </dataset>
+  </datasets>
+  <input-events>
+    <data-in name="A" dataset="a">
+    <instance>${coord:latest(0)}</instance>
+    <instance></instance>
+    </data-in>
+  </input-events>
+  <output-events>
+    <data-out name="LOCAL_A" dataset="local_a">
+      <instance>${coord:current(-1)}</instance>
+    </data-out>
+  </output-events>
+  <action>
+    <workflow>
+      <app-path>hdfs:///tmp/workflows/</app-path>
+      <configuration>
+        <property>
+          <name>inputA</name>
+          <value>${coord:dataIn('A')}</value>
+        </property>
+        <property>
+          <name>inputB</name>
+          <value>${coord:dataOut('LOCAL_A')}</value>
+        </property>
+      </configuration>
+    </workflow>
+  </action>
+</coordinator-app>
\ No newline at end of file
diff --git core/src/test/resources/coord-multiple-input-instance3.xml core/src/test/resources/coord-multiple-input-instance3.xml
new file mode 100644
index 0000000..89fc259
--- /dev/null
+++ core/src/test/resources/coord-multiple-input-instance3.xml
@@ -0,0 +1,57 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<coordinator-app xmlns="uri:oozie:coordinator:0.2" name="NAME" frequency="${coord:days(1)}" start="2009-02-01T01:00Z" end="2009-02-03T23:59Z" timezone="UTC">
+  <controls>
+    <concurrency>2</concurrency>
+    <execution>LIFO</execution>
+  </controls>
+  <datasets>
+    <dataset name="a" frequency="${coord:days(7)}" initial-instance="2009-02-01T01:00Z" timezone="UTC">
+        <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+    </dataset>
+    <dataset name="local_a" frequency="${coord:days(7)}" initial-instance="2009-02-01T01:00Z" timezone="UTC">
+        <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+    </dataset>
+  </datasets>
+  <input-events>
+    <data-in name="A" dataset="a">
+    <instance>${coord:latest(0)}</instance>
+    <instance>${coord:latest(-1)}</instance>
+    </data-in>
+  </input-events>
+  <output-events>
+    <data-out name="LOCAL_A" dataset="local_a">
+      <instance>${coord:current(-1)}</instance>
+    </data-out>
+  </output-events>
+  <action>
+    <workflow>
+      <app-path>hdfs:///tmp/workflows/</app-path>
+      <configuration>
+        <property>
+          <name>inputA</name>
+          <value>${coord:dataIn('A')}</value>
+        </property>
+        <property>
+          <name>inputB</name>
+          <value>${coord:dataOut('LOCAL_A')}</value>
+        </property>
+      </configuration>
+    </workflow>
+  </action>
+</coordinator-app>
\ No newline at end of file
diff --git core/src/test/resources/coord-multiple-output-instance1.xml core/src/test/resources/coord-multiple-output-instance1.xml
new file mode 100644
index 0000000..96ff06d
--- /dev/null
+++ core/src/test/resources/coord-multiple-output-instance1.xml
@@ -0,0 +1,56 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<coordinator-app xmlns="uri:oozie:coordinator:0.2" name="NAME" frequency="${coord:days(1)}" start="2009-02-01T01:00Z" end="2009-02-03T23:59Z" timezone="UTC">
+  <controls>
+    <concurrency>2</concurrency>
+    <execution>LIFO</execution>
+  </controls>
+  <datasets>
+    <dataset name="a" frequency="${coord:days(7)}" initial-instance="2009-02-01T01:00Z" timezone="UTC">
+        <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+    </dataset>
+    <dataset name="local_a" frequency="${coord:days(7)}" initial-instance="2009-02-01T01:00Z" timezone="UTC">
+        <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+    </dataset>
+  </datasets>
+  <input-events>
+    <data-in name="A" dataset="a">
+    <instance>${coord:latest(0)}</instance>
+    </data-in>
+  </input-events>
+  <output-events>
+    <data-out name="LOCAL_A" dataset="local_a">
+      <instance>${coord:current(-1)},${coord:latest(-2)}</instance>
+    </data-out>
+  </output-events>
+  <action>
+    <workflow>
+      <app-path>hdfs:///tmp/workflows/</app-path>
+      <configuration>
+        <property>
+          <name>inputA</name>
+          <value>${coord:dataIn('A')}</value>
+        </property>
+        <property>
+          <name>inputB</name>
+          <value>${coord:dataOut('LOCAL_A')}</value>
+        </property>
+      </configuration>
+    </workflow>
+  </action>
+</coordinator-app>
\ No newline at end of file
diff --git core/src/test/resources/coord-multiple-output-instance2.xml core/src/test/resources/coord-multiple-output-instance2.xml
new file mode 100644
index 0000000..f30edff
--- /dev/null
+++ core/src/test/resources/coord-multiple-output-instance2.xml
@@ -0,0 +1,56 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<coordinator-app xmlns="uri:oozie:coordinator:0.2" name="NAME" frequency="${coord:days(1)}" start="2009-02-01T01:00Z" end="2009-02-03T23:59Z" timezone="UTC">
+  <controls>
+    <concurrency>2</concurrency>
+    <execution>LIFO</execution>
+  </controls>
+  <datasets>
+    <dataset name="a" frequency="${coord:days(7)}" initial-instance="2009-02-01T01:00Z" timezone="UTC">
+        <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+    </dataset>
+    <dataset name="local_a" frequency="${coord:days(7)}" initial-instance="2009-02-01T01:00Z" timezone="UTC">
+        <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+    </dataset>
+  </datasets>
+  <input-events>
+    <data-in name="A" dataset="a">
+    <instance>${coord:latest(0)}</instance>
+    </data-in>
+  </input-events>
+  <output-events>
+    <data-out name="LOCAL_A" dataset="local_a">
+      <instance></instance>
+    </data-out>
+  </output-events>
+  <action>
+    <workflow>
+      <app-path>hdfs:///tmp/workflows/</app-path>
+      <configuration>
+        <property>
+          <name>inputA</name>
+          <value>${coord:dataIn('A')}</value>
+        </property>
+        <property>
+          <name>inputB</name>
+          <value>${coord:dataOut('LOCAL_A')}</value>
+        </property>
+      </configuration>
+    </workflow>
+  </action>
+</coordinator-app>
\ No newline at end of file
diff --git core/src/test/resources/coord-multiple-output-instance3.xml core/src/test/resources/coord-multiple-output-instance3.xml
new file mode 100644
index 0000000..ab80cc8
--- /dev/null
+++ core/src/test/resources/coord-multiple-output-instance3.xml
@@ -0,0 +1,57 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<coordinator-app xmlns="uri:oozie:coordinator:0.2" name="NAME" frequency="${coord:days(1)}" start="2009-02-01T01:00Z" end="2009-02-03T23:59Z" timezone="UTC">
+  <controls>
+    <concurrency>2</concurrency>
+    <execution>LIFO</execution>
+  </controls>
+  <datasets>
+    <dataset name="a" frequency="${coord:days(7)}" initial-instance="2009-02-01T01:00Z" timezone="UTC">
+        <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+    </dataset>
+    <dataset name="local_a" frequency="${coord:days(7)}" initial-instance="2009-02-01T01:00Z" timezone="UTC">
+        <uri-template>file:///tmp/coord/workflows/${YEAR}/${DAY}</uri-template>
+    </dataset>
+  </datasets>
+  <input-events>
+    <data-in name="A" dataset="a">
+    <instance>${coord:latest(0)}</instance>
+    </data-in>
+  </input-events>
+  <output-events>
+    <data-out name="LOCAL_A" dataset="local_a">
+      <instance></instance>
+      <instance></instance>
+    </data-out>
+  </output-events>
+  <action>
+    <workflow>
+      <app-path>hdfs:///tmp/workflows/</app-path>
+      <configuration>
+        <property>
+          <name>inputA</name>
+          <value>${coord:dataIn('A')}</value>
+        </property>
+        <property>
+          <name>inputB</name>
+          <value>${coord:dataOut('LOCAL_A')}</value>
+        </property>
+      </configuration>
+    </workflow>
+  </action>
+</coordinator-app>
\ No newline at end of file
diff --git core/src/test/resources/derby-oozie-site.xml core/src/test/resources/derby-oozie-site.xml
new file mode 100644
index 0000000..9f251cc
--- /dev/null
+++ core/src/test/resources/derby-oozie-site.xml
@@ -0,0 +1,26 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+  Copyright (c) 2010 Yahoo! Inc. All rights reserved.
+  Licensed under the Apache License, Version 2.0 (the "License");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License. See accompanying LICENSE file.
+-->
+<configuration>
+    <property>
+        <name>oozie.service.JPAService.jdbc.driver</name>
+        <value>org.apache.derby.jdbc.EmbeddedDriver</value>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.url</name>
+        <value>jdbc:derby:${oozie.data.dir}/oozie-derby;create=true</value>
+    </property>
+</configuration>
diff --git core/src/test/resources/hsqldb-oozie-site.xml core/src/test/resources/hsqldb-oozie-site.xml
new file mode 100644
index 0000000..825c225
--- /dev/null
+++ core/src/test/resources/hsqldb-oozie-site.xml
@@ -0,0 +1,26 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+  Copyright (c) 2010 Yahoo! Inc. All rights reserved.
+  Licensed under the Apache License, Version 2.0 (the "License");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License. See accompanying LICENSE file.
+-->
+<configuration>
+    <property>
+        <name>oozie.service.JPAService.jdbc.driver</name>
+        <value>org.hsqldb.jdbcDriver</value>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.url</name>
+        <value>jdbc:hsqldb:mem:oozie-db;create=true</value>
+    </property>
+</configuration>
diff --git core/src/test/resources/mysql-oozie-site.xml core/src/test/resources/mysql-oozie-site.xml
index d5a6107..eab5c4d 100644
--- core/src/test/resources/mysql-oozie-site.xml
+++ core/src/test/resources/mysql-oozie-site.xml
@@ -7,9 +7,9 @@
   to you under the Apache License, Version 2.0 (the
   "License"); you may not use this file except in compliance
   with the License.  You may obtain a copy of the License at
-  
+
        http://www.apache.org/licenses/LICENSE-2.0
-  
+
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -19,22 +19,26 @@
 <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
 <configuration>
     <property>
-        <name>oozie.service.DataSourceService.jdbc.driver</name>
+      <name>oozie.service.JPAService.jdbc.driver</name>
         <value>com.mysql.jdbc.Driver</value>
         <description>JDBC driver class.</description>
     </property>
     <property>
-        <name>oozie.service.DataSourceService.jdbc.url</name>
-        <value>jdbc:mysql://localhost:3306</value>
+        <name>oozie.test.db.port</name>
+        <value>3306</value>
+    </property>
+    <property>
+      <name>oozie.service.JPAService.jdbc.url</name>
+        <value>jdbc:mysql://${oozie.test.db.host}:${oozie.test.db.port}/oozie</value>
         <description>JDBC URL.</description>
     </property>
     <property>
-        <name>oozie.service.DataSourceService.jdbc.username</name>
+        <name>oozie.service.JPAService.jdbc.username</name>
         <value>oozie</value>
         <description>DB user name.</description>
     </property>
     <property>
-        <name>oozie.service.DataSourceService.jdbc.password</name>
+        <name>oozie.service.JPAService.jdbc.password</name>
         <value>oozie</value>
         <description>
             DB user password. IMPORTANT: if password is emtpy leave a 1 space string, the service trims the
diff --git core/src/test/resources/oracle-oozie-site.xml core/src/test/resources/oracle-oozie-site.xml
new file mode 100644
index 0000000..f199779
--- /dev/null
+++ core/src/test/resources/oracle-oozie-site.xml
@@ -0,0 +1,42 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+  Copyright (c) 2010 Yahoo! Inc. All rights reserved.
+  Licensed under the Apache License, Version 2.0 (the "License");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License. See accompanying LICENSE file.
+-->
+<configuration>
+    <property>
+        <name>oozie.service.JPAService.jdbc.driver</name>
+        <value>oracle.jdbc.driver.OracleDriver</value>
+    </property>
+    <property>
+        <name>oozie.test.db.port</name>
+        <value>1521</value>
+    </property>
+    <property>
+        <name>oozie.test.db.name</name>
+        <value>xe</value>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.url</name>
+        <value>jdbc:oracle:thin:@//${oozie.test.db.host}:${oozie.test.db.port}/${oozie.test.db.name}</value>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.username</name>
+        <value>oozie</value>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.password</name>
+        <value>oozie</value>
+    </property>
+</configuration>
diff --git core/src/test/resources/postgres-oozie-site.xml core/src/test/resources/postgres-oozie-site.xml
new file mode 100644
index 0000000..840e1c9
--- /dev/null
+++ core/src/test/resources/postgres-oozie-site.xml
@@ -0,0 +1,42 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+  Copyright (c) 2010 Yahoo! Inc. All rights reserved.
+  Licensed under the Apache License, Version 2.0 (the "License");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License. See accompanying LICENSE file.
+-->
+<configuration>
+    <property>
+        <name>oozie.service.JPAService.jdbc.driver</name>
+        <value>org.postgresql.Driver</value>
+    </property>
+    <property>
+        <name>oozie.test.db.port</name>
+        <value>5432</value>
+    </property>
+    <property>
+        <name>oozie.test.db.name</name>
+        <value>oozie</value>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.url</name>
+        <value>jdbc:postgresql://${oozie.test.db.host}:${oozie.test.db.port}/${oozie.test.db.name}</value>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.username</name>
+        <value>oozie</value>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.password</name>
+        <value>oozie</value>
+    </property>
+</configuration>
diff --git core/src/test/resources/postgresql-oozie-site.xml core/src/test/resources/postgresql-oozie-site.xml
index f7953d3..e69de29 100644
--- core/src/test/resources/postgresql-oozie-site.xml
+++ core/src/test/resources/postgresql-oozie-site.xml
@@ -1,44 +0,0 @@
-<?xml version="1.0"?>
-<!--
-  Licensed to the Apache Software Foundation (ASF) under one
-  or more contributor license agreements.  See the NOTICE file
-  distributed with this work for additional information
-  regarding copyright ownership.  The ASF licenses this file
-  to you under the Apache License, Version 2.0 (the
-  "License"); you may not use this file except in compliance
-  with the License.  You may obtain a copy of the License at
-  
-       http://www.apache.org/licenses/LICENSE-2.0
-  
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License.
--->
-<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
-<configuration>
-    <property>
-        <name>oozie.service.JPAService.jdbc.driver</name>
-        <value>org.postgresql.Driver</value>
-        <description>JDBC driver class.</description>
-    </property>
-    <property>
-        <name>oozie.service.JPAService.jdbc.url</name>
-        <value>jdbc:postgresql://localhost:5432/OOZIEDB</value>
-        <description>JDBC URL.</description>
-    </property>
-    <property>
-        <name>oozie.service.JPAService.jdbc.username</name>
-        <value>OOZIE</value>
-        <description>DB user name.</description>
-    </property>
-    <property>
-        <name>oozie.service.JPAService.jdbc.password</name>
-        <value>oozie</value>
-        <description>
-            DB user password. IMPORTANT: if password is emtpy leave a 1 space string, the service trims the
-            value, if empty Configuration assumes it is NULL.
-        </description>
-    </property>
-</configuration>
diff --git core/src/test/resources/rerun-varsub-wf.xml core/src/test/resources/rerun-varsub-wf.xml
new file mode 100644
index 0000000..84348ae
--- /dev/null
+++ core/src/test/resources/rerun-varsub-wf.xml
@@ -0,0 +1,38 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<workflow-app xmlns="uri:oozie:workflow:0.3" name="rerun-wf">
+    <start to="fs1"/>
+    <action name="fs1">
+        <fs>
+            <mkdir path="${srcDir}"/>
+        </fs>
+        <ok to="fs2"/>
+        <error to="k"/>
+    </action>
+    <action name="fs2">
+        <fs>
+            <move source="${srcDir}" target="${dstDir}"/>
+        </fs>
+        <ok to="end"/>
+        <error to="k"/>
+    </action>
+    <kill name="k">
+        <message>kill</message>
+    </kill>
+    <end name="end"/>
+</workflow-app>
diff --git core/src/test/resources/user-hive-default.xml core/src/test/resources/user-hive-default.xml
new file mode 100644
index 0000000..4fedb07
--- /dev/null
+++ core/src/test/resources/user-hive-default.xml
@@ -0,0 +1,451 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+<configuration>
+
+<!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
+<!-- that are implied by Hadoop setup variables.                                                -->
+<!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
+<!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
+<!-- resource).                                                                                 -->
+
+<!-- Hive Execution Parameters -->
+<property>
+  <name>mapred.reduce.tasks</name>
+  <value>-1</value>
+    <description>The default number of reduce tasks per job.  Typically set
+  to a prime close to the number of available hosts.  Ignored when
+  mapred.job.tracker is "local". Hadoop set this to 1 by default, whereas hive uses -1 as its default value.
+  By setting this property to -1, Hive will automatically figure out what should be the number of reducers.
+  </description>
+</property>
+
+<property>
+  <name>hive.exec.reducers.bytes.per.reducer</name>
+  <value>1000000000</value>
+  <description>size per reducer.The default is 1G, i.e if the input size is 10G, it will use 10 reducers.</description>
+</property>
+
+<property>
+  <name>hive.exec.reducers.max</name>
+  <value>999</value>
+  <description>max number of reducers will be used. If the one
+	specified in the configuration parameter mapred.reduce.tasks is
+	negative, hive will use this one as the max number of reducers when
+	automatically determine number of reducers.</description>
+</property>
+
+<property>
+  <name>hive.exec.scratchdir</name>
+  <value>/tmp/hive-${user.name}</value>
+  <description>Scratch space for Hive jobs</description>
+</property>
+
+<property>
+  <name>hive.test.mode</name>
+  <value>false</value>
+  <description>whether hive is running in test mode. If yes, it turns on sampling and prefixes the output tablename</description>
+</property>
+
+<property>
+  <name>hive.test.mode.prefix</name>
+  <value>test_</value>
+  <description>if hive is running in test mode, prefixes the output table by this string</description>
+</property>
+
+<!-- If the input table is not bucketed, the denominator of the tablesample is determinied by the parameter below   -->
+<!-- For example, the following query:                                                                              -->
+<!--   INSERT OVERWRITE TABLE dest                                                                                  -->
+<!--   SELECT col1 from src                                                                                         -->
+<!-- would be converted to                                                                                          -->
+<!--   INSERT OVERWRITE TABLE test_dest                                                                             -->
+<!--   SELECT col1 from src TABLESAMPLE (BUCKET 1 out of 32 on rand(1))                                             -->
+<property>
+  <name>hive.test.mode.samplefreq</name>
+  <value>32</value>
+  <description>if hive is running in test mode and table is not bucketed, sampling frequency</description>
+</property>
+
+<property>
+  <name>hive.test.mode.nosamplelist</name>
+  <value></value>
+  <description>if hive is running in test mode, dont sample the above comma seperated list of tables</description>
+</property>
+
+<property>
+  <name>hive.metastore.local</name>
+  <value>true</value>
+  <description>controls whether to connect to remove metastore server or open a new metastore server in Hive Client JVM</description>
+</property>
+
+<property>
+  <name>javax.jdo.option.ConnectionURL</name>
+  <value>jdbc:derby:;databaseName=metastore_db;create=true</value>
+  <description>JDBC connect string for a JDBC metastore</description>
+</property>
+
+<property>
+  <name>javax.jdo.option.ConnectionDriverName</name>
+  <value>org.apache.derby.jdbc.EmbeddedDriver</value>
+  <description>Driver class name for a JDBC metastore</description>
+</property>
+
+<property>
+  <name>javax.jdo.PersistenceManagerFactoryClass</name>
+  <value>org.datanucleus.jdo.JDOPersistenceManagerFactory</value>
+  <description>class implementing the jdo persistence</description>
+</property>
+
+<property>
+  <name>javax.jdo.option.DetachAllOnCommit</name>
+  <value>true</value>
+  <description>detaches all objects from session so that they can be used after transaction is committed</description>
+</property>
+
+<property>
+  <name>javax.jdo.option.NonTransactionalRead</name>
+  <value>true</value>
+  <description>reads outside of transactions</description>
+</property>
+
+<property>
+  <name>javax.jdo.option.ConnectionUserName</name>
+  <value>APP</value>
+  <description>username to use against metastore database</description>
+</property>
+
+<!--<property>-->
+  <!--<name>javax.jdo.option.ConnectionPassword</name>-->
+  <!--<value>mine</value>-->
+  <!--<description>password to use against metastore database</description>-->
+<!--</property>-->
+
+<property>
+  <name>datanucleus.validateTables</name>
+  <value>false</value>
+  <description>validates existing schema against code. turn this on if you want to verify existing schema </description>
+</property>
+
+<property>
+  <name>datanucleus.validateColumns</name>
+  <value>false</value>
+  <description>validates existing schema against code. turn this on if you want to verify existing schema </description>
+</property>
+
+<property>
+  <name>datanucleus.validateConstraints</name>
+  <value>false</value>
+  <description>validates existing schema against code. turn this on if you want to verify existing schema </description>
+</property>
+
+<property>
+  <name>datanucleus.storeManagerType</name>
+  <value>rdbms</value>
+  <description>metadata store type</description>
+</property>
+
+<property>
+  <name>datanucleus.autoCreateSchema</name>
+  <value>true</value>
+  <description>creates necessary schema on a startup if one doesn't exist. set this to false, after creating it once</description>
+</property>
+
+<property>
+  <name>datanucleus.autoStartMechanismMode</name>
+  <value>checked</value>
+  <description>throw exception if metadata tables are incorrect</description>
+</property>
+
+<property>
+  <name>datancucleus.transactionIsolation</name>
+  <value>read-committed</value>
+  <description></description>
+</property>
+
+<property>
+  <name>datanuclues.cache.level2</name>
+  <value>true</value>
+  <description>use a level 2 cache. turn this off if metadata is changed independently of hive metastore server</description>
+</property>
+
+<property>
+  <name>datanuclues.cache.level2.type</name>
+  <value>SOFT</value>
+  <description>SOFT=soft reference based cache, WEAK=weak reference based cache.</description>
+</property>
+
+<property>
+  <name>hive.metastore.warehouse.dir</name>
+  <value>/user/hive/warehouse</value>
+  <description>location of default database for the warehouse</description>
+</property>
+
+<property>
+  <name>hive.metastore.connect.retries</name>
+  <value>5</value>
+  <description>Number of retries while opening a connection to metastore</description>
+</property>
+
+<property>
+  <name>hive.metastore.rawstore.impl</name>
+  <value>org.apache.hadoop.hive.metastore.ObjectStore</value>
+  <description>Name of the class that implements org.apache.hadoop.hive.metastore.rawstore interface. This class is used to store and retrieval of raw metadata objects such as table, database</description>
+</property>
+
+<property>
+  <name>hive.default.fileformat</name>
+  <value>TextFile</value>
+  <description>Default file format for CREATE TABLE statement. Options are TextFile and SequenceFile. Users can explicitly say CREATE TABLE ... STORED AS &lt;TEXTFILE|SEQUENCEFILE&gt; to override</description>
+</property>
+
+<property>
+  <name>hive.fileformat.check</name>
+  <value>true</value>
+  <description>Whether to check file format or not when loading data files</description>
+</property>
+
+<property>
+  <name>hive.map.aggr</name>
+  <value>true</value>
+  <description>Whether to use map-side aggregation in Hive Group By queries</description>
+</property>
+
+<property>
+  <name>hive.groupby.skewindata</name>
+  <value>false</value>
+  <description>Whether there is skew in data to optimize group by queries</description>
+</property>
+
+<property>
+  <name>hive.groupby.mapaggr.checkinterval</name>
+  <value>100000</value>
+  <description>Number of rows after which size of the grouping keys/aggregation classes is performed</description>
+</property>
+
+<property>
+  <name>hive.mapred.local.mem</name>
+  <value>0</value>
+  <description>For local mode, memory of the mappers/reducers</description>
+</property>
+
+<property>
+  <name>hive.map.aggr.hash.percentmemory</name>
+  <value>0.5</value>
+  <description>Portion of total memory to be used by map-side grup aggregation hash table</description>
+</property>
+
+<property>
+  <name>hive.map.aggr.hash.min.reduction</name>
+  <value>0.5</value>
+  <description>Hash aggregation will be turned off if the ratio between hash
+  table size and input rows is bigger than this number. Set to 1 to make sure
+  hash aggregation is never turned off.</description>
+</property>
+
+<property>
+  <name>hive.optimize.cp</name>
+  <value>true</value>
+  <description>Whether to enable column pruner</description>
+</property>
+
+<property>
+  <name>hive.optimize.ppd</name>
+  <value>true</value>
+  <description>Whether to enable predicate pushdown</description>
+</property>
+
+<property>
+  <name>hive.optimize.pruner</name>
+  <value>true</value>
+  <description>Whether to enable the new partition pruner which depends on predicate pushdown. If this is disabled,
+  the old partition pruner which is based on AST will be enabled.</description>
+</property>
+
+<property>
+  <name>hive.optimize.groupby</name>
+  <value>true</value>
+  <description>Whether to enable the bucketed group by from bucketed partitions/tables.</description>
+</property>
+
+<property>
+  <name>hive.join.emit.interval</name>
+  <value>1000</value>
+  <description>How many rows in the right-most join operand Hive should buffer before emitting the join result. </description>
+</property>
+
+<property>
+  <name>hive.join.cache.size</name>
+  <value>25000</value>
+  <description>How many rows in the joining tables (except the streaming table) should be cached in memory. </description>
+</property>
+
+<property>
+  <name>hive.mapjoin.bucket.cache.size</name>
+  <value>100</value>
+  <description>How many values in each keys in the map-joined table should be cached in memory. </description>
+</property>
+
+<property>
+  <name>hive.mapjoin.maxsize</name>
+  <value>100000</value>
+  <description>Maximum # of rows of the small table that can be handled by map-side join. If the size is reached and hive.task.progress is set, a fatal error counter is set and the job will be killed.</description>
+</property>
+
+<property>
+  <name>hive.mapjoin.cache.numrows</name>
+  <value>25000</value>
+  <description>How many rows should be cached by jdbm for map join. </description>
+</property>
+
+<property>
+  <name>hive.mapred.mode</name>
+  <value>nonstrict</value>
+  <description>The mode in which the hive operations are being performed. In strict mode, some risky queries are not allowed to run</description>
+</property>
+
+<property>
+  <name>hive.exec.script.maxerrsize</name>
+  <value>100000</value>
+  <description>Maximum number of bytes a script is allowed to emit to standard error (per map-reduce task). This prevents runaway scripts from filling logs partitions to capacity </description>
+</property>
+
+<property>
+  <name>hive.exec.script.allow.partial.consumption</name>
+  <value>false</value>
+  <description> When enabled, this option allows a user script to exit successfully without consuming all the data from the standard input.
+  </description>
+</property>
+
+<property>
+  <name>hive.script.operator.id.env.var</name>
+  <value>HIVE_SCRIPT_OPERATOR_ID</value>
+  <description> Name of the environment variable that holds the unique script operator ID in the user's transform function (the custom mapper/reducer that the user has specified in the query)
+  </description>
+</property>
+
+<property>
+  <name>hive.exec.compress.output</name>
+  <value>false</value>
+  <description> This controls whether the final outputs of a query (to a local/hdfs file or a hive table) is compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* </description>
+</property>
+
+<property>
+  <name>hive.exec.compress.intermediate</name>
+  <value>false</value>
+  <description> This controls whether intermediate files produced by hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* </description>
+</property>
+
+<property>
+  <name>hive.exec.parallel</name>
+  <value>false</value>
+  <description>Whether to execute jobs in parallel</description>
+</property>
+
+<property>
+  <name>hive.hwi.war.file</name>
+  <value>lib/hive-hwi-0.5.0+20.war</value>
+  <description>This sets the path to the HWI war file, relative to ${HIVE_HOME}. </description>
+</property>
+
+<property>
+  <name>hive.hwi.listen.host</name>
+  <value>0.0.0.0</value>
+  <description>This is the host address the Hive Web Interface will listen on</description>
+</property>
+
+<property>
+  <name>hive.hwi.listen.port</name>
+  <value>9999</value>
+  <description>This is the port the Hive Web Interface will listen on</description>
+</property>
+
+<property>
+  <name>hive.exec.pre.hooks</name>
+  <value></value>
+  <description>Pre Execute Hook for Tests</description>
+</property>
+
+<property>
+  <name>hive.merge.mapfiles</name>
+  <value>true</value>
+  <description>Merge small files at the end of a map-only job</description>
+</property>
+
+<property>
+  <name>hive.merge.mapredfiles</name>
+  <value>false</value>
+  <description>Merge small files at the end of any job(map only or map-reduce)</description>
+</property>
+
+<property>
+  <name>hive.heartbeat.interval</name>
+  <value>1000</value>
+  <description>Send a heartbeat after this interval - used by mapjoin and filter operators</description>
+</property>
+
+<property>
+  <name>hive.merge.size.per.task</name>
+  <value>256000000</value>
+  <description>Size of merged files at the end of the job</description>
+</property>
+
+<property>
+  <name>hive.script.auto.progress</name>
+  <value>false</value>
+  <description>Whether Hive Tranform/Map/Reduce Clause should automatically send progress information to TaskTracker to avoid the task getting killed because of inactivity.  Hive sends progress information when the script is outputting to stderr.  This option removes the need of periodically producing stderr messages, but users should be cautious because this may prevent infinite loops in the scripts to be killed by TaskTracker.  </description>
+</property>
+
+<property>
+  <name>hive.script.serde</name>
+  <value>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</value>
+  <description>The default serde for trasmitting input data to and reading output data from the user scripts. </description>
+</property>
+
+<property>
+  <name>hive.script.recordreader</name>
+  <value>org.apache.hadoop.hive.ql.exec.TextRecordReader</value>
+  <description>The default record reader for reading data from the user scripts. </description>
+</property>
+
+<property>
+  <name>hive.script.recordwriter</name>
+  <value>org.apache.hadoop.hive.ql.exec.TextRecordWriter</value>
+  <description>The default record writer for writing data to the user scripts. </description>
+</property>
+
+<property>
+  <name>hive.input.format</name>
+  <value>org.apache.hadoop.hive.ql.io.HiveInputFormat</value>
+  <description>The default input format, if it is not specified, the system assigns it. It is set to HiveInputFormat for hadoop versions 17, 18 and 19, whereas it is set to CombinedHiveInputFormat for hadoop 20. The user can always overwrite it - if there is a bug in CombinedHiveInputFormat, it can always be manually set to HiveInputFormat. </description>
+</property>
+
+<property>
+  <name>hive.udtf.auto.progress</name>
+  <value>false</value>
+  <description>Whether Hive should automatically send progress information to TaskTracker when using UDTF's to prevent the task getting killed because of inactivity.  Users should be cautious because this may prevent TaskTracker from killing tasks with infinte loops.  </description>
+</property>
+
+<property>
+  <name>hive.mapred.reduce.tasks.speculative.execution</name>
+  <value>true</value>
+  <description>Whether speculative execution for reducers should be turned on. </description>
+</property>
+
+</configuration>
diff --git core/src/test/resources/wf-schema-valid.xml core/src/test/resources/wf-schema-valid.xml
index 38a8671..099b44e 100644
--- core/src/test/resources/wf-schema-valid.xml
+++ core/src/test/resources/wf-schema-valid.xml
@@ -21,7 +21,7 @@
         <switch>
             <case to="b">true</case>
             <case to="c">false</case>
-            <default to="d"/>
+            <default to="c"/>
         </switch>
     </decision>
     <kill name="b">
@@ -59,7 +59,7 @@
             <file>/tmp</file>
             <archive>/tmp</archive>
         </map-reduce>
-        <ok to="e"/>
+        <ok to="f"/>
         <error to="b"/>
     </action>
 
@@ -86,11 +86,11 @@
             <file>/tmp</file>
             <file>/tmp</file>
         </pig>
-        <ok to="z"/>
+        <ok to="f"/>
         <error to="b"/>
     </action>
 
-    <join name="f" to="g"/>
+    <join name="f" to="z"/>
 
     <end name="z"/>
 </workflow-app>
diff --git distro/pom.xml distro/pom.xml
index 22a5b64..a17cce1 100644
--- distro/pom.xml
+++ distro/pom.xml
@@ -16,17 +16,19 @@
   See the License for the specific language governing permissions and
   limitations under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
     <modelVersion>4.0.0</modelVersion>
     <parent>
         <groupId>org.apache.oozie</groupId>
         <artifactId>oozie-main</artifactId>
         <version>3.1.3-incubating</version>
     </parent>
-    <!-- calling the artifact oozie to the generated distro file is oozie- -->
-    <artifactId>oozie</artifactId>
-    <description>Oozie Distro</description>
-    <name>Oozie Distro</name>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-distro</artifactId>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie Distro</description>
+    <name>Apache Oozie Distro</name>
     <packaging>jar</packaging>
 
     <dependencies>
@@ -59,6 +61,7 @@
                 <groupId>org.apache.maven.plugins</groupId>
                 <artifactId>maven-assembly-plugin</artifactId>
                 <configuration>
+                    <finalName>oozie-${project.version}</finalName>
                     <descriptors>
                         <descriptor>../src/main/assemblies/distro.xml</descriptor>
                     </descriptors>
@@ -73,20 +76,22 @@
                     <execution>
                         <configuration>
                             <target>
-                                <mkdir dir="downloads" />
-                                <get src="http://archive.apache.org/dist/tomcat/tomcat-6/v6.0.32/bin/apache-tomcat-6.0.32.tar.gz" dest="downloads/tomcat.tar.gz" verbose="true" skipexisting="true" />
-                                <delete dir="target/tomcat" />
-                                <mkdir dir="target/tomcat" />
-                                <gunzip src="downloads/tomcat.tar.gz" dest="target/tomcat/tomcat.tar" />
-                                <untar src="target/tomcat/tomcat.tar" dest="target/tomcat" />
-                                <move file="target/tomcat/apache-tomcat-6.0.32" tofile="target/tomcat/oozie-server" />
-                                <delete dir="target/tomcat/oozie-server/webapps" />
-                                <mkdir dir="target/tomcat/oozie-server/webapps" />
-                                <delete file="target/tomcat/oozie-server/conf/server.xml" />
-                                <copy file="src/main/tomcat/server.xml" toDir="target/tomcat/oozie-server/conf" />
-                                <copy file="src/main/tomcat/logging.properties" toDir="target/tomcat/oozie-server/conf" />
+                                <mkdir dir="downloads"/>
+                                <get src="http://archive.apache.org/dist/tomcat/tomcat-6/v6.0.32/bin/apache-tomcat-6.0.32.tar.gz"
+                                     dest="downloads/tomcat.tar.gz" verbose="true" skipexisting="true"/>
+                                <delete dir="target/tomcat"/>
+                                <mkdir dir="target/tomcat"/>
+                                <gunzip src="downloads/tomcat.tar.gz" dest="target/tomcat/tomcat.tar"/>
+                                <untar src="target/tomcat/tomcat.tar" dest="target/tomcat"/>
+                                <move file="target/tomcat/apache-tomcat-6.0.32" tofile="target/tomcat/oozie-server"/>
+                                <delete dir="target/tomcat/oozie-server/webapps"/>
+                                <mkdir dir="target/tomcat/oozie-server/webapps"/>
+                                <delete file="target/tomcat/oozie-server/conf/server.xml"/>
+                                <copy file="src/main/tomcat/server.xml" toDir="target/tomcat/oozie-server/conf"/>
+                                <copy file="src/main/tomcat/logging.properties"
+                                      toDir="target/tomcat/oozie-server/conf"/>
                                 <copy todir="target/tomcat/oozie-server/webapps/ROOT">
-                                    <fileset dir="src/main/tomcat/ROOT" />
+                                    <fileset dir="src/main/tomcat/ROOT"/>
                                 </copy>
                             </target>
                         </configuration>
@@ -97,13 +102,13 @@
                     </execution>
                 </executions>
             </plugin>
-        <plugin>
-            <groupId>org.apache.maven.plugins</groupId>
-            <artifactId>maven-deploy-plugin</artifactId>
-            <configuration>
-                <skip>true</skip>
-            </configuration>
-        </plugin>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-deploy-plugin</artifactId>
+                <configuration>
+                    <skip>true</skip>
+                </configuration>
+            </plugin>
         </plugins>
     </build>
 
diff --git distro/src/main/bin/oozie-sys.sh distro/src/main/bin/oozie-sys.sh
index f1aec25..bfa58b2 100755
--- distro/src/main/bin/oozie-sys.sh
+++ distro/src/main/bin/oozie-sys.sh
@@ -174,6 +174,13 @@ else
   print "Using   OOZIE_HTTP_PORT:     ${OOZIE_HTTP_PORT}"
 fi
 
+if [ "${OOZIE_ADMIN_PORT}" = "" ]; then
+  export OOZIE_ADMIN_PORT=`expr $OOZIE_HTTP_PORT +  1`
+  print "Setting OOZIE_ADMIN_PORT:     ${OOZIE_ADMIN_PORT}"
+else
+  print "Using   OOZIE_ADMIN_PORT:     ${OOZIE_ADMIN_PORT}"
+fi
+
 if [ "${OOZIE_BASE_URL}" = "" ]; then
   export OOZIE_BASE_URL="http://${OOZIE_HTTP_HOSTNAME}:${OOZIE_HTTP_PORT}/oozie"
   print "Setting OOZIE_BASE_URL:      ${OOZIE_BASE_URL}"
diff --git distro/src/main/bin/oozied.sh distro/src/main/bin/oozied.sh
index 379f6c8..8249987 100644
--- distro/src/main/bin/oozied.sh
+++ distro/src/main/bin/oozied.sh
@@ -62,6 +62,7 @@ setup_catalina_opts() {
   catalina_opts="${catalina_opts} -Doozie.log4j.reload=${OOZIE_LOG4J_RELOAD}";
 
   catalina_opts="${catalina_opts} -Doozie.http.hostname=${OOZIE_HTTP_HOSTNAME}";
+  catalina_opts="${catalina_opts} -Doozie.admin.port=${OOZIE_ADMIN_PORT}";
   catalina_opts="${catalina_opts} -Doozie.http.port=${OOZIE_HTTP_PORT}";
   catalina_opts="${catalina_opts} -Doozie.base.url=${OOZIE_BASE_URL}";
 
@@ -88,6 +89,11 @@ case $actionCmd in
     $CATALINA $actionCmd "$@"
     ;;
   (stop)
+    setup_catalina_opts
+
+    # A bug in catalina.sh script does not use CATALINA_OPTS for stopping the server
+    export JAVA_OPTS=${CATALINA_OPTS}
+
     $CATALINA $actionCmd "$@"
     ;;
 esac
diff --git distro/src/main/tomcat/server.xml distro/src/main/tomcat/server.xml
index d25e16c..bdcea00 100644
--- distro/src/main/tomcat/server.xml
+++ distro/src/main/tomcat/server.xml
@@ -20,7 +20,7 @@
      define subcomponents such as "Valves" at this level.
      Documentation at /docs/config/server.html
  -->
-<Server port="8005" shutdown="SHUTDOWN">
+<Server port="${oozie.admin.port}" shutdown="SHUTDOWN">
 
   <!--APR library loader. Documentation at /docs/apr.html -->
   <Listener className="org.apache.catalina.core.AprLifecycleListener" SSLEngine="on" />
diff --git docs/pom.xml docs/pom.xml
index 5e3a21f..d9c894b 100644
--- docs/pom.xml
+++ docs/pom.xml
@@ -16,16 +16,19 @@
   See the License for the specific language governing permissions and
   limitations under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
     <modelVersion>4.0.0</modelVersion>
     <parent>
         <groupId>org.apache.oozie</groupId>
         <artifactId>oozie-main</artifactId>
         <version>3.1.3-incubating</version>
     </parent>
+    <groupId>org.apache.oozie</groupId>
     <artifactId>oozie-docs</artifactId>
-    <description>Oozie Docs</description>
-    <name>Oozie Docs</name>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie Docs</description>
+    <name>Apache Oozie Docs</name>
     <packaging>war</packaging>
 
     <dependencies>
@@ -40,6 +43,13 @@
         <plugins>
             <plugin>
                 <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-deploy-plugin</artifactId>
+                <configuration>
+                    <skip>true</skip>
+                </configuration>
+            </plugin>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
                 <artifactId>maven-assembly-plugin</artifactId>
                 <configuration>
                     <descriptors>
@@ -47,16 +57,16 @@
                     </descriptors>
                 </configuration>
             </plugin>
-                    <plugin>
-                        <groupId>org.apache.rat</groupId>
-                        <artifactId>apache-rat-plugin</artifactId>
-                        <configuration>
-                            <excludes>
-                                <exclude>src/site/twiki/*.twiki</exclude>
-                                <exclude>SecurityAuth.audit</exclude>
-                            </excludes>
-                         </configuration>
-                    </plugin>
+            <plugin>
+                <groupId>org.apache.rat</groupId>
+                <artifactId>apache-rat-plugin</artifactId>
+                <configuration>
+                    <excludes>
+                        <exclude>src/site/twiki/*.twiki</exclude>
+                        <exclude>SecurityAuth.audit</exclude>
+                    </excludes>
+                </configuration>
+            </plugin>
         </plugins>
     </build>
 
@@ -71,11 +81,11 @@
             </activation>
             <reporting>
                 <plugins>
-                     <plugin>
+                    <plugin>
                         <groupId>org.apache.maven.plugins</groupId>
                         <artifactId>maven-project-info-reports-plugin</artifactId>
-                         <!-- Using version because plugin management does not work for reporting plugins -->
-                         <version>2.2</version>
+                        <!-- Using version because plugin management does not work for reporting plugins -->
+                        <version>2.2</version>
                         <reportSets>
                             <reportSet>
                                 <reports>
diff --git docs/src/site/twiki/AG_Install.twiki docs/src/site/twiki/AG_Install.twiki
index 79a3be4..1ae0fcc 100644
--- docs/src/site/twiki/AG_Install.twiki
+++ docs/src/site/twiki/AG_Install.twiki
@@ -34,6 +34,8 @@ Default value =10=
 
 *OOZIE_HTTP_PORT* : The port Oozie server runs. Default value =11000=.
 
+*OOZIE_ADMIN_PORT* : The admin port Oozie server runs. Default value =11001=.
+
 *OOZIE_HTTP_HOSTNAME* : The host name Oozie server runs on. Default value is the output of the
 command =hostname -f=.
 
@@ -98,7 +100,7 @@ servlet container (if using Tomcat, copy the prepared =oozie.war= file to Tomcat
 
 ---++ Database Configuration
 
-Oozie works with HSQL, Derby, MySQL, Oracle and PostgreSQL databases.
+Oozie works with HSQL, Derby, MySQL, Oracle or PostgreSQL databases.
 
 By default, Oozie is configured to use Embedded Derby.
 
@@ -106,17 +108,17 @@ Oozie bundles the JDBC drivers for HSQL, Embedded Derby and PostgreSQL.
 
 HSQL is normally used for testcases as it is an in-memory database and all data is lost everytime Oozie is stopped.
 
-If using MySQL, Oracle or PostgreSQL, the Oozie database schema must be created. By default, Oozie creates its
-tables automatically.
+If using Derby, MySQL, Oracle or PostgreSQL, the Oozie database schema must be created using the =ooziedb.sh= command
+line tool.
 
-The =bin/addtowar.sh= and the =oozie-setup.sh= scripts have an option =-jars= that can be used to add the Oracle or
-MySQL JDBC driver JARs to the Oozie WAR file.
+If using MySQL or Oracle, the corresponding JDBC driver JAR file mut be copied to Oozie's =libext/= directory and
+it must be added to Oozie WAR file using the =bin/addtowar.sh= or the =oozie-setup.sh= scripts using the =-jars= option.
 
 The SQL database used by Oozie is configured using the following configuration properties (default values shown):
 
 <verbatim>
   oozie.db.schema.name=oozie
-  oozie.service.JPAService.create.db.schema=true
+  oozie.service.JPAService.create.db.schema=false
   oozie.service.JPAService.validate.db.connection=false
   oozie.service.JPAService.jdbc.driver=org.apache.derby.jdbc.EmbeddedDriver
   oozie.service.JPAService.jdbc.url=jdbc:derby:${oozie.data.dir}/${oozie.db.schema.name}-db;create=true
@@ -125,7 +127,40 @@ The SQL database used by Oozie is configured using the following configuration p
   oozie.service.JPAService.pool.max.active.conn=10
 </verbatim>
 
-If using HSQL, these following configuration properties have to be in oozie-site.xml:
+*NOTE:* If the =oozie.db.schema.create= property is set to =true= (default value is =false=) the Oozie tables
+will be created automatically without having to use the =ooziedb= command line tool. Setting this property to
+ =true= it is recommended only for development.
+
+*NOTE:* If the =oozie.db.schema.create= property is set to true, the =oozie.service.JPAService.validate.db.connection=
+property value is ignored and Oozie handles it as set to =false=.
+
+Once =oozie-site.xml= has been configured with the database configuration execute the =ooziedb.sh= command line tool to
+create the database:
+
+<verbatim>
+$ bin/ooziedb.sh create -sqlfile oozie.sql -run
+
+Validate DB Connection.
+DONE
+Check DB schema does not exist
+DONE
+Check OOZIE_SYS table does not exist
+DONE
+Create SQL schema
+DONE
+DONE
+Create OOZIE_SYS table
+DONE
+
+Oozie DB has been created for Oozie version '3.2.0'
+
+$
+</verbatim>
+
+If the '-run' option is not specified, only the SQL file with the command that would be executed will be created.
+
+If using HSQL there is no need to use the =ooziedb= command line tool as HSQL is an im-memory database. Use the
+following configuration properties in the oozie-site.xml:
 
 <verbatim>
   oozie.db.schema.name=oozie
@@ -138,13 +173,6 @@ If using HSQL, these following configuration properties have to be in oozie-site
   oozie.service.JPAService.pool.max.active.conn=10
 </verbatim>
 
-*NOTE:* If the =oozie.db.schema.create= property is set to =true= (default) the Oozie tables will be created
-automatically if they are not found in the database at Oozie start up time. In a production system this option should
-be set to =false= once the databaset tables have been created.
-
-*NOTE:* If the =oozie.db.schema.create= property is set to true, the =oozie.service.JPAService.validate.db.connection=
-property value is ignored and Oozie handles it as set to =false=.
-
 ---++ Oozie Configuration
 
 By default, Oozie configuration is read from Oozie's =conf/= directory
@@ -185,11 +213,65 @@ Oozie logs in 4 different files:
 
 The embedded Tomcat and embedded Derby log files are also written to Oozie's =logs/= directory.
 
----+++ Oozie Authentication Configuration
+---+++ Oozie User Authentication Configuration
+
+Oozie supports Kerberos HTTP SPNEGO authentication, pseudo/simple authentication and anonymous access
+for client connections.
+
+Anonymous access (*default*) does not require the user to authenticate and the user ID is obtained from
+the job properties on job submission operations, other operations are anonymous.
+
+Pseudo/simple authentication requires the user to specify the user name on the request, this is done by
+the PseudoAuthenticator class by injecting the =user.name= parameter in the query string of all requests.
+The =user.name= parameter value is taken from the client process Java System property =user.name=.
+
+Kerberos HTTP SPNEGO authentication requires the user to perform a Kerberos HTTP SPNEGO authentication sequence.
+
+If Pseudo/simple or Kerberos HTTP SPNEGO authentication mechanisms are used, Oozie will return the user an
+authentication token HTTP Cookie that can be used in later requests as identy proof.
+
+Oozie uses Apache Hadoop-Auth (Java HTTP SPENGO) library for authentication.
+This library can be extended to support other authentication mechanisms.
+
+Oozie user authentication is configured using the following configuration properties (default values shown):
+
+<verbatim>
+  oozie.authentication.type=simple
+  oozie.authentication.token.validity=36000
+  oozie.authentication.signature.secret=
+  oozie.authentication.cookie.domain=
+  oozie.authentication.simple.anonymous.allowed=true
+  oozie.authentication.kerberos.principal=HTTP/localhost@${local.realm}
+  oozie.authentication.kerberos.keytab=${oozie.service.HadoopAccessorService.keytab.file}
+</verbatim>
+
+The =type= defines authentication used for Oozie HTTP endpoint, the supported values are:
+simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#.
+
+The =token.validity= indicates how long (in seconds) an authentication token is valid before it has
+to be renewed.
+
+The =signature.secret= is the signature secret for signing the authentication tokens. If not set a random
+secret is generated at startup time.
+
+The =oozie.authentication.cookie.domain= The domain to use for the HTTP cookie that stores the
+authentication token. In order to authentiation to work correctly across all Hadoop nodes web-consoles
+the domain must be correctly set.
+
+The =simple.anonymous.allowed= indicates if anonymous requests are allowed. This setting is meaningful
+only when using 'simple' authentication.
+
+The =kerberos.principal= indicates the Kerberos principal to be used for HTTP endpoint.
+The principal MUST start with 'HTTP/' as per Kerberos HTTP SPNEGO specification.
+
+The =kerberos.keytab= indicates the location of the keytab file with the credentials for the principal.
+It should be the same keytab file Oozie uses for its Kerberos credentials for Hadoop.
+
+---+++ Oozie Hadoop Authentication Configuration
 
 Oozie can work with Hadoop 20 with Security distribution which supports Kerberos authentication.
 
-Oozie authentication is configured using the following configuration properties (default values shown):
+Oozie Hadoop authentication is configured using the following configuration properties (default values shown):
 
 <verbatim>
   oozie.service.HadoopAccessorService.kerberos.enabled=false
@@ -223,6 +305,26 @@ When using Kerberos authentication, the following properties must be set to the
 *IMPORTANT:* When using Oozie with a Hadoop 20 with Security distribution, the Oozie user in Hadoop must be configured
 as a proxy user.
 
+---+++ User ProxyUser Configuration
+
+Oozie supports impersonation or proxyuser functionality (identical to Hadoop proxyuser capabilities and conceptually similar to Unix 'sudo').
+
+Proxyuser enables other systems that are Oozie clients to submit jobs on behalf of other users.
+
+Because proxyuser is a powerful capability, Oozie provides the following restriction capabilities
+(similar to Hadoop):
+
+   * Proxyuser is an explicit configuration on per proxyuser user basis.
+   * A proxyuser user can be restricted to impersonate other users from a set of hosts.
+   * A proxyser user can be restricted to impersonate users belonging to a set of groups.
+
+There are 2 configuration properties needed to set up a proxyuser:
+
+   * oozie.service.ProxyUserService.proxyuser.#USER#.hosts: hosts from where the user #USER# can impersonate other users.
+   * oozie.service.ProxyUserService.proxyuser.#USER#.groups: groups the users being impersonated by user #USER# must belong to.
+
+Both properties support the '*' wildcard as value. Although this is recommended only for testing/development.
+
 ---+++ User Authorization Configuration
 
 Oozie has a basic authorization model:
diff --git docs/src/site/twiki/AG_Monitoring.twiki docs/src/site/twiki/AG_Monitoring.twiki
index ef23dd9..c6ff6fe 100644
--- docs/src/site/twiki/AG_Monitoring.twiki
+++ docs/src/site/twiki/AG_Monitoring.twiki
@@ -22,7 +22,7 @@ Instrumentation data includes variables, samplers, timers and counters.
       * version: Oozie build version.
 
    * configuration
-      * config.dir: directory from where the configuration files are loaded. If null, all configuration files are loaded from the classpath. [[AG_Configuration][Configuration Files]].
+      * config.dir: directory from where the configuration files are loaded. If null, all configuration files are loaded from the classpath. [[AG_Install#Oozie_Configuration][Configuration files are described here]].
       * config.file: the Oozie custom configuration for the instance.
 
    * jvm
diff --git docs/src/site/twiki/AG_OozieUpgrade.twiki docs/src/site/twiki/AG_OozieUpgrade.twiki
new file mode 100644
index 0000000..082d63b
--- /dev/null
+++ docs/src/site/twiki/AG_OozieUpgrade.twiki
@@ -0,0 +1,80 @@
+<noautolink>
+
+[[index][::Go back to Oozie Documentation Index::]]
+
+---+!! Oozie Upgrade
+
+%TOC%
+
+---+ Preparation
+
+Make sure there are not Workflows in RUNNING or SUSPENDED status, otherwise the database upgrade will fail.
+
+Shutdown Oozie and backup the Oozie database.
+
+Copy the oozie-site.xml from your current setup.
+
+---+ Oozie Server Upgrade
+
+Expand the new Oozie tarball in a new location.
+
+Edit the new oozie-site.xml setting all custom properties values from the old oozie-site.xml
+
+IMPORTANT: From Oozie 2.x to Oozie 3.x the names of the database configuration properties have
+changed. Their prefix has changed from =oozie.service.StoreService.*= to =oozie.service.JPAService.*=.
+Make sure you are using the new prefix.
+
+After upgrading the Oozie server, the =oozie-setup.sh= MUST be rerun before starting the
+upgraded Oozie server.
+
+Oozie database migration is required when there Oozie database schema changes, like
+upgrading from Oozie 2.x to Oozie 3.x.
+
+Configure the oozie-site.xml with the correct database configuration properties as
+explained in the 'Database Configuration' section  in [[AG_Install][Oozie Install]].
+
+Once =oozie-site.xml= has been configured with the database configuration execute the =ooziedb.sh=
+command line tool to upgrade the database:
+
+<verbatim>
+$ bin/ooziedb.sh upgrade -sqlfile oozie.sql -run
+
+Validate DB Connection.
+DONE
+Check DB schema exists
+DONE
+Check OOZIE_SYS table does not exist
+DONE
+Verify there are not active Workflow Jobs
+DONE
+Create SQL schema
+DONE
+DONE
+Create OOZIE_SYS table
+DONE
+Upgrade COORD_JOBS new columns default values.
+DONE
+Upgrade COORD_JOBS & COORD_ACTIONS status values.
+DONE
+
+IMPORTANT: the following manual changes may have to be done in the Oozie DB
+
+  The 'execution_path' column in the 'WF_ACTIONS' table should be modified to be a VARCHAR2(1024)
+
+
+Oozie DB has been upgraded to Oozie version '3.2.0'
+
+$
+</verbatim>
+
+The new version of the Oozie server is ready to be started.
+
+---+ Oozie Client Upgrade
+
+While older Oozie clients work with newer Oozie server, to have access to all the
+functionality of the Oozie server the same version of Oozie client should be installed
+and used by users.
+
+[[index][::Go back to Oozie Documentation Index::]]
+
+</noautolink>
diff --git docs/src/site/twiki/CoordinatorFunctionalSpec.twiki docs/src/site/twiki/CoordinatorFunctionalSpec.twiki
index 7ac3d3d..29ddbd1 100644
--- docs/src/site/twiki/CoordinatorFunctionalSpec.twiki
+++ docs/src/site/twiki/CoordinatorFunctionalSpec.twiki
@@ -12,11 +12,14 @@ The goal of this document is to define a coordinator engine system specialized i
 
 ---++ Changelog
 
+---+++!! 23/NOV/2011:
+
+   * Update execution order typo 
+
 ---+++!! 05/MAY/2011:
 
    * Update coordinator schema 0.2
 
-
 ---+++!! 09/MAR/2011:
 
    * Update coordinator status
@@ -755,7 +758,7 @@ A synchronous coordinator definition is a is defined by a name, start time and e
       * *%BLUE% execution: %ENDCOLOR%* Specifies the execution order if multiple instances of the coordinator job have satisfied their execution criteria. Valid values are: 
          * =FIFO= (oldest first) *default*.
          * =LIFO= (newest first).
-         * =ONLYLAST= (discards all older materializations).
+         * =LAST_ONLY= (discards all older materializations).
    * *%BLUE% datasets: %ENDCOLOR%* The datasets coordinator application uses.
    * *%BLUE% input-events: %ENDCOLOR%* The coordinator job input events. 
       * *%BLUE% data-in: %ENDCOLOR%* It defines one job input condition that resolves to one or more instances of a dataset. 
@@ -2021,7 +2024,7 @@ This section describes the EL functions that could be used to parameterized both
 
 The =${coord:dateOffset(String baseDate, int instance, String timeUnit)}= EL function calculates date based on the following equaltion : =newDate = baseDate + instance,  * timeUnit=
 
-For example, if baseDate is '2009-01-01T00:00Z', instance is '2' and timeUnit is 'MONTH', the return date will be' 2009-03-01T00:00Z'.
+For example, if baseDate is '2009-01-01T00:00Z', instance is '2' and timeUnit is 'MONTH', the return date will be '2009-03-01T00:00Z'. If baseDate is '2009-01-01T00:00Z', instance is '1' and timeUnit is 'YEAR', the return date will be '2010-01-01T00:00Z'.
 
 *%GREEN% Example: %ENDCOLOR%*:
 
diff --git docs/src/site/twiki/DG_CommandLineTool.twiki docs/src/site/twiki/DG_CommandLineTool.twiki
index c08e435..b6f62e8 100644
--- docs/src/site/twiki/DG_CommandLineTool.twiki
+++ docs/src/site/twiki/DG_CommandLineTool.twiki
@@ -21,9 +21,9 @@ usage:
       custom headers for Oozie web services can be specified using '-Dheader:NAME=VALUE'
 
       oozie help : display usage
-
+.
       oozie version : show client version
-
+.
       oozie job <OPTIONS> : job operations
                 -action <arg>         coordinator rerun on action ids (requires -rerun); coordinator log retrieval on action ids (requires -log)
                 -change <arg>         change a coordinator/bundle job
@@ -31,6 +31,7 @@ usage:
                 -D <property=value>   set/override value for given property
                 -date <arg>           coordinator/bundle rerun on action dates (requires -rerun)
                 -definition <arg>     job definition
+                -doas <arg>           doAs user, impersonates as the specified user
                 -dryrun               Supported in Oozie-2.0 or later versions ONLY - dryrun or test
                                       run a coordinator job, job is not queued
                 -info <arg>           info of a job
@@ -52,8 +53,9 @@ usage:
                 -value <arg>          new endtime/concurrency/pausetime value for changing a
                                       coordinator job; new pausetime value for changing a bundle job
                 -verbose              verbose mode
-
+.
       oozie jobs <OPTIONS> : jobs status
+                 -doas <arg>           doAs user, impersonates as the specified user.
                  -filter <arg>    user=<U>;name=<N>;group=<G>;status=<S>;...
                  -jobtype <arg>   job type ('Supported in Oozie-2.0 or later versions ONLY -
                                   coordinator' or 'wf' (default))
@@ -62,23 +64,25 @@ usage:
                  -offset <arg>    jobs offset (default '1')
                  -oozie <arg>     Oozie URL
                  -verbose         verbose mode
-
+.
       oozie admin <OPTIONS> : admin operations
+                -doas <arg>           doAs user, impersonates as the specified user.
                   -oozie <arg>        Oozie URL
                   -queuedump          show Oozie server queue elements
                   -status             show the current system status
                   -systemmode <arg>   Supported in Oozie-2.0 or later versions ONLY. Change oozie
                                       system mode [NORMAL|NOWEBSERVICE|SAFEMODE]
                   -version            show Oozie server build version
-
+.
       oozie validate <ARGS> : validate a workflow XML file
-
+.
       oozie sla <OPTIONS> : sla operations (Supported in Oozie-2.0 or later)
                 -len <arg>      number of results (default '100')
                 -offset <arg>   start offset (default '0')
                 -oozie <arg>    Oozie URL
-
+.
       oozie pig <OPTIONS> -X <ARGS> : submit a pig job, everything after '-X' are pass-through parameters to pig
+                -doas <arg>           doAs user, impersonates as the specified user.
                 -config <arg>         job configuration file '.properties'
                 -D <property=value>   set/override value for given property
                 -file <arg>           Pig script
@@ -87,6 +91,34 @@ usage:
 
 ---++ Common CLI Options
 
+---+++ Authentication
+
+The =oozie= CLI automatically perform authentication if the Oozie server requests it. By default it supports both
+pseudo/simple authentication and Kerberos HTTP SPNEGO authentication.
+
+For pseudo/simple authentication the =oozie= CLI uses the user name of the current OS user.
+
+For Kerberos HTTP SPNEGO authentication the =oozie= CLI uses the default principal for the OS Kerberos cache
+(normally the principal that did =kinit=).
+
+Oozie uses Apache Hadoop-Auth (Java HTTP SPENGO) library for authentication.
+This library can be extended to support other authentication mechanisms.
+
+Once authentication is performed successfully the received authentication token is cached in the user home directory
+in the =.oozie-auth-token= file with owner-only permissions. Subsequent requests reuse the cached token while valid.
+
+The use of the cache file can be disabled by invoking the =oozie= CLI with the =-Doozie.auth.token.cache=false=
+option.
+
+To use an custom authentication mechanism, a Hadoop-Auth =Authenticator= implementation must be specified with the
+ =-Dauthenticator.class= = =CLASS= option.
+
+---+++ Impersonation, doAs
+
+The <code>-doas</code> option allows the current user to impersonate other users when interacting with the Oozie
+system. The current user must be configured as a proxyuser in the Oozie system. The proxyuser configuration may
+restrict from which hosts a user may impersonate users, as well as users of which groups can be impersonated.
+
 ---+++ Oozie URL
 
 All =oozie= CLI sub-commands expect the <code>-oozie OOZIE_URL</code> option indicating the URL of the Oozie system
@@ -244,7 +276,7 @@ Valid value names are:
 
 Repeated value names are not allowed. An empty string "" can be used to reset pause time to none.
 
-After the command is executed the job's pause time should be changed. 
+After the command is executed the job's pause time should be changed.
 
 ---+++ Rerunning a Workflow Job
 
@@ -328,8 +360,9 @@ hadoop1                 map-reduce  OK         end         job_200904281535_0254
 </verbatim>
 
 The =info= option can display information about a workflow job or coordinator job or coordinator action.
+The =info= option for a Coordinator job will retrieve the Coordinator actions ordered by nominal time. However, the =info= command may timeout if the number of Coordinator actions are very high. In that case, =info= should be used with =offset= and =len= option.
 
-The =offset= and =len= option specified the offset and number of actions to display, if checking a workflow job or coordinator job.
+The =offset= and =len= option specifies the offset and number of actions to display, if checking a workflow job or coordinator job.
 
 The =localtime= option displays times in local time, if not specified times are displayed in GMT.
 
@@ -459,9 +492,11 @@ Valid filter names are:
    * user: the user that submitted the job.
    * group: the group for the job.
    * status: the status of the job.
+   * frequency: the frequency of the Coordinator job.
+   * unit: the time unit. It can take one of the following four values: months, days, hours or minutes. Time unit should be added only when frequency is specified.
 
 The query will do an AND among all the filter names. The query will do an OR among all the filter values for the same
-name. Multiple values must be specified as different  name value pairs.
+name. Multiple values must be specified as different name value pairs.
 
 ---+++ Checking the Status of multiple Coordinator Jobs
 
@@ -496,7 +531,7 @@ Job ID                                   Bundle Name    Status    Kickoff
 ------------------------------------------------------------------------------------------------------------------------------------
 0000001-110322105610515-oozie-chao-B     BUNDLE-TEST    RUNNING   2012-01-15 00:24    2011-03-22 18:06    joe        users
 ------------------------------------------------------------------------------------------------------------------------------------
-0000000-110322105610515-oozie-chao-B     BUNDLE-TEST    DONEWITHERROR2012-01-15 00:24    2011-03-22 17:58    joe        users       
+0000000-110322105610515-oozie-chao-B     BUNDLE-TEST    DONEWITHERROR2012-01-15 00:24    2011-03-22 17:58    joe        users
 ------------------------------------------------------------------------------------------------------------------------------------
 </verbatim>
 
diff --git docs/src/site/twiki/DG_CoordinatorRerun.twiki docs/src/site/twiki/DG_CoordinatorRerun.twiki
index f46410d..cd593d1 100644
--- docs/src/site/twiki/DG_CoordinatorRerun.twiki
+++ docs/src/site/twiki/DG_CoordinatorRerun.twiki
@@ -43,7 +43,7 @@ $oozie job -rerun <coord_Job_id> [-nocleanup] [-refresh]
    * Rerun for job, user should use job's start date and end date in -date.
    * If the user specifies a date range (say Jan 1 to May 1), the actions that will be re-run are the existing actions
      within that range.  If the existing actions are action #5....#40, which map to Jan 15 to Feb 15, then only those actions will run.
-   * When rerun succeeds, the rerun action_id and nominal_time will be return.
+   * The rerun action_id and nominal_time of the actions which are eligible to rerun will be returned.
 
 [[index][::Go back to Oozie Documentation Index::]]
 
diff --git docs/src/site/twiki/DG_Examples.twiki docs/src/site/twiki/DG_Examples.twiki
index 0c4357d..827ff86 100644
--- docs/src/site/twiki/DG_Examples.twiki
+++ docs/src/site/twiki/DG_Examples.twiki
@@ -25,6 +25,8 @@ copied.
 
 ---+++ Running the Examples
 
+For the Streaming and Pig example, the [[DG_QuickStart#OozieShareLib][Oozie Share Library]] must be installed in HDFS.
+
 Add Oozie =bin/= to the environment PATH.
 
 The examples assume the JobTracker is =localhost:9001= and the NameNode is =hdfs://localhost:9000=. If the actual
@@ -82,17 +84,6 @@ $
 $ oozie job -info 14-20090525161321-oozie-tucu
 </verbatim>
 
----++ Examples Share Lib
-
-The streaming, pig, demo and custom-main examples pick up the streaming and pig JARs from an example share lib
-bundled with the examples =examples/apps/examples-lib/= .
-
-The setting for the share lib is done in the job properties using the =oozie.libpath= property.
-
-<verbatim>
-oozie.libpath=/user/${user.name}/examples/apps/examples-lib
-</verbatim>
-
 ---++ Java API Example
 
 Oozie provides a =[[./apidocs/org/org/apache/oozie/client/package-summary.html][Java Client API]] that simplifies
diff --git docs/src/site/twiki/DG_HiveActionExtension.twiki docs/src/site/twiki/DG_HiveActionExtension.twiki
new file mode 100644
index 0000000..10b7e2e
--- /dev/null
+++ docs/src/site/twiki/DG_HiveActionExtension.twiki
@@ -0,0 +1,221 @@
+<noautolink>
+
+[[index][::Go back to Oozie Documentation Index::]]
+
+-----
+
+---+!! Oozie Hive Action Extension
+
+%TOC%
+
+#HiveAction
+---++ Hive Action
+
+The =hive= action runs a Hive job.
+
+The workflow job will wait until the Hive job completes before
+continuing to the next action.
+
+To run the Hive job, you have to configure the =hive= action with the
+=job-tracker=, =name-node= and Hive =script= elements as
+well as the necessary parameters and configuration.
+
+A =hive= action can be configured to create or delete HDFS directories
+before starting the Hive job.
+
+Hive configuration can be specified with a file, using the =job-xml=
+element, and inline, using the =configuration= elements.
+
+Oozie EL expressions can be used in the inline configuration. Property
+values specified in the =configuration= element override values specified
+in the =job-xml= file.
+
+Note that Hadoop =mapred.job.tracker= and =fs.default.name= properties
+must not be present in the inline configuration.
+
+As with Hadoop =map-reduce= jobs, it is possible to add files and
+archives in order to make them available to the Hive job. Refer to the
+[WorkflowFunctionalSpec#FilesAchives][Adding Files and Archives for the Job]
+section for more information about this feature.
+
+Oozie Hive action supports Hive scripts with parameter variables, their
+syntax is =${VARIABLES}=.
+
+*Syntax:*
+
+<verbatim>
+<workflow-app name="[WF-DEF-NAME]" xmlns="uri:oozie:workflow:0.1">
+    ...
+    <action name="[NODE-NAME]">
+        <hive xmlns="uri:oozie:hive-action:0.2">
+            <job-tracker>[JOB-TRACKER]</job-tracker>
+            <name-node>[NAME-NODE]</name-node>
+            <prepare>
+               <delete path="[PATH]"/>
+               ...
+               <mkdir path="[PATH]"/>
+               ...
+            </prepare>
+            <job-xml>[HIVE SETTINGS FILE]</job-xml>
+            <configuration>
+                <property>
+                    <name>[PROPERTY-NAME]</name>
+                    <value>[PROPERTY-VALUE]</value>
+                </property>
+                ...
+            </configuration>
+            <script>[HIVE-SCRIPT]</script>
+            <param>[PARAM-VALUE]</param>
+                ...
+            <param>[PARAM-VALUE]</param>
+            <file>[FILE-PATH]</file>
+            ...
+            <archive>[FILE-PATH]</archive>
+            ...
+        </hive>
+        <ok to="[NODE-NAME]"/>
+        <error to="[NODE-NAME]"/>
+    </action>
+    ...
+</workflow-app>
+</verbatim>
+
+The =prepare= element, if present, indicates a list of paths to delete
+or create before starting the job. Specified paths must start with =hdfs://HOST:PORT=.
+
+The =job-xml= element, if present, specifies a file containing configuration
+for the Hive job.
+
+The =configuration= element, if present, contains configuration
+properties that are passed to the Hive job.
+
+The =script= element must contain the path of the Hive script to
+execute. The Hive script can be templatized with variables of the form
+=${VARIABLE}=. The values of these variables can then be specified
+using the =params= element.
+
+The =params= element, if present, contains parameters to be passed to
+the Hive script.
+
+All the above elements can be parameterized (templatized) using EL
+expressions.
+
+*Example:*
+
+<verbatim>
+<workflow-app name="sample-wf" xmlns="uri:oozie:workflow:0.1">
+    ...
+    <action name="myfirsthivejob">
+        <hive xmlns="uri:oozie:hive-action:0.2">
+            <job-traker>foo:9001</job-tracker>
+            <name-node>bar:9000</name-node>
+            <prepare>
+                <delete path="${jobOutput}"/>
+            </prepare>
+            <configuration>
+                <property>
+                    <name>mapred.compress.map.output</name>
+                    <value>true</value>
+                </property>
+                <property>
+                    <name>oozie.hive.defaults</name>
+                    <value>/usr/foo/hive-0.6-default.xml</value>
+                </property>
+            </configuration>
+            <script>myscript.q</script>
+            <param>InputDir=/home/tucu/input-data</param>
+            <param>OutputDir=${jobOutput}</param>
+        </hive>
+        <ok to="myotherjob"/>
+        <error to="errorcleanup"/>
+    </action>
+    ...
+</workflow-app>
+</verbatim>
+
+---+++ Hive Default and Site Configuration Files
+
+All the properties defined in the =job-xml= and inline in the =configuration=
+element become the =hive-site.xml= that Hive will use.
+
+Hive (as of Hive 0.6) does not yet include a =hive-default.xml= file, it is the responsibility
+of the user to provide one. When using Oozie Hive action, the =hive-default.xml= file must be
+copied to HDFS and in the Hive action =configuration= section it must be set in the =oozie.hive.defaults=
+property. If a relative path is given, the path will be resolved within the workflow application
+directory.
+
+*NOTE:* When Hive starts bundling a =hive-default.xml= file within its JARs, Oozie will ignore
+the =hive-default.xml= file specified in the Hive action configuration.
+
+If a =hive-site.xml= file is not specified (or available in Hive JARs), the Oozie Hive action will fail.
+
+---+++ Hive Action Logging
+
+Hive action logs are redirected to the Oozie Launcher map-reduce job task STDOUT/STDERR that runs Hive.
+
+From Oozie web-console, from the Hive action pop up using the 'Console URL' link, it is possible
+to navigate to the Oozie Launcher map-reduce job task logs via the Hadoop job-tracker web-console.
+
+The logging level of the Hive action can set in the Hive action configuration using the
+property =oozie.hive.log.level=. The default value is =INFO=.
+
+---++ Appendix, Hive XML-Schema
+
+---+++ AE.A Appendix A, Hive XML-Schema
+
+<verbatim>
+<xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema"
+           xmlns:hive="uri:oozie:hive-action:0.2" elementFormDefault="qualified"
+           targetNamespace="uri:oozie:hive-action:0.2">
+.
+    <xs:element name="hive" type="hive:ACTION"/>
+.
+    <xs:complexType name="ACTION">
+        <xs:sequence>
+            <xs:element name="job-tracker" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="name-node" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="prepare" type="hive:PREPARE" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="job-xml" type="xs:string" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="configuration" type="hive:CONFIGURATION" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="script" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="param" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="file" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="archive" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+        </xs:sequence>
+    </xs:complexType>
+.
+    <xs:complexType name="CONFIGURATION">
+        <xs:sequence>
+            <xs:element name="property" minOccurs="1" maxOccurs="unbounded">
+                <xs:complexType>
+                    <xs:sequence>
+                        <xs:element name="name" minOccurs="1" maxOccurs="1" type="xs:string"/>
+                        <xs:element name="value" minOccurs="1" maxOccurs="1" type="xs:string"/>
+                        <xs:element name="description" minOccurs="0" maxOccurs="1" type="xs:string"/>
+                    </xs:sequence>
+                </xs:complexType>
+            </xs:element>
+        </xs:sequence>
+    </xs:complexType>
+.
+    <xs:complexType name="PREPARE">
+        <xs:sequence>
+            <xs:element name="delete" type="hive:DELETE" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="mkdir" type="hive:MKDIR" minOccurs="0" maxOccurs="unbounded"/>
+        </xs:sequence>
+    </xs:complexType>
+.
+    <xs:complexType name="DELETE">
+        <xs:attribute name="path" type="xs:string" use="required"/>
+    </xs:complexType>
+.
+    <xs:complexType name="MKDIR">
+        <xs:attribute name="path" type="xs:string" use="required"/>
+    </xs:complexType>
+.
+</xs:schema>
+</verbatim>
+
+[[index][::Go back to Oozie Documentation Index::]]
+
+</noautolink>
diff --git docs/src/site/twiki/DG_QuickStart.twiki docs/src/site/twiki/DG_QuickStart.twiki
index e2a9bd4..2787326 100644
--- docs/src/site/twiki/DG_QuickStart.twiki
+++ docs/src/site/twiki/DG_QuickStart.twiki
@@ -13,7 +13,7 @@ For detailed install and configuration instructions refer to [[AG_Install][Oozie
    * Unix (tested in Linux and Mac OS X)
    * Java 1.6+
    * Hadoop
-      * [[http://hadoop.apache.org][Apache Hadoop]] (tested with 0.20.205. But compatible with Hadoop 0.20.x and 1.0.0)
+      * [[http://hadoop.apache.org][Apache Hadoop]] (tested with 1.0.0 & 0.23.1)
    * ExtJS library (optional, to enable Oozie webconsole)
       * [[http://extjs.com/deploy/ext-2.2.zip][ExtJS 2.2]]
 
@@ -35,22 +35,53 @@ Expand the Oozie distribution =tar.gz=.
 
 Expand the Hadoop distribution =tar.gz= (as the Oozie Unix user).
 
-Oozie is bundled without Hadoop JAR files and without the ExtJS library.
+Expand the Oozie hadooplibs =tar.gz= in the same location Oozie distribution =tar.gz= was expanded. A
+*hadooplibs/* directory will be created containing the Hadoop JARs for the versions of Hadoop that
+the Oozie distribution supports.
 
-   * The Hadoop JARs are required to run Oozie
-   * The ExtJS library is optional (only required for the Oozie web-console to work)
+The ExtJS library is optional (only required for the Oozie web-console to work)
 
 *IMPORTANT:* all Oozie server scripts (=oozie-setup.sh=, =oozie-start.sh=, =oozie-run.sh=
 and =oozie-stop.sh=) run only under the Unix user that owns the Oozie installation directory,
 if necessary use =sudo -u OOZIE_USER= when invoking the scripts.
 
-Use the =oozie-setup.sh= script to add the Hadoop JARs and the ExtJS library to Oozie.
+Create a *libext/* directory in the directory where Oozie was expanded.
+
+If using a version of Hadoop bundled in Oozie *hadooplibs/*, copy the corresponding Hadoop JARs
+from *hadooplibs/* to the *libext/* directory. If using a different version of Hadoop, copy the
+required Hadoop JARs from such version in the *libext/* directory.
+
+If using the ExtJS library copy the ZIP file to the *libext/* directory.
+
+Run the =oozie-setup.sh= script to configure Oozie with all the components added to the *libext/* directory.
+
+<verbatim>
+$ bin/oozie-setup.sh
+</verbatim>
+
+Create the Oozie DB using the 'ooziedb.sh' command line tool:
 
 <verbatim>
-$ bin/oozie-setup.sh -hadoop 0.20.200 ${HADOOP_HOME} -extjs /tmp/ext-2.2.zip
+$ bin/ooziedb.sh create -sqlfile oozie.sql -run
+
+Validate DB Connection.
+DONE
+Check DB schema does not exist
+DONE
+Check OOZIE_SYS table does not exist
+DONE
+Create SQL schema
+DONE
+DONE
+Create OOZIE_SYS table
+DONE
+
+Oozie DB has been created for Oozie version '3.2.0'
+
+$
 </verbatim>
 
-To start Oozie as a daemon process run:
+Start Oozie as a daemon process run:
 
 <verbatim>
 $ bin/oozie-start.sh
@@ -84,6 +115,7 @@ command line tool.
 NOTE: The Oozie server installation includes the Oozie client. The Oozie client should be installed in remote machines
 only.
 
+#OozieShareLib
 ---++ Oozie Share Lib Installation
 
 Expand the =oozie-sharelib= TAR.GZ file bundled with the distribution.
diff --git docs/src/site/twiki/DG_ShellActionExtension.twiki docs/src/site/twiki/DG_ShellActionExtension.twiki
new file mode 100644
index 0000000..4b2181e
--- /dev/null
+++ docs/src/site/twiki/DG_ShellActionExtension.twiki
@@ -0,0 +1,317 @@
+<<noautolink>
+
+[[index][::Go back to Oozie Documentation Index::]]
+
+-----
+
+---+!! Oozie Shell Action Extension
+
+%TOC%
+
+#ShellAction
+---++ Shell Action
+
+The =shell= action runs a Shell command.
+
+The workflow job will wait until the Shell command completes before
+continuing to the next action.
+
+To run the Shell job, you have to configure the =shell= action with the
+=job-tracker=, =name-node= and Shell =exec= elements as
+well as the necessary arguments and configuration.
+
+A =shell= action can be configured to create or delete HDFS directories
+before starting the Shell job.
+
+Shell _launcher_ configuration can be specified with a file, using the =job-xml=
+element, and inline, using the =configuration= elements.
+
+Oozie EL expressions can be used in the inline configuration. Property
+values specified in the =configuration= element override values specified
+in the =job-xml= file.
+
+Note that Hadoop =mapred.job.tracker= and =fs.default.name= properties
+must not be present in the inline configuration.
+
+As with Hadoop =map-reduce= jobs, it is possible to add files and
+archives in order to make them available to the Shell job. Refer to the
+[WorkflowFunctionalSpec#FilesAchives][Adding Files and Archives for the Job]
+section for more information about this feature.
+
+The output (STDOUT) of the Shell job can be made available to the workflow job after the Shell job ends. This information
+could be used from within decision nodes. If the output of the Shell job is made available to the workflow job the shell
+command must follow the following requirements:
+
+   * The format of the output must be a valid Java Properties file.
+   * The size of the output must not exceed 2KB.
+
+*Syntax:*
+
+<verbatim>
+<workflow-app name="[WF-DEF-NAME]" xmlns="uri:oozie:workflow:0.3">
+    ...
+    <action name="[NODE-NAME]">
+        <shell xmlns="uri:oozie:shell-action:0.1">
+            <job-tracker>[JOB-TRACKER]</job-tracker>
+            <name-node>[NAME-NODE]</name-node>
+            <prepare>
+               <delete path="[PATH]"/>
+               ...
+               <mkdir path="[PATH]"/>
+               ...
+            </prepare>
+            <job-xml>[SHELL SETTINGS FILE]</job-xml>
+            <configuration>
+                <property>
+                    <name>[PROPERTY-NAME]</name>
+                    <value>[PROPERTY-VALUE]</value>
+                </property>
+                ...
+            </configuration>
+            <exec>[SHELL-COMMAND]</exec>
+            <argument>[ARG-VALUE]</argument>
+                ...
+            <argument>[ARG-VALUE]</argument>
+            <env-var>[VAR1=VALUE1]</env-var>
+               ...
+            <env-var>[VARN=VALUEN]</env-var>
+            <file>[FILE-PATH]</file>
+            ...
+            <archive>[FILE-PATH]</archive>
+            ...
+            <capture-output/>
+        </shell>
+        <ok to="[NODE-NAME]"/>
+        <error to="[NODE-NAME]"/>
+    </action>
+    ...
+</workflow-app>
+</verbatim>
+
+The =prepare= element, if present, indicates a list of paths to delete
+or create before starting the job. Specified paths must start with =hdfs://HOST:PORT=.
+
+The =job-xml= element, if present, specifies a file containing configuration
+for the Shell job.
+
+The =configuration= element, if present, contains configuration
+properties that are passed to the Shell job.
+
+The =exec= element must contain the path of the Shell command to
+execute. The arguments of Shell command can then be specified
+using one or more =argument= element.
+
+The =argument= element, if present, contains argument to be passed to
+the Shell command.
+
+The =env-var= element, if present, contains the environemnt to be passed
+to the Shell command. =env-var= should contain only one pair of environment variable
+and value. If the pair contains the variable such as $PATH, it should follow the
+Unix convention such as PATH=$PATH:mypath. Don't use ${PATH} which will be
+substitued by Oozie's EL evaluator.
+
+A =shell= action creates a Hadoop configuration. The Hadoop configuration is made available as a local file to the
+Shell application in its running directory. The exact file path is exposed to the spawned shell using the environment
+variable called =OOZIE_ACTION_CONF_XML=.The Shell application can access the environemnt variable to read the action
+configuration XML file path.
+ 
+If the =capture-output= element is present, it indicates Oozie to capture output of the STDOUT of the shell command
+execution. The Shell command output must be in Java Properties file format and it must not exceed 2KB. From within the
+workflow definition, the output of an Shell action node is accessible via the =String action:output(String node,
+String key)= function (Refer to section '4.2.6 Action EL Functions').
+
+All the above elements can be parameterized (templatized) using EL
+expressions.
+
+*Example:*
+
+How to run any shell script or perl script or CPP executable
+
+<verbatim>
+<workflow-app xmlns='uri:oozie:workflow:0.3' name='shell-wf'>
+    <start to='shell1' />
+    <action name='shell1'>
+        <shell xmlns="uri:oozie:shell-action:0.1">
+            <job-tracker>${jobTracker}</job-tracker>
+            <name-node>${nameNode}</name-node>
+            <configuration>
+                <property>
+                  <name>mapred.job.queue.name</name>
+                  <value>${queueName}</value>
+                </property>
+            </configuration>
+            <exec>${EXEC}</exec>
+            <argument>A</argument>
+            <argument>B</argument>
+            <file>${EXEC}#${EXEC}</file> <!--Copy the executable to compute node's current working directory -->
+        </shell>
+        <ok to="end" />
+        <error to="fail" />
+    </action>
+    <kill name="fail">
+        <message>Script failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
+    </kill>
+    <end name='end' />
+</workflow-app>
+</verbatim>
+
+The corresponding job properties file used to submit Oozie job could be as follows:
+
+<verbatim>
+oozie.wf.application.path=hdfs://localhost:9000/user/kamrul/workflows/script
+
+#Execute is expected to be in the Workflow directory.
+#Shell Script to run
+EXEC=script.sh
+#CPP executable. Executable should be binary compatible to the compute node OS.
+#EXEC=hello
+#Perl script
+#EXEC=script.pl
+
+jobTracker=localhost:9001
+nameNode=hdfs://localhost:9000
+queueName=default
+
+</verbatim>
+
+How to run any java program bundles in a jar.
+
+<verbatim>
+<workflow-app xmlns='uri:oozie:workflow:0.3' name='shell-wf'>
+    <start to='shell1' />
+    <action name='shell1'>
+        <shell xmlns="uri:oozie:shell-action:0.1">
+            <job-tracker>${jobTracker}</job-tracker>
+            <name-node>${nameNode}</name-node>
+            <configuration>
+                <property>
+                  <name>mapred.job.queue.name</name>
+                  <value>${queueName}</value>
+                </property>
+            </configuration>
+            <exec>java</exec>
+            <argument>-classpath</argument>
+            <argument>./${EXEC}:$CLASSPATH</argument>
+            <argument>Hello</argument>
+            <file>${EXEC}#${EXEC}</file> <!--Copy the jar to compute node current working directory -->
+        </shell>
+        <ok to="end" />
+        <error to="fail" />
+    </action>
+    <kill name="fail">
+        <message>Script failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
+    </kill>
+    <end name='end' />
+</workflow-app>
+</verbatim>
+
+The corresponding job properties file used to submit Oozie job could be as follows:
+
+<verbatim>
+oozie.wf.application.path=hdfs://localhost:9000/user/kamrul/workflows/script
+
+#Hello.jar file is expected to be in the Workflow directory.
+EXEC=Hello.jar
+
+jobTracker=localhost:9001
+nameNode=hdfs://localhost:9000
+queueName=default
+</verbatim>
+
+---+++ Shell Action Logging
+
+Shell action's stdout and stderr output are redirected to the Oozie Launcher map-reduce job task STDOUT that runs the shell command.
+
+From Oozie web-console, from the Shell action pop up using the 'Console URL' link, it is possible
+to navigate to the Oozie Launcher map-reduce job task logs via the Hadoop job-tracker web-console.
+
+---+++ Shell Action Limitations
+Although Shell action can execute any shell command, there are some limitations.
+   * No interactive command is supported.
+   * Command can't be executed as different user using sudo.
+   * User has to explicitly upload the required 3rd party packages (such as jar, so lib, executable etc). Oozie provides a way using <file> and <archive> tag through Hadoop's Distributed Cache to upload.
+   * Since Oozie will execute the shell command into a Hadoop compute node, the default installation of utility in the compute node might not be fixed. However, the most common unix utilities are usually installed on all compute nodes. It is important to note that Oozie could only support the commands that are installed into the compute nodes or that are uploaded through Distributed Cache.
+
+---++ Appendix, Shell XML-Schema
+
+---+++ AE.A Appendix A, Shell XML-Schema
+
+<verbatim>
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+<xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema"
+           xmlns:shell="uri:oozie:shell-action:0.1" elementFormDefault="qualified"
+           targetNamespace="uri:oozie:shell-action:0.1">
+
+    <xs:element name="shell" type="shell:ACTION"/>
+
+    <xs:complexType name="ACTION">
+      <xs:sequence>
+            <xs:element name="job-tracker" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="name-node" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="prepare" type="shell:PREPARE" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="job-xml" type="xs:string" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="configuration" type="shell:CONFIGURATION" minOccurs="0" maxOccurs="1"/>
+             <xs:element name="exec" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="argument" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="env-var" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="file" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="archive" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="capture-output" type="shell:FLAG" minOccurs="0" maxOccurs="1"/>
+        </xs:sequence>
+    </xs:complexType>
+
+    <xs:complexType name="FLAG"/>
+
+    <xs:complexType name="CONFIGURATION">
+        <xs:sequence>
+            <xs:element name="property" minOccurs="1" maxOccurs="unbounded">
+                <xs:complexType>
+                    <xs:sequence>
+                        <xs:element name="name" minOccurs="1" maxOccurs="1" type="xs:string"/>
+                        <xs:element name="value" minOccurs="1" maxOccurs="1" type="xs:string"/>
+                        <xs:element name="description" minOccurs="0" maxOccurs="1" type="xs:string"/>
+                    </xs:sequence>
+                </xs:complexType>
+            </xs:element>
+        </xs:sequence>
+    </xs:complexType>
+
+    <xs:complexType name="PREPARE">
+        <xs:sequence>
+            <xs:element name="delete" type="shell:DELETE" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="mkdir" type="shell:MKDIR" minOccurs="0" maxOccurs="unbounded"/>
+        </xs:sequence>
+    </xs:complexType>
+
+    <xs:complexType name="DELETE">
+        <xs:attribute name="path" type="xs:string" use="required"/>
+    </xs:complexType>
+
+    <xs:complexType name="MKDIR">
+        <xs:attribute name="path" type="xs:string" use="required"/>
+    </xs:complexType>
+
+</xs:schema>
+</verbatim>
+
+[[index][::Go back to Oozie Documentation Index::]]
+
+</noautolink>
diff --git docs/src/site/twiki/DG_SqoopActionExtension.twiki docs/src/site/twiki/DG_SqoopActionExtension.twiki
new file mode 100644
index 0000000..0156570
--- /dev/null
+++ docs/src/site/twiki/DG_SqoopActionExtension.twiki
@@ -0,0 +1,243 @@
+<noautolink>
+
+[[index][::Go back to Oozie Documentation Index::]]
+
+-----
+
+---+!! Oozie Sqoop Action Extension
+
+%TOC%
+
+---++ Sqoop Action
+
+*IMPORTANT:* The Sqoop action requires Apache Hadoop 0.23.
+
+The =sqoop= action runs a Sqoop job.
+
+The workflow job will wait until the Sqoop job completes before
+continuing to the next action.
+
+To run the Sqoop job, you have to configure the =sqoop= action with the
+=job-tracker=, =name-node= and Sqoop =command= or =arg= elements as
+well as configuration.
+
+A =sqoop= action can be configured to create or delete HDFS directories
+before starting the Sqoop job.
+
+Sqoop configuration can be specified with a file, using the =job-xml=
+element, and inline, using the =configuration= elements.
+
+Oozie EL expressions can be used in the inline configuration. Property
+values specified in the =configuration= element override values specified
+in the =job-xml= file.
+
+Note that Hadoop =mapred.job.tracker= and =fs.default.name= properties
+must not be present in the inline configuration.
+
+As with Hadoop =map-reduce= jobs, it is possible to add files and
+archives in order to make them available to the Sqoop job. Refer to the
+[WorkflowFunctionalSpec#FilesAchives][Adding Files and Archives for the Job]
+section for more information about this feature.
+
+*Syntax:*
+
+<verbatim>
+<workflow-app name="[WF-DEF-NAME]" xmlns="uri:oozie:workflow:0.1">
+    ...
+    <action name="[NODE-NAME]">
+        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
+            <job-tracker>[JOB-TRACKER]</job-tracker>
+            <name-node>[NAME-NODE]</name-node>
+            <prepare>
+               <delete path="[PATH]"/>
+               ...
+               <mkdir path="[PATH]"/>
+               ...
+            </prepare>
+            <configuration>
+                <property>
+                    <name>[PROPERTY-NAME]</name>
+                    <value>[PROPERTY-VALUE]</value>
+                </property>
+                ...
+            </configuration>
+            <command>[SQOOP-COMMAND]</command>
+            <arg>[SQOOP-ARGUMENT]</arg>
+            ...
+            <file>[FILE-PATH]</file>
+            ...
+            <archive>[FILE-PATH]</archive>
+            ...
+        </sqoop>
+        <ok to="[NODE-NAME]"/>
+        <error to="[NODE-NAME]"/>
+    </action>
+    ...
+</workflow-app>
+</verbatim>
+
+The =prepare= element, if present, indicates a list of paths to delete
+or create before starting the job. Specified paths must start with =hdfs://HOST:PORT=.
+
+The =job-xml= element, if present, specifies a file containing configuration
+for the Sqoop job.
+
+The =configuration= element, if present, contains configuration
+properties that are passed to the Sqoop job.
+
+*Sqoop command*
+
+The Sqoop command can be specified either using the =command= element or multiple =arg=
+elements.
+
+When using the =command= element, Oozie will split the command on every space
+into multiple arguments.
+
+When using the =arg= elements, Oozie will pass each argument value as an argument to Sqoop.
+
+The =arg= variant should be used when there are spaces within a single argument.
+
+Consult the Sqoop documentation for a complete list of valid Sqoop commands.
+
+All the above elements can be parameterized (templatized) using EL
+expressions.
+
+*Examples:*
+
+Using the =command= element:
+
+<verbatim>
+<workflow-app name="sample-wf" xmlns="uri:oozie:workflow:0.1">
+    ...
+    <action name="myfirsthivejob">
+        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
+            <job-traker>foo:9001</job-tracker>
+            <name-node>bar:9000</name-node>
+            <prepare>
+                <delete path="${jobOutput}"/>
+            </prepare>
+            <configuration>
+                <property>
+                    <name>mapred.compress.map.output</name>
+                    <value>true</value>
+                </property>
+            </configuration>
+            <command>import  --connect jdbc:hsqldb:file:db.hsqldb --table TT --target-dir hdfs://localhost:9000/user/tucu/foo -m 1</command>
+        </sqoop>
+        <ok to="myotherjob"/>
+        <error to="errorcleanup"/>
+    </action>
+    ...
+</workflow-app>
+</verbatim>
+
+The same Sqoop action using =arg= elements:
+
+<verbatim>
+<workflow-app name="sample-wf" xmlns="uri:oozie:workflow:0.1">
+    ...
+    <action name="myfirsthivejob">
+        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
+            <job-traker>foo:9001</job-tracker>
+            <name-node>bar:9000</name-node>
+            <prepare>
+                <delete path="${jobOutput}"/>
+            </prepare>
+            <configuration>
+                <property>
+                    <name>mapred.compress.map.output</name>
+                    <value>true</value>
+                </property>
+            </configuration>
+            <arg>import</arg>
+            <arg>--connect</arg>
+            <arg>jdbc:hsqldb:file:db.hsqldb</arg>
+            <arg>--table</arg>
+            <arg>TT</arg>
+            <arg>--target-dir</arg>
+            <arg>hdfs://localhost:9000/user/tucu/foo</arg>
+            <arg>-m</arg>
+            <arg>1</arg>
+        </sqoop>
+        <ok to="myotherjob"/>
+        <error to="errorcleanup"/>
+    </action>
+    ...
+</workflow-app>
+</verbatim>
+
+NOTE: The =arg= elements syntax, while more verbose, allows to have spaces in a single argument, something useful when
+using free from queries.
+
+---+++ Sqoop Action Logging
+
+Sqoop action logs are redirected to the Oozie Launcher map-reduce job task STDOUT/STDERR that runs Sqoop.
+
+From Oozie web-console, from the Sqoop action pop up using the 'Console URL' link, it is possible
+to navigate to the Oozie Launcher map-reduce job task logs via the Hadoop job-tracker web-console.
+
+The logging level of the Sqoop action can set in the Sqoop action configuration using the
+property =oozie.sqoop.log.level=. The default value is =INFO=.
+
+---++ Appendix, Sqoop XML-Schema
+
+---+++ AE.A Appendix A, Sqoop XML-Schema
+
+<verbatim>
+<xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema"
+           xmlns:sqoop="uri:oozie:sqoop-action:0.2" elementFormDefault="qualified"
+           targetNamespace="uri:oozie:sqoop-action:0.2">
+
+    <xs:element name="sqoop" type="sqoop:ACTION"/>
+.
+    <xs:complexType name="ACTION">
+        <xs:sequence>
+            <xs:element name="job-tracker" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="name-node" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="prepare" type="sqoop:PREPARE" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="job-xml" type="xs:string" minOccurs="0" maxOccurs="1"/>
+            <xs:element name="configuration" type="sqoop:CONFIGURATION" minOccurs="0" maxOccurs="1"/>
+            <xs:choice>
+                <xs:element name="command" type="xs:string" minOccurs="1" maxOccurs="1"/>
+                <xs:element name="arg" type="xs:string" minOccurs="1" maxOccurs="unbounded"/>
+            </xs:choice>
+            <xs:element name="file" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="archive" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+        </xs:sequence>
+    </xs:complexType>
+.
+    <xs:complexType name="CONFIGURATION">
+        <xs:sequence>
+            <xs:element name="property" minOccurs="1" maxOccurs="unbounded">
+                <xs:complexType>
+                    <xs:sequence>
+                        <xs:element name="name" minOccurs="1" maxOccurs="1" type="xs:string"/>
+                        <xs:element name="value" minOccurs="1" maxOccurs="1" type="xs:string"/>
+                        <xs:element name="description" minOccurs="0" maxOccurs="1" type="xs:string"/>
+                    </xs:sequence>
+                </xs:complexType>
+            </xs:element>
+        </xs:sequence>
+    </xs:complexType>
+.
+    <xs:complexType name="PREPARE">
+        <xs:sequence>
+            <xs:element name="delete" type="sqoop:DELETE" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="mkdir" type="sqoop:MKDIR" minOccurs="0" maxOccurs="unbounded"/>
+        </xs:sequence>
+    </xs:complexType>
+.
+    <xs:complexType name="DELETE">
+        <xs:attribute name="path" type="xs:string" use="required"/>
+    </xs:complexType>
+.
+    <xs:complexType name="MKDIR">
+        <xs:attribute name="path" type="xs:string" use="required"/>
+    </xs:complexType>
+.
+</xs:schema>
+</verbatim>
+
+[[index][::Go back to Oozie Documentation Index::]]
+
+</noautolink>
diff --git docs/src/site/twiki/DG_SshActionExtension.twiki docs/src/site/twiki/DG_SshActionExtension.twiki
new file mode 100644
index 0000000..1be5860
--- /dev/null
+++ docs/src/site/twiki/DG_SshActionExtension.twiki
@@ -0,0 +1,118 @@
+<noautolink>
+
+[[index][::Go back to Oozie Documentation Index::]]
+
+-----
+
+---+!! Oozie Ssh Action Extension
+
+%TOC%
+
+---++ Ssh Action
+
+The =ssh= action starts a shell command on a remote machine as a remote secure shell in background. The workflow job
+will wait until the remote shell command completes before continuing to the next action.
+
+The shell command must be present in the remote machine and it must be available for execution via the command path.
+
+The shell command is executed in the home directory of the specified user in the remote host.
+
+The output (STDOUT) of the ssh job can be made available to the workflow job after the ssh job ends. This information
+could be used from within decision nodes. If the output of the ssh job is made available to the workflow job the shell
+command must follow the following requirements:
+
+   * The format of the output must be a valid Java Properties file.
+   * The size of the output must not exceed 2KB.
+
+*Syntax:*
+
+<verbatim>
+<workflow-app name="[WF-DEF-NAME]" xmlns="uri:oozie:workflow:0.1">
+    ...
+    <action name="[NODE-NAME]">
+        <ssh xmlns="uri:oozie:ssh-action:0.1">
+            <host>[USER]@[HOST]</host>
+            <command>[SHELL]</command>
+            <args>[ARGUMENTS]</args>
+            ...
+            <capture-output/>
+        </ssh>
+        <ok to="[NODE-NAME]"/>
+        <error to="[NODE-NAME]"/>
+    </action>
+    ...
+</workflow-app>
+</verbatim>
+
+The =host= indicates the user and host where the shell will be executed.
+
+*IMPORTANT:* The =oozie.action.ssh.allow.user.at.host= property, in the =oozie-site.xml= configuration, indicates if
+an alternate user than the one submitting the job can be used for the ssh invocation. By default this property is set
+tot =true=.
+
+The =command= element indicates the shell command to execute.
+
+The =args= element, if present, contains parameters to be passed to the shell command. If more than one =args= element
+is present they are concatenated in order.
+
+If the =capture-output= element is present, it indicates Oozie to capture output of the STDOUT of the ssh command
+execution. The ssh command output must be in Java Properties file format and it must not exceed 2KB. From within the
+workflow definition, the output of an ssh action node is accessible via the =String action:output(String node,
+String key)= function (Refer to section '4.2.6 Action EL Functions').
+
+The configuration of the =ssh= action can be parameterized (templatized) using EL expressions.
+
+*Example:*
+
+<verbatim>
+<workflow-app name="sample-wf" xmlns="uri:oozie:workflow:0.1">
+    ...
+    <action name="myssjob">
+        <ssh xmlns="uri:oozie:ssh-action:0.1">
+            <host>foo@bar.com<host>
+            <command>uploaddata</command>
+            <args>jdbc:derby://bar.com:1527/myDB</args>
+            <args>hdfs://foobar.com:9000/usr/tucu/myData</args>
+        </ssh>
+        <ok to="myotherjob"/>
+        <error to="errorcleanup"/>
+    </action>
+    ...
+</workflow-app>
+</verbatim>
+
+In the above example, the =uploaddata= shell command is executed with two arguments, =jdbc:derby://foo.com:1527/myDB=
+and =hdfs://foobar.com:9000/usr/tucu/myData=.
+
+The =uploaddata= shell must be available in the remote host and available in the command path.
+
+The output of the command will be ignored because the =capture-output= element is not present.
+
+---++ Appendix, Ssh XML-Schema
+
+---+++ AE.A Appendix A, Ssh XML-Schema
+
+<verbatim>
+<xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema"
+           xmlns:ssh="uri:oozie:ssh-action:0.1" elementFormDefault="qualified"
+           targetNamespace="uri:oozie:ssh-action:0.1">
+.
+    <xs:element name="ssh" type="ssh:ACTION"/>
+.
+    <xs:complexType name="ACTION">
+        <xs:sequence>
+            <xs:element name="host" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="command" type="xs:string" minOccurs="1" maxOccurs="1"/>
+            <xs:element name="args" type="xs:string" minOccurs="0" maxOccurs="unbounded"/>
+            <xs:element name="capture-output" type="ssh:FLAG" minOccurs="0" maxOccurs="1"/>
+        </xs:sequence>
+    </xs:complexType>
+.
+    <xs:complexType name="FLAG"/>
+.
+</xs:schema>
+</verbatim>
+
+[[index][::Go back to Oozie Documentation Index::]]
+
+</noautolink>
diff --git docs/src/site/twiki/ENG_Building.twiki docs/src/site/twiki/ENG_Building.twiki
index dccb062..41ca771 100644
--- docs/src/site/twiki/ENG_Building.twiki
+++ docs/src/site/twiki/ENG_Building.twiki
@@ -86,12 +86,22 @@ $ mvn clean test -Doozie.test.hadoop.minicluster=false -Doozie.test.hadoop.secur
 
 NOTE: The embedded minicluster cannot be used when testing with 'kerberos' authentication.
 
-*Using a custom Oozie configuration for testcases (for example, to use a MySQL DB):*
+*Using a custom Oozie configuration for testcases:*
 
 <verbatim>
-$ mvn clean test -Dmysql -Doozie.test.config.file=/home/tucu/oozie-site-mysql.xml
+$ mvn clean test -Doozie.test.config.file=/home/tucu/custom-oozie-sitel.xml
 </verbatim>
 
+*Running the testcases with different databases:*
+
+<verbatim>
+$ mvn clean test -Doozie.test.db=[hsqldb*|derby|mysql|postgres|oracle]
+</verbatim>
+
+Using =mysql= and =oracle= enables profiles that will include their JARs files in the build. If using
+ =oracle=, the Oracle JDBC JAR file must be manually installed in the local Maven cache (the JAR is
+not available in public Maven repos).
+
 ---+++ Build Options Reference
 
 All these options can be set using *-D*.
@@ -105,19 +115,15 @@ undefined (Hadoop JARs are not included).
 
 *generateSite* (*): generates Oozie documentation, default is undefined (no documentation is generated)
 
-*mysql* (*): Includes MySQL JDBC driver in the distro, default is undefined, MySQL JDBC driver is not included.
-
-*oracle* (*): Includes Oracle JDBC driver in the distro, default is undefined, Oracle JDBC driver is not included.
-
 *skipTests* (*): skips the execution of all testcases, no value required, default is undefined
 
 *test*= (*): runs a single test case, to run a test give the test class name without package and extension, no default
 
 *hadoop20*= (*) : indicates the build/test should not include classes for Hadoop 20S, default is 'false'
 
-*oozie.test.db*= (*): indicates the database to use for running the testcases, supported values are 'hsqldb', 'derby' and 'other';
-default value is 'hsqldb'. IMPORTANT, when using 'derby' a Maven profile is activated to run all the testcases in
-'always' fork mode (otherwise Derby driver goes bonker after 50 testcases or so due to repeated initializations). Use 'other' when using MySQL, Oracle or PostgreSQL databases, if using 'other' a '-Doozie.test.config.file=' must be used with the correct JDBC information must be provided (the core/src/test/resources/ directory has samples for the different databases).
+*oozie.test.db*= (*): indicates the database to use for running the testcases, supported values are 'hsqldb', 'derby',
+ 'mysql', 'postgres' and 'oracle'; default value is 'hsqldb'. For each database there is
+ =core/src/test/resources/DATABASE-oozie-site.xml= file preconfigured.
 
 *oozie.test.properties* (*): indicates the file to load the test properties from, by default is =test.properties=.
 Having this option allows having different test properties sets, for example: minicluster, simple & kerberos.
@@ -184,6 +190,30 @@ value is =/tmp=.
 
 *test.exclude.pattern*= : specifies one or more patterns for testcases to exclude, for example =**/Test*Command.java=.
 
+---+++ Testing Hive Action
+
+Because of depedencies incompatibilities between Hive 0.9.0 and Pig 0.9.0 to run Hive testcases a special profile
+must be used together with the name of the Hive testcases.
+
+<verbatim>
+$ mvn -DtestHive -Dtest=TestHiveActionExecutor,TestHiveMain
+</verbatim>
+
+---+++ Testing Sqoop Action
+
+Apache Sqoop 1.5.0 requires functionality only available in Hadoop 0.21 and onwards. Currently, the only
+stable version of Hadoop implementing those API (SQOOP-384) that is being published by to public Maven
+repositories are Hadoop CDH versions. This profile activates the use of that Hadoop version for testing
+purposes. *This is a temporary fix*.
+
+<verbatim>
+$ mvn -DtestSqoop -Dtest=TestSqoopActionExecutor
+</verbatim>
+
+<verbatim>
+$ mvn -DtestHive -Dtest=TestHiveActionExecutor,TestHiveMain
+</verbatim>
+
 ---++ Building an Oozie Distribution
 
 An Oozie distribution bundles an embedded Tomcat server. The Oozie distro module downloads Tomcat TAR.GZ from Apache
diff --git docs/src/site/twiki/index.twiki docs/src/site/twiki/index.twiki
index 7bf891b..f8ca9b4 100644
--- docs/src/site/twiki/index.twiki
+++ docs/src/site/twiki/index.twiki
@@ -45,18 +45,26 @@ Enough reading already? Follow the steps in [[DG_QuickStart][Oozie Quick Start]]
    * [[DG_CommandLineTool][Command Line Tool]]
    * [[DG_WorkflowReRun][Workflow Re-runs Explained]]
    * [[DG_UsingHadoopKerberos][Using a Hadoop cluster with Kerberos Authentication]]
-   * [[DG_EmailActionExtension][Email Action]]
 
-   * [[DG_CustomActionExecutor][Writing a Custom Action Executor]]
    * [[./client/apidocs/index.html][Oozie Client Javadocs]]
    * [[./core/apidocs/index.html][Oozie Core Javadocs]]
    * [[WebServicesAPI][Oozie Web Services API]]
 
+---+++ Action Extensions
+
+   * [[DG_EmailActionExtension][Email Action]]
+   * [[DG_ShellActionExtension][Shell Action]]
+   * [[DG_HiveActionExtension][Hive Action]]
+   * [[DG_SqoopActionExtension][Sqoop Action]]
+   * [[DG_SshActionExtension][Ssh Action]]
+   * [[DG_CustomActionExecutor][Writing a Custom Action Executor]]
+
 ---++ Administrator Documentation
 
    * [[AG_Install][Oozie Install]]
    * [[AG_Monitoring][Oozie Monitoring]]
    * [[DG_CommandLineTool][Command Line Tool]]
+   * [[AG_OozieUpgrade][Oozie Upgrade]]
 
 #LicenseInfo
 ---++ Licensing Information
diff --git examples/pom.xml examples/pom.xml
index 14bef54..f90e3c2 100644
--- examples/pom.xml
+++ examples/pom.xml
@@ -16,16 +16,19 @@
   See the License for the specific language governing permissions and
   limitations under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
     <modelVersion>4.0.0</modelVersion>
     <parent>
         <groupId>org.apache.oozie</groupId>
         <artifactId>oozie-main</artifactId>
         <version>3.1.3-incubating</version>
     </parent>
+    <groupId>org.apache.oozie</groupId>
     <artifactId>oozie-examples</artifactId>
-    <description>Oozie Examples</description>
-    <name>Oozie Examples</name>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie Examples</description>
+    <name>Apache Oozie Examples</name>
     <packaging>jar</packaging>
 
     <dependencies>
@@ -45,40 +48,57 @@
 
         <dependency>
             <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-hadoop</artifactId>
+            <scope>provided</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-hadoop-test</artifactId>
+            <scope>test</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.oozie</groupId>
             <artifactId>oozie-client</artifactId>
             <scope>provided</scope>
         </dependency>
 
         <dependency>
             <groupId>org.apache.oozie</groupId>
-            <artifactId>oozie-sharelib</artifactId>
+            <artifactId>oozie-sharelib-pig</artifactId>
             <scope>compile</scope>
         </dependency>
 
         <dependency>
-            <groupId>org.apache.hadoop</groupId>
-            <artifactId>hadoop-core</artifactId>
-            <scope>provided</scope>
+            <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-sharelib-streaming</artifactId>
+            <scope>compile</scope>
         </dependency>
 
         <dependency>
-            <groupId>org.apache.hadoop</groupId>
-            <artifactId>hadoop-test</artifactId>
+            <groupId>org.slf4j</groupId>
+            <artifactId>slf4j-log4j12</artifactId>
             <scope>test</scope>
         </dependency>
+
         <dependency>
-            <groupId>com.sun.jersey</groupId>
-            <artifactId>jersey-server</artifactId>
+            <groupId>junit</groupId>
+            <artifactId>junit</artifactId>
             <scope>test</scope>
         </dependency>
-        
 
         <dependency>
-           <groupId>org.slf4j</groupId>
-           <artifactId>slf4j-log4j12</artifactId>
-           <scope>test</scope>
+            <groupId>org.apache.derby</groupId>
+            <artifactId>derby</artifactId>
+            <scope>compile</scope>
         </dependency>
 
+        <dependency>
+            <groupId>hsqldb</groupId>
+            <artifactId>hsqldb</artifactId>
+            <scope>compile</scope>
+        </dependency>
     </dependencies>
     <build>
         <plugins>
@@ -92,19 +112,19 @@
                 </configuration>
             </plugin>
             <plugin>
-                 <groupId>org.apache.rat</groupId>
-                 <artifactId>apache-rat-plugin</artifactId>
-                 <configuration>
+                <groupId>org.apache.rat</groupId>
+                <artifactId>apache-rat-plugin</artifactId>
+                <configuration>
                     <excludes>
-                           <exclude>SecurityAuth.audit</exclude>
-                           <exclude>src/test/resources/PigMain.txt</exclude>
-                           <exclude>src/test/resources/test-ioutils.txt</exclude>
-                           <exclude>.gitignore</exclude>
-                           <exclude>src/main/data/*</exclude>
-                           <exclude>src/main/resources/.gitignore</exclude>
-                       </excludes>
-                   </configuration>
-                </plugin>
+                        <exclude>SecurityAuth.audit</exclude>
+                        <exclude>src/test/resources/PigMain.txt</exclude>
+                        <exclude>src/test/resources/test-ioutils.txt</exclude>
+                        <exclude>.gitignore</exclude>
+                        <exclude>src/main/data/*</exclude>
+                        <exclude>src/main/resources/.gitignore</exclude>
+                    </excludes>
+                </configuration>
+            </plugin>
         </plugins>
     </build>
 
@@ -120,18 +140,18 @@
                 </property>
             </activation>
             <build>
-               <plugins>
-                 <plugin>
-                   <groupId>org.apache.maven.plugins</groupId>
-                   <artifactId>maven-surefire-plugin</artifactId>
-                   <configuration>
-                     <excludes>
-                       <exclude>**/*.java</exclude>
-                     </excludes>
-                   </configuration>
-                 </plugin>
-               </plugins>
-             </build>
+                <plugins>
+                    <plugin>
+                        <groupId>org.apache.maven.plugins</groupId>
+                        <artifactId>maven-surefire-plugin</artifactId>
+                        <configuration>
+                            <excludes>
+                                <exclude>**/*.java</exclude>
+                            </excludes>
+                        </configuration>
+                    </plugin>
+                </plugins>
+            </build>
         </profile>
     </profiles>
 
diff --git examples/src/main/apps/custom-main/job.properties examples/src/main/apps/custom-main/job.properties
index 25df8e7..a5fb77e 100644
--- examples/src/main/apps/custom-main/job.properties
+++ examples/src/main/apps/custom-main/job.properties
@@ -21,7 +21,5 @@ jobTracker=localhost:8021
 queueName=default
 examplesRoot=examples
 
-oozie.libpath=/user/${user.name}/${examplesRoot}/apps/examples-lib
-
 oozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/apps/custom-main
 
diff --git examples/src/main/apps/demo/job.properties examples/src/main/apps/demo/job.properties
index ca8fff1..30c3ea6 100644
--- examples/src/main/apps/demo/job.properties
+++ examples/src/main/apps/demo/job.properties
@@ -21,7 +21,5 @@ jobTracker=localhost:8021
 queueName=default
 examplesRoot=examples
 
-oozie.libpath=/user/${user.name}/${examplesRoot}/apps/examples-lib
-
 oozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/apps/demo
 sshAction=rename
diff --git examples/src/main/apps/hadoop-el/job.properties examples/src/main/apps/hadoop-el/job.properties
index 4313bfe..1d13c27 100644
--- examples/src/main/apps/hadoop-el/job.properties
+++ examples/src/main/apps/hadoop-el/job.properties
@@ -21,6 +21,4 @@ jobTracker=localhost:8021
 queueName=default
 examplesRoot=examples
 
-oozie.libpath=/user/${user.name}/${examplesRoot}/apps/examples-lib
-
 oozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/apps/hadoop-el
diff --git examples/src/main/apps/hive/job.properties examples/src/main/apps/hive/job.properties
new file mode 100644
index 0000000..dbc7ad2
--- /dev/null
+++ examples/src/main/apps/hive/job.properties
@@ -0,0 +1,25 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+nameNode=hdfs://localhost:8020
+jobTracker=localhost:8021
+queueName=default
+examplesRoot=examples
+
+oozie.use.system.libpath=true
+
+oozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/apps/hive
diff --git examples/src/main/apps/hive/my-hive-default.xml examples/src/main/apps/hive/my-hive-default.xml
new file mode 100644
index 0000000..4fedb07
--- /dev/null
+++ examples/src/main/apps/hive/my-hive-default.xml
@@ -0,0 +1,451 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+<configuration>
+
+<!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
+<!-- that are implied by Hadoop setup variables.                                                -->
+<!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
+<!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
+<!-- resource).                                                                                 -->
+
+<!-- Hive Execution Parameters -->
+<property>
+  <name>mapred.reduce.tasks</name>
+  <value>-1</value>
+    <description>The default number of reduce tasks per job.  Typically set
+  to a prime close to the number of available hosts.  Ignored when
+  mapred.job.tracker is "local". Hadoop set this to 1 by default, whereas hive uses -1 as its default value.
+  By setting this property to -1, Hive will automatically figure out what should be the number of reducers.
+  </description>
+</property>
+
+<property>
+  <name>hive.exec.reducers.bytes.per.reducer</name>
+  <value>1000000000</value>
+  <description>size per reducer.The default is 1G, i.e if the input size is 10G, it will use 10 reducers.</description>
+</property>
+
+<property>
+  <name>hive.exec.reducers.max</name>
+  <value>999</value>
+  <description>max number of reducers will be used. If the one
+	specified in the configuration parameter mapred.reduce.tasks is
+	negative, hive will use this one as the max number of reducers when
+	automatically determine number of reducers.</description>
+</property>
+
+<property>
+  <name>hive.exec.scratchdir</name>
+  <value>/tmp/hive-${user.name}</value>
+  <description>Scratch space for Hive jobs</description>
+</property>
+
+<property>
+  <name>hive.test.mode</name>
+  <value>false</value>
+  <description>whether hive is running in test mode. If yes, it turns on sampling and prefixes the output tablename</description>
+</property>
+
+<property>
+  <name>hive.test.mode.prefix</name>
+  <value>test_</value>
+  <description>if hive is running in test mode, prefixes the output table by this string</description>
+</property>
+
+<!-- If the input table is not bucketed, the denominator of the tablesample is determinied by the parameter below   -->
+<!-- For example, the following query:                                                                              -->
+<!--   INSERT OVERWRITE TABLE dest                                                                                  -->
+<!--   SELECT col1 from src                                                                                         -->
+<!-- would be converted to                                                                                          -->
+<!--   INSERT OVERWRITE TABLE test_dest                                                                             -->
+<!--   SELECT col1 from src TABLESAMPLE (BUCKET 1 out of 32 on rand(1))                                             -->
+<property>
+  <name>hive.test.mode.samplefreq</name>
+  <value>32</value>
+  <description>if hive is running in test mode and table is not bucketed, sampling frequency</description>
+</property>
+
+<property>
+  <name>hive.test.mode.nosamplelist</name>
+  <value></value>
+  <description>if hive is running in test mode, dont sample the above comma seperated list of tables</description>
+</property>
+
+<property>
+  <name>hive.metastore.local</name>
+  <value>true</value>
+  <description>controls whether to connect to remove metastore server or open a new metastore server in Hive Client JVM</description>
+</property>
+
+<property>
+  <name>javax.jdo.option.ConnectionURL</name>
+  <value>jdbc:derby:;databaseName=metastore_db;create=true</value>
+  <description>JDBC connect string for a JDBC metastore</description>
+</property>
+
+<property>
+  <name>javax.jdo.option.ConnectionDriverName</name>
+  <value>org.apache.derby.jdbc.EmbeddedDriver</value>
+  <description>Driver class name for a JDBC metastore</description>
+</property>
+
+<property>
+  <name>javax.jdo.PersistenceManagerFactoryClass</name>
+  <value>org.datanucleus.jdo.JDOPersistenceManagerFactory</value>
+  <description>class implementing the jdo persistence</description>
+</property>
+
+<property>
+  <name>javax.jdo.option.DetachAllOnCommit</name>
+  <value>true</value>
+  <description>detaches all objects from session so that they can be used after transaction is committed</description>
+</property>
+
+<property>
+  <name>javax.jdo.option.NonTransactionalRead</name>
+  <value>true</value>
+  <description>reads outside of transactions</description>
+</property>
+
+<property>
+  <name>javax.jdo.option.ConnectionUserName</name>
+  <value>APP</value>
+  <description>username to use against metastore database</description>
+</property>
+
+<!--<property>-->
+  <!--<name>javax.jdo.option.ConnectionPassword</name>-->
+  <!--<value>mine</value>-->
+  <!--<description>password to use against metastore database</description>-->
+<!--</property>-->
+
+<property>
+  <name>datanucleus.validateTables</name>
+  <value>false</value>
+  <description>validates existing schema against code. turn this on if you want to verify existing schema </description>
+</property>
+
+<property>
+  <name>datanucleus.validateColumns</name>
+  <value>false</value>
+  <description>validates existing schema against code. turn this on if you want to verify existing schema </description>
+</property>
+
+<property>
+  <name>datanucleus.validateConstraints</name>
+  <value>false</value>
+  <description>validates existing schema against code. turn this on if you want to verify existing schema </description>
+</property>
+
+<property>
+  <name>datanucleus.storeManagerType</name>
+  <value>rdbms</value>
+  <description>metadata store type</description>
+</property>
+
+<property>
+  <name>datanucleus.autoCreateSchema</name>
+  <value>true</value>
+  <description>creates necessary schema on a startup if one doesn't exist. set this to false, after creating it once</description>
+</property>
+
+<property>
+  <name>datanucleus.autoStartMechanismMode</name>
+  <value>checked</value>
+  <description>throw exception if metadata tables are incorrect</description>
+</property>
+
+<property>
+  <name>datancucleus.transactionIsolation</name>
+  <value>read-committed</value>
+  <description></description>
+</property>
+
+<property>
+  <name>datanuclues.cache.level2</name>
+  <value>true</value>
+  <description>use a level 2 cache. turn this off if metadata is changed independently of hive metastore server</description>
+</property>
+
+<property>
+  <name>datanuclues.cache.level2.type</name>
+  <value>SOFT</value>
+  <description>SOFT=soft reference based cache, WEAK=weak reference based cache.</description>
+</property>
+
+<property>
+  <name>hive.metastore.warehouse.dir</name>
+  <value>/user/hive/warehouse</value>
+  <description>location of default database for the warehouse</description>
+</property>
+
+<property>
+  <name>hive.metastore.connect.retries</name>
+  <value>5</value>
+  <description>Number of retries while opening a connection to metastore</description>
+</property>
+
+<property>
+  <name>hive.metastore.rawstore.impl</name>
+  <value>org.apache.hadoop.hive.metastore.ObjectStore</value>
+  <description>Name of the class that implements org.apache.hadoop.hive.metastore.rawstore interface. This class is used to store and retrieval of raw metadata objects such as table, database</description>
+</property>
+
+<property>
+  <name>hive.default.fileformat</name>
+  <value>TextFile</value>
+  <description>Default file format for CREATE TABLE statement. Options are TextFile and SequenceFile. Users can explicitly say CREATE TABLE ... STORED AS &lt;TEXTFILE|SEQUENCEFILE&gt; to override</description>
+</property>
+
+<property>
+  <name>hive.fileformat.check</name>
+  <value>true</value>
+  <description>Whether to check file format or not when loading data files</description>
+</property>
+
+<property>
+  <name>hive.map.aggr</name>
+  <value>true</value>
+  <description>Whether to use map-side aggregation in Hive Group By queries</description>
+</property>
+
+<property>
+  <name>hive.groupby.skewindata</name>
+  <value>false</value>
+  <description>Whether there is skew in data to optimize group by queries</description>
+</property>
+
+<property>
+  <name>hive.groupby.mapaggr.checkinterval</name>
+  <value>100000</value>
+  <description>Number of rows after which size of the grouping keys/aggregation classes is performed</description>
+</property>
+
+<property>
+  <name>hive.mapred.local.mem</name>
+  <value>0</value>
+  <description>For local mode, memory of the mappers/reducers</description>
+</property>
+
+<property>
+  <name>hive.map.aggr.hash.percentmemory</name>
+  <value>0.5</value>
+  <description>Portion of total memory to be used by map-side grup aggregation hash table</description>
+</property>
+
+<property>
+  <name>hive.map.aggr.hash.min.reduction</name>
+  <value>0.5</value>
+  <description>Hash aggregation will be turned off if the ratio between hash
+  table size and input rows is bigger than this number. Set to 1 to make sure
+  hash aggregation is never turned off.</description>
+</property>
+
+<property>
+  <name>hive.optimize.cp</name>
+  <value>true</value>
+  <description>Whether to enable column pruner</description>
+</property>
+
+<property>
+  <name>hive.optimize.ppd</name>
+  <value>true</value>
+  <description>Whether to enable predicate pushdown</description>
+</property>
+
+<property>
+  <name>hive.optimize.pruner</name>
+  <value>true</value>
+  <description>Whether to enable the new partition pruner which depends on predicate pushdown. If this is disabled,
+  the old partition pruner which is based on AST will be enabled.</description>
+</property>
+
+<property>
+  <name>hive.optimize.groupby</name>
+  <value>true</value>
+  <description>Whether to enable the bucketed group by from bucketed partitions/tables.</description>
+</property>
+
+<property>
+  <name>hive.join.emit.interval</name>
+  <value>1000</value>
+  <description>How many rows in the right-most join operand Hive should buffer before emitting the join result. </description>
+</property>
+
+<property>
+  <name>hive.join.cache.size</name>
+  <value>25000</value>
+  <description>How many rows in the joining tables (except the streaming table) should be cached in memory. </description>
+</property>
+
+<property>
+  <name>hive.mapjoin.bucket.cache.size</name>
+  <value>100</value>
+  <description>How many values in each keys in the map-joined table should be cached in memory. </description>
+</property>
+
+<property>
+  <name>hive.mapjoin.maxsize</name>
+  <value>100000</value>
+  <description>Maximum # of rows of the small table that can be handled by map-side join. If the size is reached and hive.task.progress is set, a fatal error counter is set and the job will be killed.</description>
+</property>
+
+<property>
+  <name>hive.mapjoin.cache.numrows</name>
+  <value>25000</value>
+  <description>How many rows should be cached by jdbm for map join. </description>
+</property>
+
+<property>
+  <name>hive.mapred.mode</name>
+  <value>nonstrict</value>
+  <description>The mode in which the hive operations are being performed. In strict mode, some risky queries are not allowed to run</description>
+</property>
+
+<property>
+  <name>hive.exec.script.maxerrsize</name>
+  <value>100000</value>
+  <description>Maximum number of bytes a script is allowed to emit to standard error (per map-reduce task). This prevents runaway scripts from filling logs partitions to capacity </description>
+</property>
+
+<property>
+  <name>hive.exec.script.allow.partial.consumption</name>
+  <value>false</value>
+  <description> When enabled, this option allows a user script to exit successfully without consuming all the data from the standard input.
+  </description>
+</property>
+
+<property>
+  <name>hive.script.operator.id.env.var</name>
+  <value>HIVE_SCRIPT_OPERATOR_ID</value>
+  <description> Name of the environment variable that holds the unique script operator ID in the user's transform function (the custom mapper/reducer that the user has specified in the query)
+  </description>
+</property>
+
+<property>
+  <name>hive.exec.compress.output</name>
+  <value>false</value>
+  <description> This controls whether the final outputs of a query (to a local/hdfs file or a hive table) is compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* </description>
+</property>
+
+<property>
+  <name>hive.exec.compress.intermediate</name>
+  <value>false</value>
+  <description> This controls whether intermediate files produced by hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* </description>
+</property>
+
+<property>
+  <name>hive.exec.parallel</name>
+  <value>false</value>
+  <description>Whether to execute jobs in parallel</description>
+</property>
+
+<property>
+  <name>hive.hwi.war.file</name>
+  <value>lib/hive-hwi-0.5.0+20.war</value>
+  <description>This sets the path to the HWI war file, relative to ${HIVE_HOME}. </description>
+</property>
+
+<property>
+  <name>hive.hwi.listen.host</name>
+  <value>0.0.0.0</value>
+  <description>This is the host address the Hive Web Interface will listen on</description>
+</property>
+
+<property>
+  <name>hive.hwi.listen.port</name>
+  <value>9999</value>
+  <description>This is the port the Hive Web Interface will listen on</description>
+</property>
+
+<property>
+  <name>hive.exec.pre.hooks</name>
+  <value></value>
+  <description>Pre Execute Hook for Tests</description>
+</property>
+
+<property>
+  <name>hive.merge.mapfiles</name>
+  <value>true</value>
+  <description>Merge small files at the end of a map-only job</description>
+</property>
+
+<property>
+  <name>hive.merge.mapredfiles</name>
+  <value>false</value>
+  <description>Merge small files at the end of any job(map only or map-reduce)</description>
+</property>
+
+<property>
+  <name>hive.heartbeat.interval</name>
+  <value>1000</value>
+  <description>Send a heartbeat after this interval - used by mapjoin and filter operators</description>
+</property>
+
+<property>
+  <name>hive.merge.size.per.task</name>
+  <value>256000000</value>
+  <description>Size of merged files at the end of the job</description>
+</property>
+
+<property>
+  <name>hive.script.auto.progress</name>
+  <value>false</value>
+  <description>Whether Hive Tranform/Map/Reduce Clause should automatically send progress information to TaskTracker to avoid the task getting killed because of inactivity.  Hive sends progress information when the script is outputting to stderr.  This option removes the need of periodically producing stderr messages, but users should be cautious because this may prevent infinite loops in the scripts to be killed by TaskTracker.  </description>
+</property>
+
+<property>
+  <name>hive.script.serde</name>
+  <value>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</value>
+  <description>The default serde for trasmitting input data to and reading output data from the user scripts. </description>
+</property>
+
+<property>
+  <name>hive.script.recordreader</name>
+  <value>org.apache.hadoop.hive.ql.exec.TextRecordReader</value>
+  <description>The default record reader for reading data from the user scripts. </description>
+</property>
+
+<property>
+  <name>hive.script.recordwriter</name>
+  <value>org.apache.hadoop.hive.ql.exec.TextRecordWriter</value>
+  <description>The default record writer for writing data to the user scripts. </description>
+</property>
+
+<property>
+  <name>hive.input.format</name>
+  <value>org.apache.hadoop.hive.ql.io.HiveInputFormat</value>
+  <description>The default input format, if it is not specified, the system assigns it. It is set to HiveInputFormat for hadoop versions 17, 18 and 19, whereas it is set to CombinedHiveInputFormat for hadoop 20. The user can always overwrite it - if there is a bug in CombinedHiveInputFormat, it can always be manually set to HiveInputFormat. </description>
+</property>
+
+<property>
+  <name>hive.udtf.auto.progress</name>
+  <value>false</value>
+  <description>Whether Hive should automatically send progress information to TaskTracker when using UDTF's to prevent the task getting killed because of inactivity.  Users should be cautious because this may prevent TaskTracker from killing tasks with infinte loops.  </description>
+</property>
+
+<property>
+  <name>hive.mapred.reduce.tasks.speculative.execution</name>
+  <value>true</value>
+  <description>Whether speculative execution for reducers should be turned on. </description>
+</property>
+
+</configuration>
diff --git examples/src/main/apps/hive/script.q examples/src/main/apps/hive/script.q
new file mode 100644
index 0000000..ae05aee
--- /dev/null
+++ examples/src/main/apps/hive/script.q
@@ -0,0 +1,2 @@
+CREATE EXTERNAL TABLE test (a INT) STORED AS TEXTFILE LOCATION '${INPUT}';
+INSERT OVERWRITE DIRECTORY '${OUTPUT}' SELECT * FROM test;
diff --git examples/src/main/apps/hive/workflow.xml examples/src/main/apps/hive/workflow.xml
new file mode 100644
index 0000000..cd7f6f6
--- /dev/null
+++ examples/src/main/apps/hive/workflow.xml
@@ -0,0 +1,52 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<workflow-app xmlns="uri:oozie:workflow:0.2" name="hive-wf">
+    <start to="hive-node"/>
+
+    <action name="hive-node">
+        <hive xmlns="uri:oozie:hive-action:0.2">
+            <job-tracker>${jobTracker}</job-tracker>
+            <name-node>${nameNode}</name-node>
+            <prepare>
+                <delete path="${nameNode}/user/${wf:user()}/${examplesRoot}/output-data/hive"/>
+                <mkdir path="${nameNode}/user/${wf:user()}/${examplesRoot}/output-data"/>
+            </prepare>
+            <configuration>
+                <property>
+                    <name>mapred.job.queue.name</name>
+                    <value>${queueName}</value>
+                </property>
+                <property>
+                    <name>oozie.hive.defaults</name>
+                    <value>my-hive-default.xml</value>
+                </property>
+            </configuration>
+            <script>script.q</script>
+            <param>INPUT=/user/${wf:user()}/${examplesRoot}/input-data/table</param>
+            <param>OUTPUT=/user/${wf:user()}/${examplesRoot}/output-data/hive</param>
+        </hive>
+        <ok to="end"/>
+        <error to="fail"/>
+    </action>
+
+    <kill name="fail">
+        <message>Hive failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
+    </kill>
+    <end name="end"/>
+</workflow-app>
diff --git examples/src/main/apps/java-main/job.properties examples/src/main/apps/java-main/job.properties
index bbeebce..519c53e 100644
--- examples/src/main/apps/java-main/job.properties
+++ examples/src/main/apps/java-main/job.properties
@@ -21,6 +21,4 @@ jobTracker=localhost:8021
 queueName=default
 examplesRoot=examples
 
-oozie.libpath=/user/${user.name}/${examplesRoot}/apps/examples-lib
-
 oozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/apps/java-main
diff --git examples/src/main/apps/pig/job.properties examples/src/main/apps/pig/job.properties
index dae58cc..53e94ea 100644
--- examples/src/main/apps/pig/job.properties
+++ examples/src/main/apps/pig/job.properties
@@ -21,6 +21,6 @@ jobTracker=localhost:8021
 queueName=default
 examplesRoot=examples
 
-oozie.libpath=/user/${user.name}/${examplesRoot}/apps/examples-lib
+oozie.use.system.libpath=true
 
 oozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/apps/pig
diff --git examples/src/main/apps/sqoop-freeform/db.hsqldb.properties examples/src/main/apps/sqoop-freeform/db.hsqldb.properties
new file mode 100644
index 0000000..1c45a96
--- /dev/null
+++ examples/src/main/apps/sqoop-freeform/db.hsqldb.properties
@@ -0,0 +1,35 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+#HSQL Database Engine 1.8.0.5
+#Tue Oct 05 11:20:19 SGT 2010
+hsqldb.script_format=0
+runtime.gc_interval=0
+sql.enforce_strict_size=false
+hsqldb.cache_size_scale=8
+readonly=false
+hsqldb.nio_data_file=true
+hsqldb.cache_scale=14
+version=1.8.0
+hsqldb.default_table_type=memory
+hsqldb.cache_file_scale=1
+hsqldb.log_size=200
+modified=no
+hsqldb.cache_version=1.7.0
+hsqldb.original_version=1.8.0
+hsqldb.compatible_version=1.8.0
diff --git examples/src/main/apps/sqoop-freeform/db.hsqldb.script examples/src/main/apps/sqoop-freeform/db.hsqldb.script
new file mode 100644
index 0000000..ef34fd7
--- /dev/null
+++ examples/src/main/apps/sqoop-freeform/db.hsqldb.script
@@ -0,0 +1,9 @@
+CREATE SCHEMA PUBLIC AUTHORIZATION DBA
+CREATE MEMORY TABLE TT(I INTEGER NOT NULL PRIMARY KEY,S VARCHAR(256))
+CREATE USER SA PASSWORD ""
+GRANT DBA TO SA
+SET WRITE_DELAY 10
+SET SCHEMA PUBLIC
+INSERT INTO TT VALUES(1,'a')
+INSERT INTO TT VALUES(2,'a')
+INSERT INTO TT VALUES(3,'a')
diff --git examples/src/main/apps/sqoop-freeform/job.properties examples/src/main/apps/sqoop-freeform/job.properties
new file mode 100644
index 0000000..306421a
--- /dev/null
+++ examples/src/main/apps/sqoop-freeform/job.properties
@@ -0,0 +1,26 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+nameNode=hdfs://localhost:8020
+jobTracker=localhost:8021
+queueName=default
+examplesRoot=examples
+
+oozie.use.system.libpath=true
+
+oozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/apps/sqoop-freeform
diff --git examples/src/main/apps/sqoop-freeform/workflow.xml examples/src/main/apps/sqoop-freeform/workflow.xml
new file mode 100644
index 0000000..1097da3
--- /dev/null
+++ examples/src/main/apps/sqoop-freeform/workflow.xml
@@ -0,0 +1,61 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<workflow-app xmlns="uri:oozie:workflow:0.2" name="sqoop-freeform-wf">
+    <start to="sqoop-freeform-node"/>
+
+    <action name="sqoop-freeform-node">
+        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
+            <job-tracker>${jobTracker}</job-tracker>
+            <name-node>${nameNode}</name-node>
+            <prepare>
+                <delete path="${nameNode}/user/${wf:user()}/${examplesRoot}/output-data/sqoop-freeform"/>
+                <mkdir path="${nameNode}/user/${wf:user()}/${examplesRoot}/output-data"/>
+            </prepare>
+            <configuration>
+                <property>
+                    <name>mapred.job.queue.name</name>
+                    <value>${queueName}</value>
+                </property>
+            </configuration>
+            <arg>import</arg>
+            <arg>--connect</arg>
+            <arg>jdbc:hsqldb:file:db.hsqldb</arg>
+            <arg>--username</arg>
+            <arg>sa</arg>
+            <arg>--password</arg>
+            <arg></arg>
+            <arg>--verbose</arg>
+            <arg>--query</arg>
+            <arg>select TT.I, TT.S from TT where $CONDITIONS</arg>
+            <arg>--target-dir</arg>
+            <arg>/user/${wf:user()}/${examplesRoot}/output-data/sqoop-freeform</arg>
+            <arg>-m</arg>
+            <arg>1</arg>
+            <file>db.hsqldb.properties#db.hsqldb.properties</file>
+            <file>db.hsqldb.script#db.hsqldb.script</file>
+        </sqoop>
+        <ok to="end"/>
+        <error to="fail"/>
+    </action>
+
+    <kill name="fail">
+        <message>Sqoop free form failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
+    </kill>
+    <end name="end"/>
+</workflow-app>
diff --git examples/src/main/apps/sqoop/db.hsqldb.properties examples/src/main/apps/sqoop/db.hsqldb.properties
new file mode 100644
index 0000000..1c45a96
--- /dev/null
+++ examples/src/main/apps/sqoop/db.hsqldb.properties
@@ -0,0 +1,35 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+#HSQL Database Engine 1.8.0.5
+#Tue Oct 05 11:20:19 SGT 2010
+hsqldb.script_format=0
+runtime.gc_interval=0
+sql.enforce_strict_size=false
+hsqldb.cache_size_scale=8
+readonly=false
+hsqldb.nio_data_file=true
+hsqldb.cache_scale=14
+version=1.8.0
+hsqldb.default_table_type=memory
+hsqldb.cache_file_scale=1
+hsqldb.log_size=200
+modified=no
+hsqldb.cache_version=1.7.0
+hsqldb.original_version=1.8.0
+hsqldb.compatible_version=1.8.0
diff --git examples/src/main/apps/sqoop/db.hsqldb.script examples/src/main/apps/sqoop/db.hsqldb.script
new file mode 100644
index 0000000..ef34fd7
--- /dev/null
+++ examples/src/main/apps/sqoop/db.hsqldb.script
@@ -0,0 +1,9 @@
+CREATE SCHEMA PUBLIC AUTHORIZATION DBA
+CREATE MEMORY TABLE TT(I INTEGER NOT NULL PRIMARY KEY,S VARCHAR(256))
+CREATE USER SA PASSWORD ""
+GRANT DBA TO SA
+SET WRITE_DELAY 10
+SET SCHEMA PUBLIC
+INSERT INTO TT VALUES(1,'a')
+INSERT INTO TT VALUES(2,'a')
+INSERT INTO TT VALUES(3,'a')
diff --git examples/src/main/apps/sqoop/job.properties examples/src/main/apps/sqoop/job.properties
new file mode 100644
index 0000000..afc4de8
--- /dev/null
+++ examples/src/main/apps/sqoop/job.properties
@@ -0,0 +1,26 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+nameNode=hdfs://localhost:8020
+jobTracker=localhost:8021
+queueName=default
+examplesRoot=examples
+
+oozie.use.system.libpath=true
+
+oozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/apps/sqoop
diff --git examples/src/main/apps/sqoop/workflow.xml examples/src/main/apps/sqoop/workflow.xml
new file mode 100644
index 0000000..0ae05a1
--- /dev/null
+++ examples/src/main/apps/sqoop/workflow.xml
@@ -0,0 +1,48 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<workflow-app xmlns="uri:oozie:workflow:0.2" name="sqoop-wf">
+    <start to="sqoop-node"/>
+
+    <action name="sqoop-node">
+        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
+            <job-tracker>${jobTracker}</job-tracker>
+            <name-node>${nameNode}</name-node>
+            <prepare>
+                <delete path="${nameNode}/user/${wf:user()}/${examplesRoot}/output-data/sqoop"/>
+                <mkdir path="${nameNode}/user/${wf:user()}/${examplesRoot}/output-data"/>
+            </prepare>
+            <configuration>
+                <property>
+                    <name>mapred.job.queue.name</name>
+                    <value>${queueName}</value>
+                </property>
+            </configuration>
+            <command>import --connect jdbc:hsqldb:file:db.hsqldb --table TT --target-dir /user/${wf:user()}/${examplesRoot}/output-data/sqoop -m 1</command>
+            <file>db.hsqldb.properties#db.hsqldb.properties</file>
+            <file>db.hsqldb.script#db.hsqldb.script</file>
+        </sqoop>
+        <ok to="end"/>
+        <error to="fail"/>
+    </action>
+
+    <kill name="fail">
+        <message>Sqoop failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
+    </kill>
+    <end name="end"/>
+</workflow-app>
diff --git examples/src/main/apps/ssh/job.properties examples/src/main/apps/ssh/job.properties
new file mode 100644
index 0000000..540db02
--- /dev/null
+++ examples/src/main/apps/ssh/job.properties
@@ -0,0 +1,24 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+nameNode=hdfs://localhost:8020
+jobTracker=localhost:8021
+queueName=default
+examplesRoot=examples
+
+oozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/apps/ssh
diff --git examples/src/main/apps/ssh/workflow.xml examples/src/main/apps/ssh/workflow.xml
new file mode 100644
index 0000000..4b53a8b
--- /dev/null
+++ examples/src/main/apps/ssh/workflow.xml
@@ -0,0 +1,36 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<workflow-app xmlns="uri:oozie:workflow:0.2" name="ssh-wf">
+    <start to="ssh"/>
+
+    <action name="ssh">
+        <ssh xmlns="uri:oozie:ssh-action:0.1">
+            <host>localhost</host>
+            <command>echo</command>
+            <args>"Hello Oozie!"</args>
+        </ssh>
+        <ok to="end"/>
+        <error to="fail"/>
+    </action>
+
+    <kill name="fail">
+        <message>SSH action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
+    </kill>
+
+    <end name="end"/>
+</workflow-app>
diff --git examples/src/main/apps/streaming/job.properties examples/src/main/apps/streaming/job.properties
index fd6a8fc..41f59db 100644
--- examples/src/main/apps/streaming/job.properties
+++ examples/src/main/apps/streaming/job.properties
@@ -21,6 +21,6 @@ jobTracker=localhost:8021
 queueName=default
 examplesRoot=examples
 
-oozie.libpath=/user/${user.name}/${examplesRoot}/apps/examples-lib
+oozie.use.system.libpath=true
 
 oozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/apps/streaming
diff --git examples/src/test/java/org/apache/oozie/example/TestLocalOozieExample.java examples/src/test/java/org/apache/oozie/example/TestLocalOozieExample.java
index 0b44379..d76448d 100644
--- examples/src/test/java/org/apache/oozie/example/TestLocalOozieExample.java
+++ examples/src/test/java/org/apache/oozie/example/TestLocalOozieExample.java
@@ -55,7 +55,7 @@ public class TestLocalOozieExample extends XTestCase {
         conf.set("fs.default.name", getNameNodeUri());
         injectKerberosInfo(conf);
 
-// TODO restore this when getting rid of DoAs trick
+        // TODO restore this when getting rid of DoAs trick
 
 //        if (System.getProperty("oozie.test.kerberos", "off").equals("on")) {
 //            Configuration c = new Configuration();
diff --git examples/src/test/resources/derby-oozie-site.xml examples/src/test/resources/derby-oozie-site.xml
new file mode 100644
index 0000000..9f251cc
--- /dev/null
+++ examples/src/test/resources/derby-oozie-site.xml
@@ -0,0 +1,26 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+  Copyright (c) 2010 Yahoo! Inc. All rights reserved.
+  Licensed under the Apache License, Version 2.0 (the "License");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License. See accompanying LICENSE file.
+-->
+<configuration>
+    <property>
+        <name>oozie.service.JPAService.jdbc.driver</name>
+        <value>org.apache.derby.jdbc.EmbeddedDriver</value>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.url</name>
+        <value>jdbc:derby:${oozie.data.dir}/oozie-derby;create=true</value>
+    </property>
+</configuration>
diff --git examples/src/test/resources/hsqldb-oozie-site.xml examples/src/test/resources/hsqldb-oozie-site.xml
new file mode 100644
index 0000000..825c225
--- /dev/null
+++ examples/src/test/resources/hsqldb-oozie-site.xml
@@ -0,0 +1,26 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+  Copyright (c) 2010 Yahoo! Inc. All rights reserved.
+  Licensed under the Apache License, Version 2.0 (the "License");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License. See accompanying LICENSE file.
+-->
+<configuration>
+    <property>
+        <name>oozie.service.JPAService.jdbc.driver</name>
+        <value>org.hsqldb.jdbcDriver</value>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.url</name>
+        <value>jdbc:hsqldb:mem:oozie-db;create=true</value>
+    </property>
+</configuration>
diff --git examples/src/test/resources/mysql-oozie-site.xml examples/src/test/resources/mysql-oozie-site.xml
new file mode 100644
index 0000000..eab5c4d
--- /dev/null
+++ examples/src/test/resources/mysql-oozie-site.xml
@@ -0,0 +1,48 @@
+<?xml version="1.0"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<configuration>
+    <property>
+      <name>oozie.service.JPAService.jdbc.driver</name>
+        <value>com.mysql.jdbc.Driver</value>
+        <description>JDBC driver class.</description>
+    </property>
+    <property>
+        <name>oozie.test.db.port</name>
+        <value>3306</value>
+    </property>
+    <property>
+      <name>oozie.service.JPAService.jdbc.url</name>
+        <value>jdbc:mysql://${oozie.test.db.host}:${oozie.test.db.port}/oozie</value>
+        <description>JDBC URL.</description>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.username</name>
+        <value>oozie</value>
+        <description>DB user name.</description>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.password</name>
+        <value>oozie</value>
+        <description>
+            DB user password. IMPORTANT: if password is emtpy leave a 1 space string, the service trims the
+            value, if empty Configuration assumes it is NULL.
+        </description>
+    </property>
+</configuration>
diff --git examples/src/test/resources/oracle-oozie-site.xml examples/src/test/resources/oracle-oozie-site.xml
new file mode 100644
index 0000000..f199779
--- /dev/null
+++ examples/src/test/resources/oracle-oozie-site.xml
@@ -0,0 +1,42 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+  Copyright (c) 2010 Yahoo! Inc. All rights reserved.
+  Licensed under the Apache License, Version 2.0 (the "License");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License. See accompanying LICENSE file.
+-->
+<configuration>
+    <property>
+        <name>oozie.service.JPAService.jdbc.driver</name>
+        <value>oracle.jdbc.driver.OracleDriver</value>
+    </property>
+    <property>
+        <name>oozie.test.db.port</name>
+        <value>1521</value>
+    </property>
+    <property>
+        <name>oozie.test.db.name</name>
+        <value>xe</value>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.url</name>
+        <value>jdbc:oracle:thin:@//${oozie.test.db.host}:${oozie.test.db.port}/${oozie.test.db.name}</value>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.username</name>
+        <value>oozie</value>
+    </property>
+    <property>
+        <name>oozie.service.JPAService.jdbc.password</name>
+        <value>oozie</value>
+    </property>
+</configuration>
diff --git hadooplibs/hadoop-0_23_1/pom.xml hadooplibs/hadoop-0_23_1/pom.xml
new file mode 100644
index 0000000..df4b581
--- /dev/null
+++ hadooplibs/hadoop-0_23_1/pom.xml
@@ -0,0 +1,61 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../../pom.xml</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-hadoop</artifactId>
+    <version>0.23.1</version>
+    <description>Apache Oozie Hadoop ${project.version}</description>
+    <name>Apache Oozie Hadoop ${project.version}</name>
+    <packaging>jar</packaging>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-client</artifactId>
+            <version>${project.version}</version>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/hadooplib.xml</descriptor>
+                    </descriptors>
+                    <finalName>hadooplibs</finalName>
+                    <appendAssemblyId>false</appendAssemblyId>
+                 </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git hadooplibs/hadoop-0_23_2/pom.xml hadooplibs/hadoop-0_23_2/pom.xml
new file mode 100644
index 0000000..d554f9b
--- /dev/null
+++ hadooplibs/hadoop-0_23_2/pom.xml
@@ -0,0 +1,61 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../../pom.xml</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-hadoop</artifactId>
+    <version>0.23.2-SNAPSHOT</version>
+    <description>Apache Oozie Hadoop ${project.version}</description>
+    <name>Apache Oozie Hadoop ${project.version}</name>
+    <packaging>jar</packaging>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-client</artifactId>
+            <version>${project.version}</version>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/hadooplib.xml</descriptor>
+                    </descriptors>
+                    <finalName>hadooplibs</finalName>
+                    <appendAssemblyId>false</appendAssemblyId>
+                 </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git hadooplibs/hadoop-0_24_0/pom.xml hadooplibs/hadoop-0_24_0/pom.xml
new file mode 100644
index 0000000..3ee656a
--- /dev/null
+++ hadooplibs/hadoop-0_24_0/pom.xml
@@ -0,0 +1,61 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../../pom.xml</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-hadoop</artifactId>
+    <version>0.24.0-SNAPSHOT</version>
+    <description>Apache Oozie Hadoop ${project.version}</description>
+    <name>Apache Oozie Hadoop ${project.version}</name>
+    <packaging>jar</packaging>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-client</artifactId>
+            <version>${project.version}</version>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/hadooplib.xml</descriptor>
+                    </descriptors>
+                    <finalName>hadooplibs</finalName>
+                    <appendAssemblyId>false</appendAssemblyId>
+                 </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git hadooplibs/hadoop-1_0_0/pom.xml hadooplibs/hadoop-1_0_0/pom.xml
new file mode 100644
index 0000000..039c195
--- /dev/null
+++ hadooplibs/hadoop-1_0_0/pom.xml
@@ -0,0 +1,123 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../../pom.xml</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-hadoop</artifactId>
+    <version>1.0.0</version>
+    <description>Apache Oozie Hadoop ${project.version}</description>
+    <name>Apache Oozie Hadoop ${project.version}</name>
+    <packaging>jar</packaging>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-core</artifactId>
+            <version>${project.version}</version>
+            <scope>compile</scope>
+            <exclusions>
+                <exclusion>
+                    <groupId>commons-cli</groupId>
+                    <artifactId>commons-cli</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>commons-httpclient</groupId>
+                    <artifactId>commons-httpclient</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>tomcat</groupId>
+                    <artifactId>jasper-compiler</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>tomcat</groupId>
+                    <artifactId>jasper-runtime</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>javax.servlet</groupId>
+                    <artifactId>servlet-api</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>javax.servlet.jsp</groupId>
+                    <artifactId>jsp-api</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>jetty</groupId>
+                    <artifactId>org.mortbay.jetty</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mortbay.jetty</groupId>
+                    <artifactId>jetty</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mortbay.jetty</groupId>
+                    <artifactId>jetty-util</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mortbay.jetty</groupId>
+                    <artifactId>jsp-2.1</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mortbay.jetty</groupId>
+                    <artifactId>jsp-api-2.1</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mortbay.jetty</groupId>
+                    <artifactId>servlet-api-2.5</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>net.sf.kosmosfs</groupId>
+                    <artifactId>kfs</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>net.java.dev.jets3t</groupId>
+                    <artifactId>jets3t</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.eclipse.jdt</groupId>
+                    <artifactId>core</artifactId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/hadooplib.xml</descriptor>
+                    </descriptors>
+                    <finalName>hadooplibs</finalName>
+                    <appendAssemblyId>false</appendAssemblyId>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git hadooplibs/hadoop-1_0_1/pom.xml hadooplibs/hadoop-1_0_1/pom.xml
new file mode 100644
index 0000000..ae85104
--- /dev/null
+++ hadooplibs/hadoop-1_0_1/pom.xml
@@ -0,0 +1,61 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../../pom.xml</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-hadoop</artifactId>
+    <version>1.0.1</version>
+    <description>Apache Oozie Hadoop ${project.version}</description>
+    <name>Apache Oozie Hadoop ${project.version}</name>
+    <packaging>jar</packaging>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-client</artifactId>
+            <version>${project.version}</version>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/hadooplib.xml</descriptor>
+                    </descriptors>
+                    <finalName>hadooplibs</finalName>
+                    <appendAssemblyId>false</appendAssemblyId>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git hadooplibs/hadoop-test-0_23_1/pom.xml hadooplibs/hadoop-test-0_23_1/pom.xml
new file mode 100644
index 0000000..c1ce70d
--- /dev/null
+++ hadooplibs/hadoop-test-0_23_1/pom.xml
@@ -0,0 +1,59 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../../pom.xml</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-hadoop-test</artifactId>
+    <version>0.23.1</version>
+    <description>Apache Oozie Hadoop ${project.version} Test</description>
+    <name>Apache Oozie Hadoop ${project.version} Test</name>
+    <packaging>jar</packaging>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-minicluster</artifactId>
+            <version>${project.version}</version>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/empty.xml</descriptor>
+                    </descriptors>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git hadooplibs/hadoop-test-0_23_2/pom.xml hadooplibs/hadoop-test-0_23_2/pom.xml
new file mode 100644
index 0000000..dad5ba2
--- /dev/null
+++ hadooplibs/hadoop-test-0_23_2/pom.xml
@@ -0,0 +1,59 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../../pom.xml</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-hadoop-test</artifactId>
+    <version>0.23.2-SNAPSHOT</version>
+    <description>Apache Oozie Hadoop ${project.version} Test</description>
+    <name>Apache Oozie Hadoop ${project.version} Test</name>
+    <packaging>jar</packaging>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-minicluster</artifactId>
+            <version>${project.version}</version>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/empty.xml</descriptor>
+                    </descriptors>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git hadooplibs/hadoop-test-0_24_0/pom.xml hadooplibs/hadoop-test-0_24_0/pom.xml
new file mode 100644
index 0000000..3aaaa7e
--- /dev/null
+++ hadooplibs/hadoop-test-0_24_0/pom.xml
@@ -0,0 +1,59 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../../pom.xml</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-hadoop-test</artifactId>
+    <version>0.24.0-SNAPSHOT</version>
+    <description>Apache Oozie Hadoop ${project.version} Test</description>
+    <name>Apache Oozie Hadoop ${project.version} Test</name>
+    <packaging>jar</packaging>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-minicluster</artifactId>
+            <version>${project.version}</version>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/empty.xml</descriptor>
+                    </descriptors>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git hadooplibs/hadoop-test-1_0_0/pom.xml hadooplibs/hadoop-test-1_0_0/pom.xml
new file mode 100644
index 0000000..c83a6b0
--- /dev/null
+++ hadooplibs/hadoop-test-1_0_0/pom.xml
@@ -0,0 +1,72 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../../pom.xml</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-hadoop-test</artifactId>
+    <version>1.0.0</version>
+    <description>Apache Oozie Hadoop ${project.version} Test</description>
+    <name>Apache Oozie Hadoop ${project.version} Test</name>
+    <packaging>jar</packaging>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-core</artifactId>
+            <version>${project.version}</version>
+            <scope>compile</scope>
+        </dependency>
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-test</artifactId>
+            <version>${project.version}</version>
+            <scope>compile</scope>
+        </dependency>
+        <dependency>
+            <groupId>com.sun.jersey</groupId>
+            <artifactId>jersey-server</artifactId>
+            <version>1.0</version>
+            <scope>compile</scope>
+        </dependency>
+
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/empty.xml</descriptor>
+                    </descriptors>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git hadooplibs/hadoop-test-1_0_1/pom.xml hadooplibs/hadoop-test-1_0_1/pom.xml
new file mode 100644
index 0000000..a8a81d5
--- /dev/null
+++ hadooplibs/hadoop-test-1_0_1/pom.xml
@@ -0,0 +1,59 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../../pom.xml</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-hadoop-test</artifactId>
+    <version>1.0.1</version>
+    <description>Apache Oozie Hadoop ${project.version} Test</description>
+    <name>Apache Oozie Hadoop ${project.version} Test</name>
+    <packaging>jar</packaging>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-minicluster</artifactId>
+            <version>${project.version}</version>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/empty.xml</descriptor>
+                    </descriptors>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git hadooplibs/pom.xml hadooplibs/pom.xml
new file mode 100644
index 0000000..f28557e
--- /dev/null
+++ hadooplibs/pom.xml
@@ -0,0 +1,68 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-hadooplibs</artifactId>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie Hadoop Libs</description>
+    <name>Apache Oozie Hadoop Libs</name>
+    <packaging>pom</packaging>
+
+    <modules>
+        <module>hadoop-1_0_1</module>
+        <module>hadoop-test-1_0_1</module>
+        <module>hadoop-0_23_1</module>
+        <module>hadoop-test-0_23_1</module>
+        <module>hadoop-0_23_2</module>
+        <module>hadoop-test-0_23_2</module>
+        <module>hadoop-0_24_0</module>
+        <module>hadoop-test-0_24_0</module>
+    </modules>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-deploy-plugin</artifactId>
+                <configuration>
+                    <skip>true</skip>
+                </configuration>
+            </plugin>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <descriptors>
+                        <descriptor>../src/main/assemblies/hadooplibs.xml</descriptor>
+                    </descriptors>
+                    <finalName>oozie-${project.version}</finalName>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git minitest/pom.xml minitest/pom.xml
index d51f93a..66d6509 100644
--- minitest/pom.xml
+++ minitest/pom.xml
@@ -7,9 +7,9 @@
   to you under the Apache License, Version 2.0 (the
   "License"); you may not use this file except in compliance
   with the License.  You may obtain a copy of the License at
-  
+
        http://www.apache.org/licenses/LICENSE-2.0
-  
+
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -19,6 +19,11 @@
 <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
 	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
 	<modelVersion>4.0.0</modelVersion>
+        <parent>
+            <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-main</artifactId>
+            <version>3.1.3-incubating</version>
+        </parent>
 	<groupId>org.apache.oozie.test</groupId>
 	<artifactId>oozie-mini</artifactId>
 	<version>3.1.3-incubating</version>
diff --git pom.xml pom.xml
index 4454723..33ce1dd 100644
--- pom.xml
+++ pom.xml
@@ -16,13 +16,14 @@
   See the License for the specific language governing permissions and
   limitations under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
     <modelVersion>4.0.0</modelVersion>
     <groupId>org.apache.oozie</groupId>
     <artifactId>oozie-main</artifactId>
     <version>3.1.3-incubating</version>
-    <description>Oozie Main</description>
-    <name>Oozie Main</name>
+    <description>Apache Oozie Main</description>
+    <name>Apache Oozie Main</name>
     <packaging>pom</packaging>
 
     <licenses>
@@ -37,11 +38,6 @@
         <url>http://www.apache.org</url>
     </organization>
 
-    <scm>
-        <connection>scm:git:git://github.org.apache.oozie.git</connection>
-        <url>https://github.org.apache.oozie</url>
-    </scm>
-
     <properties>
         <!-- to be able to run a single test case from the main project -->
         <failIfNoTests>false</failIfNoTests>
@@ -55,20 +51,44 @@
         <test.exclude>_</test.exclude>
         <test.exclude.pattern>_</test.exclude.pattern>
 
-        <oozie.data.dir>${project.build.directory}/test-data</oozie.data.dir>
+        <oozie.test.dir>${project.build.directory}/test-data</oozie.test.dir>
+        <oozie.test.forkMode>once</oozie.test.forkMode>
 
         <maven.test.redirectTestOutputToFile>true</maven.test.redirectTestOutputToFile>
 
-        <hadoop.version>1.0.0</hadoop.version>
+        <oozie.data.dir>${oozie.test.dir}</oozie.data.dir>
+        <oozie.test.db.host>localhost</oozie.test.db.host>
+        <oozie.test.db>hsqldb</oozie.test.db>
+        <oozie.test.default.config.file>${basedir}/src/test/resources/${oozie.test.db}-oozie-site.xml
+        </oozie.test.default.config.file>
+        <oozie.test.config.file>${oozie.test.default.config.file}</oozie.test.config.file>
+
+        <sharelib.use.transitive.dependencies>true</sharelib.use.transitive.dependencies>
+
+        <hadoop.version>1.0.1</hadoop.version>
+
+        <!--
+        This is required while we support a a pre 0.23 version of Hadoop which does not have
+        the hadoop-auth artifact. After we phase-out pre 0.23 we can get rid of this property.
+        -->
+        <hadoop.auth.version>0.23.1</hadoop.auth.version>
+
+         <!-- Sharelib component versions -->
+         <hive.version>0.9.0-SNAPSHOT</hive.version>
+         <pig.version>0.9.0</pig.version>
+         <sqoop.version>1.5.0-incubating-SNAPSHOT</sqoop.version>
+         <streaming.version>${hadoop.version}</streaming.version>
     </properties>
 
     <modules>
         <module>client</module>
+        <module>hadooplibs</module>
         <module>core</module>
         <module>webapp</module>
         <module>examples</module>
         <module>docs</module>
         <module>sharelib</module>
+        <module>tools</module>
         <module>distro</module>
     </modules>
 
@@ -103,6 +123,22 @@
                 <enabled>false</enabled>
             </snapshots>
         </repository>
+        <repository>
+            <id>apache.snapshots.repo</id>
+            <url>https://repository.apache.org/content/groups/snapshots</url>
+            <name>Apache Snapshots Repository</name>
+            <snapshots>
+                <enabled>true</enabled>
+            </snapshots>
+        </repository>
+        <repository>
+            <id>datanucleus</id>
+            <url>http://www.datanucleus.org/downloads/maven2</url>
+            <name>Datanucleus</name>
+            <snapshots>
+                <enabled>false</enabled>
+            </snapshots>
+        </repository>
     </repositories>
 
     <pluginRepositories>
@@ -125,48 +161,78 @@
 
     <dependencyManagement>
         <dependencies>
-
             <dependency>
                 <groupId>org.apache.oozie</groupId>
                 <artifactId>oozie-client</artifactId>
-                <version>3.1.3-incubating</version>
+                <version>${project.version}</version>
             </dependency>
             <dependency>
                 <groupId>org.apache.oozie</groupId>
                 <artifactId>oozie-core</artifactId>
-                <version>3.1.3-incubating</version>
+                <version>${project.version}</version>
             </dependency>
             <dependency>
                 <groupId>org.apache.oozie</groupId>
                 <artifactId>oozie-core</artifactId>
                 <classifier>tests</classifier>
-                <version>3.1.3-incubating</version>
+                <version>${project.version}</version>
             </dependency>
             <dependency>
                 <groupId>org.apache.oozie</groupId>
                 <artifactId>oozie-examples</artifactId>
-                <version>3.1.3-incubating</version>
+                <version>${project.version}</version>
             </dependency>
             <dependency>
                 <groupId>org.apache.oozie</groupId>
                 <artifactId>oozie-sharelib</artifactId>
-                <version>3.1.3-incubating</version>
+                <version>${project.version}</version>
+            </dependency>
+            <dependency>
+                <groupId>org.apache.oozie</groupId>
+                <artifactId>oozie-sharelib-streaming</artifactId>
+                <version>${project.version}</version>
+            </dependency>
+            <dependency>
+                <groupId>org.apache.oozie</groupId>
+                <artifactId>oozie-sharelib-pig</artifactId>
+                <version>${project.version}</version>
+            </dependency>
+            <dependency>
+                <groupId>org.apache.oozie</groupId>
+                <artifactId>oozie-sharelib-hive</artifactId>
+                <version>${project.version}</version>
+            </dependency>
+            <dependency>
+                <groupId>org.apache.oozie</groupId>
+                <artifactId>oozie-sharelib-sqoop</artifactId>
+                <version>${project.version}</version>
+            </dependency>
+            <dependency>
+                <groupId>org.apache.oozie</groupId>
+                <artifactId>oozie-sharelib-oozie</artifactId>
+                <version>${project.version}</version>
             </dependency>
             <dependency>
                 <groupId>org.apache.oozie</groupId>
                 <artifactId>oozie-docs</artifactId>
-                <version>3.1.3-incubating</version>
+                <version>${project.version}</version>
                 <type>war</type>
             </dependency>
             <dependency>
                 <groupId>org.apache.oozie</groupId>
                 <artifactId>oozie-webapp</artifactId>
-                <version>3.1.3-incubating</version>
+                <version>${project.version}</version>
                 <type>war</type>
             </dependency>
 
             <!-- client -->
             <dependency>
+                <groupId>org.apache.hadoop</groupId>
+                <artifactId>hadoop-auth</artifactId>
+                <version>${hadoop.auth.version}</version>
+            </dependency>
+
+            <dependency>
                 <groupId>com.googlecode.json-simple</groupId>
                 <artifactId>json-simple</artifactId>
                 <version>1.1</version>
@@ -192,6 +258,18 @@
 
             <!-- core -->
             <dependency>
+                <groupId>org.apache.oozie</groupId>
+                <artifactId>oozie-hadoop</artifactId>
+                <version>${hadoop.version}</version>
+            </dependency>
+
+            <dependency>
+                <groupId>org.apache.oozie</groupId>
+                <artifactId>oozie-hadoop-test</artifactId>
+                <version>${hadoop.version}</version>
+            </dependency>
+
+            <dependency>
                 <groupId>org.apache.openjpa</groupId>
                 <artifactId>openjpa-persistence</artifactId>
                 <version>2.1.0</version>
@@ -204,6 +282,12 @@
             </dependency>
 
             <dependency>
+                <groupId>commons-lang</groupId>
+                <artifactId>commons-lang</artifactId>
+                <version>2.4</version>
+            </dependency>
+
+            <dependency>
                 <groupId>org.apache.openjpa</groupId>
                 <artifactId>openjpa-jdbc</artifactId>
                 <version>2.1.0</version>
@@ -220,95 +304,137 @@
                 <artifactId>openjpa-persistence-jdbc</artifactId>
                 <version>2.1.0</version>
             </dependency>
-            
             <dependency>
                 <groupId>commons-codec</groupId>
                 <artifactId>commons-codec</artifactId>
-                <version>1.3</version>
+                <version>1.4</version>
             </dependency>
             
             <dependency>
                 <groupId>org.apache.hadoop</groupId>
-                <artifactId>hadoop-core</artifactId>
-                <version>${hadoop.version}</version>
-                <exclusions>
-                    <exclusion>
-                        <groupId>org.apache.commons</groupId>
-                        <artifactId>commons-cli</artifactId>
-                    </exclusion>
-                </exclusions>
+                <artifactId>hadoop-streaming</artifactId>
+                <version>${streaming.version}</version>
             </dependency>
 
             <dependency>
-                <groupId>org.apache.hadoop</groupId>
-                <artifactId>hadoop-test</artifactId>
-                <version>${hadoop.version}</version>
+                <groupId>org.apache.pig</groupId>
+                <artifactId>pig</artifactId>
+                <version>${pig.version}</version>
                 <exclusions>
                     <exclusion>
-                        <groupId>org.apache.commons</groupId>
-                        <artifactId>commons-cli</artifactId>
+                        <groupId>org.apache.hadoop</groupId>
+                        <artifactId>hadoop-core</artifactId>
+                    </exclusion>
+                    <exclusion>
+                        <groupId>commons-httpclient</groupId>
+                        <artifactId>commons-httpclient</artifactId>
+                    </exclusion>
+                    <exclusion>
+                        <groupId>org.mortbay.jetty</groupId>
+                        <artifactId>jetty</artifactId>
+                    </exclusion>
+                    <exclusion>
+                        <groupId>org.mortbay.jetty</groupId>
+                        <artifactId>jetty-util</artifactId>
+                    </exclusion>
+                    <exclusion>
+                        <groupId>org.mortbay.jetty</groupId>
+                        <artifactId>servlet-api-2.5</artifactId>
+                    </exclusion>
+                    <exclusion>
+                        <groupId>org.mortbay.jetty</groupId>
+                        <artifactId>jsp-api-2.1</artifactId>
+                    </exclusion>
+                    <exclusion>
+                        <groupId>org.mortbay.jetty</groupId>
+                        <artifactId>jsp-2.1</artifactId>
+                    </exclusion>
+                    <exclusion>
+                        <groupId>commons-el</groupId>
+                        <artifactId>commons-el</artifactId>
+                    </exclusion>
+                    <exclusion>
+                        <groupId>tomcat</groupId>
+                        <artifactId>jasper-compiler</artifactId>
+                    </exclusion>
+                    <exclusion>
+                        <groupId>tomcat</groupId>
+                        <artifactId>jasper-runtime</artifactId>
+                    </exclusion>
+                    <exclusion>
+                        <groupId>junit</groupId>
+                        <artifactId>junit</artifactId>
                     </exclusion>
                     <exclusion>
-                        <groupId>commons-cli</groupId>
-                        <artifactId>commons-cli</artifactId>
+                        <groupId>net.sf.kosmosfs</groupId>
+                        <artifactId>kfs</artifactId>
                     </exclusion>
                     <exclusion>
-                        <groupId>org.slf4j</groupId>
-                        <artifactId>slf4j-api</artifactId>
+                        <groupId>net.java.dev.jets3t</groupId>
+                        <artifactId>jets3t</artifactId>
                     </exclusion>
                     <exclusion>
-                        <groupId>org.slf4j</groupId>
-                        <artifactId>slf4j-log4j12</artifactId>
+                        <groupId>oro</groupId>
+                        <artifactId>oro</artifactId>
                     </exclusion>
                 </exclusions>
             </dependency>
 
             <dependency>
-                <groupId>com.sun.jersey</groupId>
-                <artifactId>jersey-server</artifactId>
-                <version>1.0</version>
-            </dependency>
-
-            <dependency>
-                <groupId>org.apache.hadoop</groupId>
-                <artifactId>hadoop-streaming</artifactId>
-                <version>${hadoop.version}</version>
+                <groupId>jline</groupId>
+                <artifactId>jline</artifactId>
+                <version>0.9.94</version>
                 <exclusions>
                     <exclusion>
-                        <groupId>org.apache.commons</groupId>
-                        <artifactId>commons-cli</artifactId>
+                        <groupId>junit</groupId>
+                        <artifactId>junit</artifactId>
                     </exclusion>
                 </exclusions>
             </dependency>
 
             <dependency>
-                <groupId>org.apache.pig</groupId>
-                <artifactId>pig</artifactId>
-                <version>0.9.0</version>
+                <groupId>org.apache.thrift</groupId>
+                <artifactId>thrift</artifactId>
+                <version>0.5.0-cdh</version>
+            </dependency>
+
+            <dependency>
+                <groupId>org.apache.hive</groupId>
+                <artifactId>hive-cli</artifactId>
+                <version>${hive.version}</version>
                 <exclusions>
                     <exclusion>
-                        <groupId>org.apache.hadoop</groupId>
-                        <artifactId>hadoop-core</artifactId>
+                        <groupId>hadoop</groupId>
+                        <artifactId>core</artifactId>
                     </exclusion>
                 </exclusions>
             </dependency>
-
             <dependency>
-                <groupId>jline</groupId>
-                <artifactId>jline</artifactId>
-                <version>0.9.94</version>
+                <groupId>org.apache.hive</groupId>
+                <artifactId>hive-contrib</artifactId>
+                <version>0.9.0-SNAPSHOT</version>
                 <exclusions>
                     <exclusion>
-                        <groupId>junit</groupId>
-                        <artifactId>junit</artifactId>
+                        <groupId>hadoop</groupId>
+                        <artifactId>core</artifactId>
                     </exclusion>
                 </exclusions>
             </dependency>
 
             <dependency>
                 <groupId>org.slf4j</groupId>
+                <artifactId>slf4j-api</artifactId>
+                <version>1.5.8</version>
+            </dependency>
+            <dependency>
+                <groupId>org.slf4j</groupId>
                 <artifactId>slf4j-log4j12</artifactId>
-                <version>1.4.3</version>
+                <version>1.5.8</version>
+            </dependency>
+            <dependency>
+                <groupId>org.apache.hive</groupId>
+                <artifactId>hive-builtins</artifactId>
+                <version>${hive.version}</version>
             </dependency>
 
             <dependency>
@@ -416,7 +542,7 @@
             <dependency>
                 <groupId>com.oracle</groupId>
                 <artifactId>ojdbc6</artifactId>
-                <version>11.2.0.1.0</version>
+                <version>11.2.0.2.0</version>
             </dependency>
 
             <dependency>
@@ -444,23 +570,41 @@
                 <version>9.0-801.jdbc4</version>
             </dependency>
 
-			<dependency>
-				<groupId>org.python</groupId>
-				<artifactId>jython</artifactId>
-				<version>2.5.0</version>
-			</dependency>
+            <dependency>
+                <groupId>org.python</groupId>
+                <artifactId>jython</artifactId>
+                <version>2.5.0</version>
+            </dependency>
 
-			<dependency>
-				<groupId>org.antlr</groupId>
-				<artifactId>antlr-runtime</artifactId>
-				<version>3.4</version>
-			</dependency>
+            <dependency>
+                <groupId>org.antlr</groupId>
+                <artifactId>antlr-runtime</artifactId>
+                <version>3.4</version>
+            </dependency>
 
-			<dependency>
-				<groupId>com.google.guava</groupId>
-				<artifactId>guava</artifactId>
-				<version>r09</version>
-			</dependency>
+            <dependency>
+                <groupId>com.google.guava</groupId>
+                <artifactId>guava</artifactId>
+                <version>r09</version>
+            </dependency>
+
+            <dependency>
+                <groupId>org.apache.sqoop</groupId>
+                <artifactId>sqoop</artifactId>
+                <version>${sqoop.version}</version>
+            </dependency>
+
+            <dependency>
+                <groupId>commons-io</groupId>
+                <artifactId>commons-io</artifactId>
+                <version>1.4</version>
+            </dependency>
+
+            <dependency>
+                <groupId>org.mockito</groupId>
+                <artifactId>mockito-all</artifactId>
+                <version>1.8.5</version>
+            </dependency>
 
         </dependencies>
     </dependencyManagement>
@@ -491,7 +635,7 @@
                 <plugin>
                     <groupId>org.apache.maven.plugins</groupId>
                     <artifactId>maven-assembly-plugin</artifactId>
-                    <version>2.2-beta-3</version>
+                    <version>2.2.1</version>
                 </plugin>
                 <plugin>
                     <groupId>org.apache.maven.plugins</groupId>
@@ -525,11 +669,26 @@
                     <artifactId>maven-war-plugin</artifactId>
                     <version>2.1</version>                    
                 </plugin>
-		<plugin>
+                <plugin>
                     <groupId>org.apache.rat</groupId>
                     <artifactId>apache-rat-plugin</artifactId>
                     <version>0.7</version>
                 </plugin>
+                <plugin>
+                    <groupId>org.apache.maven.plugins</groupId>
+                    <artifactId>maven-resources-plugin</artifactId>
+                    <version>2.5</version>
+                </plugin>
+                <plugin>
+                    <groupId>org.apache.maven.plugins</groupId>
+                    <artifactId>maven-deploy-plugin</artifactId>
+                    <version>2.5</version>
+                </plugin>
+                <plugin>
+                    <groupId>org.apache.maven.plugins</groupId>
+                    <artifactId>maven-dependency-plugin</artifactId>
+                    <version>2.4</version>
+                </plugin>
             </plugins>
         </pluginManagement>
 
@@ -567,13 +726,13 @@
                         <phase>validate</phase>
                     </execution>
                     <execution>
-                         <id>site</id>
-                         <goals>
-                             <goal>enforce</goal>
-                         </goals>
-                         <phase>pre-site</phase>
-                     </execution>
-                 </executions>
+                        <id>site</id>
+                        <goals>
+                            <goal>enforce</goal>
+                        </goals>
+                        <phase>pre-site</phase>
+                    </execution>
+                </executions>
             </plugin>
             <plugin>
                 <groupId>org.apache.maven.plugins</groupId>
@@ -588,14 +747,31 @@
                 <artifactId>maven-surefire-plugin</artifactId>
                 <configuration>
                     <forkMode>${oozie.test.forkMode}</forkMode>
-                    <argLine>-Xmx1024m</argLine>
-                    <systemPropertiesVariables>
+                    <forkedProcessTimeoutInSeconds>3600</forkedProcessTimeoutInSeconds>
+                    <argLine>-Xmx1024m -da</argLine>
+                    <systemPropertyVariables>
                         <hadoop.log.dir>/tmp</hadoop.log.dir>
+                        <oozie.test.db.host>${oozie.test.db.host}</oozie.test.db.host>
+                        <oozie.test.config.file>${oozie.test.config.file}</oozie.test.config.file>
                         <oozie.data.dir>${oozie.data.dir}</oozie.data.dir>
-                    </systemPropertiesVariables>
+                    </systemPropertyVariables>
+                    <environmentVariables>
+                        <HADOOP_HOME>dummy</HADOOP_HOME>
+                    </environmentVariables>
                     <excludes>
                         <exclude>**/${test.exclude}.java</exclude>
                         <exclude>${test.exclude.pattern}</exclude>
+
+                        <!-- Explictly use -Dtest=TestSshActionExecutor,TestSshActionExecutorExtension
+                             to test the SSH action
+                        -->
+                        <exclude>**/TestSsh*.java</exclude>
+
+                        <!-- See 'testHive' profile in core/pom.xml and the Building doc-->
+                        <exclude>**/TestHive*.java</exclude>
+
+                        <!-- See 'testSqoop' profile in core/pom.xml and the Building doc-->
+                        <exclude>**/TestSqoop*.java</exclude>
                     </excludes>
                 </configuration>
             </plugin>
@@ -607,18 +783,18 @@
                     </descriptors>
                 </configuration>
             </plugin>
-                <plugin>
-                    <groupId>org.apache.rat</groupId>
-                    <artifactId>apache-rat-plugin</artifactId>
-                    <configuration>
-                       <excludes>
-                           <exclude>release-log.txt</exclude>
-                           <exclude>work.log</exclude>
-                           <exclude>SecurityAuth.audit</exclude>
-                           <exclude>.gitignore</exclude>
-                       </excludes>
-                    </configuration>
-                </plugin>
+            <plugin>
+                <groupId>org.apache.rat</groupId>
+                <artifactId>apache-rat-plugin</artifactId>
+                <configuration>
+                    <excludes>
+                        <exclude>release-log.txt</exclude>
+                        <exclude>work.log</exclude>
+                        <exclude>SecurityAuth.audit</exclude>
+                        <exclude>.gitignore</exclude>
+                    </excludes>
+                </configuration>
+            </plugin>
         </plugins>
     </build>
 
@@ -637,5 +813,5 @@
             </properties>
         </profile>
     </profiles>
-    
+
 </project>
diff --git sharelib/hive/pom.xml sharelib/hive/pom.xml
new file mode 100644
index 0000000..e261cec
--- /dev/null
+++ sharelib/hive/pom.xml
@@ -0,0 +1,162 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../..</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-sharelib-hive</artifactId>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie Share Lib Hive</description>
+    <name>Apache Oozie Share Lib Hive</name>
+    <packaging>jar</packaging>
+
+    <properties>
+        <sharelib.action.postfix>hive</sharelib.action.postfix>
+    </properties>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.hive</groupId>
+            <artifactId>hive-cli</artifactId>
+            <scope>compile</scope>
+            <exclusions>
+                <exclusion>
+                    <groupId>org.apache.hadoop</groupId>
+                    <artifactId>hadoop-core</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.apache.hive</groupId>
+                    <artifactId>hive-hwi</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.apache.hive</groupId>
+                    <artifactId>hive-jdbc</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.apache.hive</groupId>
+                    <artifactId>hive-anttasks</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>junit</groupId>
+                    <artifactId>junit</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>log4j</groupId>
+                    <artifactId>log4j</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>commons-codec</groupId>
+                    <artifactId>commons-codec</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>commons-logging</groupId>
+                    <artifactId>commons-logging</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>commons-logging</groupId>
+                    <artifactId>commons-logging-api</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mockito</groupId>
+                    <artifactId>mockito-all</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>javax.servlet</groupId>
+                    <artifactId>servlet-api</artifactId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+        <dependency>
+            <groupId>org.apache.hive</groupId>
+            <artifactId>hive-contrib</artifactId>
+            <scope>compile</scope>
+            <exclusions>
+                <exclusion>
+                    <groupId>org.apache.hadoop</groupId>
+                    <artifactId>hadoop-core</artifactId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+        <dependency>
+            <groupId>org.apache.hive</groupId>
+            <artifactId>hive-builtins</artifactId>
+            <scope>compile</scope>
+        </dependency>
+        <dependency>
+            <groupId>org.antlr</groupId>
+            <artifactId>antlr-runtime</artifactId>
+            <version>3.0.1</version>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <resources>
+            <resource>
+                <directory>src/main/resources</directory>
+                <filtering>true</filtering>
+            </resource>
+        </resources>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-dependency-plugin</artifactId>
+                <executions>
+                    <execution>
+                        <id>gen-classpath</id>
+                        <phase>generate-test-resources</phase>
+                        <goals>
+                            <goal>build-classpath</goal>
+                        </goals>
+                        <configuration>
+                            <includeScope>compile</includeScope>
+                            <outputFile>${project.build.directory}/classpath</outputFile>
+                        </configuration>
+                    </execution>
+                </executions>
+            </plugin>
+            <!--<plugin>-->
+                <!--<groupId>org.apache.maven.plugins</groupId>-->
+                <!--<artifactId>maven-deploy-plugin</artifactId>-->
+                <!--<configuration>-->
+                    <!--<skip>true</skip>-->
+                <!--</configuration>-->
+            <!--</plugin>-->
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <finalName>partial-sharelib</finalName>
+                    <appendAssemblyId>false</appendAssemblyId>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/partial-sharelib.xml</descriptor>
+                    </descriptors>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git sharelib/oozie/pom.xml sharelib/oozie/pom.xml
new file mode 100644
index 0000000..a1fd5d9
--- /dev/null
+++ sharelib/oozie/pom.xml
@@ -0,0 +1,87 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../..</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-sharelib-oozie</artifactId>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie Share Lib Oozie</description>
+    <name>Apache Oozie Share Lib Oozie</name>
+    <packaging>jar</packaging>
+
+    <properties>
+        <sharelib.action.postfix>oozie</sharelib.action.postfix>
+    </properties>
+
+    <dependencies>
+       <dependency>
+            <groupId>com.googlecode.json-simple</groupId>
+            <artifactId>json-simple</artifactId>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <resources>
+            <resource>
+                <directory>src/main/resources</directory>
+                <filtering>true</filtering>
+            </resource>
+        </resources>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-dependency-plugin</artifactId>
+                <executions>
+                    <execution>
+                        <id>gen-classpath</id>
+                        <phase>generate-test-resources</phase>
+                        <goals>
+                            <goal>build-classpath</goal>
+                        </goals>
+                        <configuration>
+                            <includeScope>compile</includeScope>
+                            <outputFile>${project.build.directory}/classpath</outputFile>
+                        </configuration>
+                    </execution>
+                </executions>
+            </plugin>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <finalName>partial-sharelib</finalName>
+                    <appendAssemblyId>false</appendAssemblyId>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/partial-sharelib.xml</descriptor>
+                    </descriptors>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git sharelib/pig/pom.xml sharelib/pig/pom.xml
new file mode 100644
index 0000000..a7c8301
--- /dev/null
+++ sharelib/pig/pom.xml
@@ -0,0 +1,117 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../..</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-sharelib-pig</artifactId>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie Share Lib Pig</description>
+    <name>Apache Oozie Share Lib Pig</name>
+    <packaging>jar</packaging>
+
+    <properties>
+        <sharelib.action.postfix>pig</sharelib.action.postfix>
+    </properties>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.pig</groupId>
+            <artifactId>pig</artifactId>
+            <scope>compile</scope>
+        </dependency>
+        <dependency>
+            <groupId>jline</groupId>
+            <artifactId>jline</artifactId>
+            <scope>compile</scope>
+        </dependency>
+        <dependency>
+            <groupId>org.python</groupId>
+            <artifactId>jython</artifactId>
+            <scope>compile</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.antlr</groupId>
+            <artifactId>antlr-runtime</artifactId>
+            <scope>compile</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>com.google.guava</groupId>
+            <artifactId>guava</artifactId>
+            <scope>compile</scope>
+        </dependency>
+
+    </dependencies>
+
+    <build>
+        <resources>
+            <resource>
+                <directory>src/main/resources</directory>
+                <filtering>true</filtering>
+            </resource>
+        </resources>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-dependency-plugin</artifactId>
+                <executions>
+                    <execution>
+                        <id>gen-classpath</id>
+                        <phase>generate-test-resources</phase>
+                        <goals>
+                            <goal>build-classpath</goal>
+                        </goals>
+                        <configuration>
+                            <includeScope>compile</includeScope>
+                            <outputFile>${project.build.directory}/classpath</outputFile>
+                        </configuration>
+                    </execution>
+                </executions>
+            </plugin>
+            <!--<plugin>-->
+                <!--<groupId>org.apache.maven.plugins</groupId>-->
+                <!--<artifactId>maven-deploy-plugin</artifactId>-->
+                <!--<configuration>-->
+                    <!--<skip>true</skip>-->
+                <!--</configuration>-->
+            <!--</plugin>-->
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <finalName>partial-sharelib</finalName>
+                    <appendAssemblyId>false</appendAssemblyId>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/partial-sharelib.xml</descriptor>
+                    </descriptors>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git sharelib/pom.xml sharelib/pom.xml
index a6eaa07..57a3d31 100644
--- sharelib/pom.xml
+++ sharelib/pom.xml
@@ -16,65 +16,28 @@
   See the License for the specific language governing permissions and
   limitations under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
     <modelVersion>4.0.0</modelVersion>
     <parent>
         <groupId>org.apache.oozie</groupId>
         <artifactId>oozie-main</artifactId>
         <version>3.1.3-incubating</version>
     </parent>
+    <groupId>org.apache.oozie</groupId>
     <artifactId>oozie-sharelib</artifactId>
-    <description>Oozie Share Lib</description>
-    <name>Oozie Share Lib</name>
-    <packaging>jar</packaging>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie Share Lib</description>
+    <name>Apache Oozie Share Lib</name>
+    <packaging>pom</packaging>
 
-    <dependencies>
-        <dependency>
-            <groupId>org.apache.hadoop</groupId>
-            <artifactId>hadoop-streaming</artifactId>
-            <scope>compile</scope>
-            <exclusions>
-                <exclusion>
-                    <groupId>org.apache.hadoop</groupId>
-                    <artifactId>hadoop-core</artifactId>
-                </exclusion>
-            </exclusions>
-        </dependency>
-        <dependency>
-            <groupId>org.apache.pig</groupId>
-            <artifactId>pig</artifactId>
-            <scope>compile</scope>
-            <exclusions>
-                <exclusion>
-                    <groupId>org.apache.hadoop</groupId>
-                    <artifactId>hadoop-core</artifactId>
-                </exclusion>
-            </exclusions>
-        </dependency>
-        <dependency>
-            <groupId>jline</groupId>
-            <artifactId>jline</artifactId>
-            <scope>compile</scope>
-        </dependency>
-
-		<dependency>
-			<groupId>org.python</groupId>
-			<artifactId>jython</artifactId>
-			<scope>runtime</scope>
-		</dependency>
-
-		<dependency>
-			<groupId>org.antlr</groupId>
-			<artifactId>antlr-runtime</artifactId>
-			<scope>runtime</scope>
-		</dependency>
-
-		<dependency>
-			<groupId>com.google.guava</groupId>
-			<artifactId>guava</artifactId>
-			<scope>runtime</scope>
-		</dependency>
-    </dependencies>
+    <modules>
+        <module>streaming</module>
+        <module>pig</module>
+        <module>hive</module>
+        <module>sqoop</module>
+        <module>oozie</module>
+    </modules>
 
     <build>
         <resources>
@@ -86,13 +49,33 @@
         <plugins>
             <plugin>
                 <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-resources-plugin</artifactId>
+                <executions>
+                    <execution>
+                        <phase>package</phase>
+                        <goals>
+                            <goal>resources</goal>
+                        </goals>
+                    </execution>
+                </executions>
+            </plugin>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
                 <artifactId>maven-assembly-plugin</artifactId>
                 <configuration>
+                    <appendAssemblyId>false</appendAssemblyId>
                     <descriptors>
                         <descriptor>../src/main/assemblies/sharelib.xml</descriptor>
                     </descriptors>
                 </configuration>
             </plugin>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-deploy-plugin</artifactId>
+                <configuration>
+                    <skip>true</skip>
+                </configuration>
+            </plugin>
         </plugins>
     </build>
 
diff --git sharelib/sqoop/pom.xml sharelib/sqoop/pom.xml
new file mode 100644
index 0000000..42778fd
--- /dev/null
+++ sharelib/sqoop/pom.xml
@@ -0,0 +1,227 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../..</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-sharelib-sqoop</artifactId>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie Share Lib Sqoop</description>
+    <name>Apache Oozie Share Lib Sqoop</name>
+    <packaging>jar</packaging>
+
+    <properties>
+        <sharelib.action.postfix>sqoop</sharelib.action.postfix>
+    </properties>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.sqoop</groupId>
+            <artifactId>sqoop</artifactId>
+            <scope>compile</scope>
+            <exclusions>
+                <exclusion>
+                    <groupId>org.apache.hadoop</groupId>
+                    <artifactId>hadoop-core</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>log4j</groupId>
+                    <artifactId>log4j</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>commons-codec</groupId>
+                    <artifactId>commons-codec</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>commons-logging</groupId>
+                    <artifactId>commons-logging</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>commons-logging</groupId>
+                    <artifactId>commons-logging-api</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>hsqldb</groupId>
+                    <artifactId>hsqldb</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>commons-cli</groupId>
+                    <artifactId>commons-cli</artifactId>
+                </exclusion>
+
+
+                <exclusion>
+                    <groupId>org.apache.thrift</groupId>
+                    <artifactId>thrift</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>commons-httpclient</groupId>
+                    <artifactId>commons-httpclient</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.slf4j</groupId>
+                    <artifactId>slf4j-api</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.slf4j</groupId>
+                    <artifactId>slf4j-log4j12</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mortbay.jetty</groupId>
+                    <artifactId>jetty</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mortbay.jetty</groupId>
+                    <artifactId>jetty-util</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mortbay.jetty</groupId>
+                    <artifactId>servlet-api-2.5</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>javax.servlet.jsp</groupId>
+                    <artifactId>jsp-api</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>tomcat</groupId>
+                    <artifactId>jasper-runtime</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>tomcat</groupId>
+                    <artifactId>jasper-compiler</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.jruby</groupId>
+                    <artifactId>jruby-complete</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>com.sun.jersey</groupId>
+                    <artifactId>jersey-core</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>com.sun.jersey</groupId>
+                    <artifactId>jersey-json</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>com.sun.jersey</groupId>
+                    <artifactId>jersey-server</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>javax.ws.rs</groupId>
+                    <artifactId>jsr311-api</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.codehaus.jackson</groupId>
+                    <artifactId>jackson-core-asl</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.codehaus.jackson</groupId>
+                    <artifactId>jackson-mapper-asl</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>junit</groupId>
+                    <artifactId>junit</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mockito</groupId>
+                    <artifactId>mockito-all</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.apache.hadoop</groupId>
+                    <artifactId>hadoop-test</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>com.google.protobuf</groupId>
+                    <artifactId>protobuf-java</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>javax.xml.bind</groupId>
+                    <artifactId>jaxb-api</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>stax</groupId>
+                    <artifactId>stax-api</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.apache.hadoop</groupId>
+                    <artifactId>avro</artifactId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+        <dependency>
+            <groupId>commons-io</groupId>
+            <artifactId>commons-io</artifactId>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <resources>
+            <resource>
+                <directory>src/main/resources</directory>
+                <filtering>true</filtering>
+            </resource>
+        </resources>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-dependency-plugin</artifactId>
+                <executions>
+                    <execution>
+                        <id>gen-classpath</id>
+                        <phase>generate-test-resources</phase>
+                        <goals>
+                            <goal>build-classpath</goal>
+                        </goals>
+                        <configuration>
+                            <includeScope>compile</includeScope>
+                            <outputFile>${project.build.directory}/classpath</outputFile>
+                        </configuration>
+                    </execution>
+                </executions>
+            </plugin>
+            <!--<plugin>-->
+                <!--<groupId>org.apache.maven.plugins</groupId>-->
+                <!--<artifactId>maven-deploy-plugin</artifactId>-->
+                <!--<configuration>-->
+                    <!--<skip>true</skip>-->
+                <!--</configuration>-->
+            <!--</plugin>-->
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <finalName>partial-sharelib</finalName>
+                    <appendAssemblyId>false</appendAssemblyId>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/partial-sharelib.xml</descriptor>
+                    </descriptors>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git sharelib/streaming/pom.xml sharelib/streaming/pom.xml
new file mode 100644
index 0000000..1ac5b39
--- /dev/null
+++ sharelib/streaming/pom.xml
@@ -0,0 +1,88 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+        <relativePath>../..</relativePath>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-sharelib-streaming</artifactId>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie Share Lib Streaming</description>
+    <name>Apache Oozie Share Lib Streaming</name>
+    <packaging>jar</packaging>
+
+    <properties>
+        <sharelib.use.transitive.dependencies>false</sharelib.use.transitive.dependencies>
+        <sharelib.action.postfix>mapreduce-streaming</sharelib.action.postfix>
+    </properties>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-streaming</artifactId>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <resources>
+            <resource>
+                <directory>src/main/resources</directory>
+                <filtering>true</filtering>
+            </resource>
+        </resources>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-dependency-plugin</artifactId>
+                <executions>
+                    <execution>
+                        <id>gen-classpath</id>
+                        <phase>generate-test-resources</phase>
+                        <goals>
+                            <goal>build-classpath</goal>
+                        </goals>
+                        <configuration>
+                            <includeScope>compile</includeScope>
+                            <outputFile>${project.build.directory}/classpath</outputFile>
+                        </configuration>
+                    </execution>
+                </executions>
+            </plugin>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <finalName>partial-sharelib</finalName>
+                    <appendAssemblyId>false</appendAssemblyId>
+                    <descriptors>
+                        <descriptor>../../src/main/assemblies/partial-sharelib.xml</descriptor>
+                    </descriptors>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
+
diff --git src/main/assemblies/distro.xml src/main/assemblies/distro.xml
index a1d9272..64f29ca 100644
--- src/main/assemblies/distro.xml
+++ src/main/assemblies/distro.xml
@@ -67,6 +67,22 @@
                 <include>*</include>
             </includes>
         </fileSet>
+        <!-- Tools -->
+        <fileSet>
+            <directory>${basedir}/../tools/target/oozie-tools-${project.version}-tools/oozie-tools-${project.version}/bin</directory>
+            <outputDirectory>/bin</outputDirectory>
+            <includes>
+                <include>*</include>
+            </includes>
+            <fileMode>0755</fileMode>
+        </fileSet>
+        <fileSet>
+            <directory>${basedir}/../tools/target/oozie-tools-${project.version}-tools/oozie-tools-${project.version}/libtools</directory>
+            <outputDirectory>/libtools</outputDirectory>
+            <includes>
+                <include>*</include>
+            </includes>
+        </fileSet>
         <!-- Embedded Tomcat -->
         <fileSet>
             <directory>${basedir}/target/tomcat/oozie-server</directory>
@@ -126,9 +142,8 @@
         </file>
         <!-- Oozie sharelib TAR.GZ  -->
         <file>
-            <source>${basedir}/../sharelib/target/oozie-sharelib-${project.version}-sharelib.tar.gz</source>
+            <source>${basedir}/../sharelib/target/oozie-sharelib-${project.version}.tar.gz</source>
             <outputDirectory>/</outputDirectory>
-            <destName>oozie-sharelib-${project.version}.tar.gz</destName>
             <fileMode>0444</fileMode>
         </file>
     </files>
diff --git src/main/assemblies/examples.xml src/main/assemblies/examples.xml
index 49d9aab..d6169d8 100644
--- src/main/assemblies/examples.xml
+++ src/main/assemblies/examples.xml
@@ -113,16 +113,4 @@
         </file> 
     </files>
 
-    <dependencySets>
-        <dependencySet>
-            <outputDirectory>/examples/apps/examples-lib</outputDirectory>
-            <unpack>false</unpack>
-            <excludes>
-                <exclude>${project.groupId}:oozie-examples</exclude>
-                <exclude>${project.groupId}:oozie-sharelib</exclude>
-            </excludes>
-            <useTransitiveDependencies/>
-        </dependencySet>
-    </dependencySets>
-
 </assembly>
diff --git src/main/assemblies/hadooplib.xml src/main/assemblies/hadooplib.xml
new file mode 100644
index 0000000..3151521
--- /dev/null
+++ src/main/assemblies/hadooplib.xml
@@ -0,0 +1,36 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<assembly>
+    <id>hadooplib</id>
+    <formats>
+        <format>dir</format>
+    </formats>
+    <includeBaseDirectory>false</includeBaseDirectory>
+
+    <dependencySets>
+        <dependencySet>
+            <useProjectArtifact>false</useProjectArtifact>
+            <useTransitiveDependencies>true</useTransitiveDependencies>
+            <useTransitiveFiltering>true</useTransitiveFiltering>
+            <unpack>false</unpack>
+            <scope>compile</scope>
+            <outputDirectory>hadooplib-${project.version}</outputDirectory>
+        </dependencySet>
+    </dependencySets>
+
+</assembly>
diff --git src/main/assemblies/hadooplibs.xml src/main/assemblies/hadooplibs.xml
new file mode 100644
index 0000000..c862a63
--- /dev/null
+++ src/main/assemblies/hadooplibs.xml
@@ -0,0 +1,45 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<assembly>
+    <id>hadooplibs</id>
+    <formats>
+        <format>dir</format>
+        <format>tar.gz</format>
+    </formats>
+    <includeBaseDirectory>true</includeBaseDirectory>
+    <baseDirectory>oozie-${project.version}</baseDirectory>
+    <fileSets>
+        <!-- Hadoop libs -->
+        <fileSet>
+            <directory>${basedir}/../hadooplibs/hadoop-1_0_0/target/hadooplibs</directory>
+            <outputDirectory>/hadooplibs</outputDirectory>
+        </fileSet>
+        <fileSet>
+            <directory>${basedir}/../hadooplibs/hadoop-0_22_0/target/hadooplibs</directory>
+            <outputDirectory>/hadooplibs</outputDirectory>
+        </fileSet>
+        <fileSet>
+            <directory>${basedir}/../hadooplibs/hadoop-0_23_1/target/hadooplibs</directory>
+            <outputDirectory>/hadooplibs</outputDirectory>
+        </fileSet>
+        <fileSet>
+            <directory>${basedir}/../hadooplibs/hadoop-0_24_0/target/hadooplibs</directory>
+            <outputDirectory>/hadooplibs</outputDirectory>
+        </fileSet>
+    </fileSets>
+</assembly>
diff --git src/main/assemblies/partial-sharelib.xml src/main/assemblies/partial-sharelib.xml
new file mode 100644
index 0000000..eaa2fb2
--- /dev/null
+++ src/main/assemblies/partial-sharelib.xml
@@ -0,0 +1,35 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<assembly>
+    <id>sharelib</id>
+    <formats>
+        <format>dir</format>
+    </formats>
+    <includeBaseDirectory>false</includeBaseDirectory>
+
+    <dependencySets>
+        <dependencySet>
+            <useProjectArtifact>false</useProjectArtifact>
+            <outputDirectory>/share/lib/${sharelib.action.postfix}</outputDirectory>
+            <unpack>false</unpack>
+            <useTransitiveDependencies>${sharelib.use.transitive.dependencies}</useTransitiveDependencies>
+            <useTransitiveFiltering>true</useTransitiveFiltering>
+        </dependencySet>
+    </dependencySets>
+
+</assembly>
diff --git src/main/assemblies/sharelib.xml src/main/assemblies/sharelib.xml
index b04c954..406d057 100644
--- src/main/assemblies/sharelib.xml
+++ src/main/assemblies/sharelib.xml
@@ -30,15 +30,27 @@
         </file>
     </files>
 
-    <dependencySets>
-        <dependencySet>
-            <outputDirectory>/share/lib</outputDirectory>
-            <unpack>false</unpack>
-            <excludes>
-                <exclude>${project.groupId}:oozie-sharelib</exclude>
-            </excludes>
-            <useTransitiveDependencies/>
-        </dependencySet>
-    </dependencySets>
+    <fileSets>
+        <fileSet>
+            <directory>${basedir}/pig/target/partial-sharelib</directory>
+            <outputDirectory>/</outputDirectory>
+        </fileSet>
+        <fileSet>
+            <directory>${basedir}/streaming/target/partial-sharelib</directory>
+            <outputDirectory>/</outputDirectory>
+        </fileSet>
+        <fileSet>
+            <directory>${basedir}/hive/target/partial-sharelib</directory>
+            <outputDirectory>/</outputDirectory>
+        </fileSet>
+        <fileSet>
+            <directory>${basedir}/sqoop/target/partial-sharelib</directory>
+            <outputDirectory>/</outputDirectory>
+        </fileSet>
+         <fileSet>
+            <directory>${basedir}/oozie/target/partial-sharelib</directory>
+            <outputDirectory>/</outputDirectory>
+        </fileSet>
+    </fileSets>
 
 </assembly>
diff --git src/main/assemblies/tools.xml src/main/assemblies/tools.xml
new file mode 100644
index 0000000..9b65773
--- /dev/null
+++ src/main/assemblies/tools.xml
@@ -0,0 +1,50 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<assembly>
+    <id>tools</id>
+    <formats>
+        <format>dir</format>
+        <!--<format>tar.gz</format>-->
+    </formats>
+    <includeBaseDirectory>false</includeBaseDirectory>
+
+    <fileSets>
+        <fileSet>
+            <directory>${basedir}/src/main/bin</directory>
+            <outputDirectory>oozie-tools-${project.version}/bin</outputDirectory>
+            <includes>
+                <include>*</include>
+            </includes>
+            <fileMode>0755</fileMode>
+        </fileSet>
+    </fileSets>
+
+    <dependencySets>
+        <dependencySet>
+            <useProjectArtifact>true</useProjectArtifact>
+            <outputDirectory>oozie-tools-${project.version}/libtools</outputDirectory>
+            <unpack>false</unpack>
+            <scope>compile</scope>
+            <excludes>
+                 <exclude>*:*:pom:*</exclude>
+            </excludes>
+            <useTransitiveFiltering>true</useTransitiveFiltering>
+        </dependencySet>
+    </dependencySets>
+
+</assembly>
diff --git tools/pom.xml tools/pom.xml
new file mode 100644
index 0000000..05f3355
--- /dev/null
+++ tools/pom.xml
@@ -0,0 +1,165 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  "License"); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+    <parent>
+        <groupId>org.apache.oozie</groupId>
+        <artifactId>oozie-main</artifactId>
+        <version>3.1.3-incubating</version>
+    </parent>
+    <groupId>org.apache.oozie</groupId>
+    <artifactId>oozie-tools</artifactId>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie Tools</description>
+    <name>Apache Oozie Tools</name>
+    <packaging>jar</packaging>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-client</artifactId>
+        </dependency>
+        <dependency>
+            <groupId>commons-cli</groupId>
+            <artifactId>commons-cli</artifactId>
+        </dependency>
+        <dependency>
+            <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-core</artifactId>
+            <scope>compile</scope>
+            <exclusions>
+                <exclusion>
+                    <groupId>commons-el</groupId>
+                    <artifactId>commons-el</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>commons-httpclient</groupId>
+                    <artifactId>commons-httpclient</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>tomcat</groupId>
+                    <artifactId>jasper-compiler</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>tomcat</groupId>
+                    <artifactId>jasper-runtime</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>javax.servlet</groupId>
+                    <artifactId>servlet-api</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>javax.servlet.jsp</groupId>
+                    <artifactId>jsp-api</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>javax.mail</groupId>
+                    <artifactId>mail</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>commons-logging</groupId>
+                    <artifactId>commons-logging-api</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>jetty</groupId>
+                    <artifactId>org.mortbay.jetty</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mortbay.jetty</groupId>
+                    <artifactId>jetty</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mortbay.jetty</groupId>
+                    <artifactId>jetty-util</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mortbay.jetty</groupId>
+                    <artifactId>jsp-api-2.1</artifactId>
+                </exclusion>
+                <exclusion>
+                    <groupId>org.mortbay.jetty</groupId>
+                    <artifactId>servlet-api-2.5</artifactId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+        <dependency>
+            <groupId>org.apache.oozie</groupId>
+            <artifactId>oozie-hadoop</artifactId>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+                <configuration>
+                    <descriptors>
+                        <descriptor>../src/main/assemblies/tools.xml</descriptor>
+                    </descriptors>
+                </configuration>
+            </plugin>
+        </plugins>
+    </build>
+
+    <profiles>
+        <!-- Include MySQL JDBC driver -->
+        <profile>
+            <id>mysql</id>
+            <activation>
+                <activeByDefault>false</activeByDefault>
+                <property>
+                    <name>oozie.test.db</name>
+                    <value>mysql</value>
+                </property>
+            </activation>
+            <dependencies>
+                <dependency>
+                    <groupId>mysql</groupId>
+                    <artifactId>mysql-connector-java</artifactId>
+                    <scope>compile</scope>
+                </dependency>
+            </dependencies>
+        </profile>
+
+        <!-- Include Oracle JDBC driver                                         -->
+        <!-- Oracle JDBC driver is not available in any public Maven repository -->
+        <!-- It must be manually installed in the local Maven cache             -->
+        <profile>
+            <id>oracle</id>
+            <activation>
+                <activeByDefault>false</activeByDefault>
+                <property>
+                    <name>oozie.test.db</name>
+                    <value>oracle</value>
+                </property>
+            </activation>
+            <dependencies>
+                <dependency>
+                    <groupId>com.oracle</groupId>
+                    <artifactId>ojdbc6</artifactId>
+                </dependency>
+            </dependencies>
+        </profile>
+
+    </profiles>
+</project>
+
diff --git tools/src/main/bin/ooziedb.sh tools/src/main/bin/ooziedb.sh
new file mode 100644
index 0000000..fa20e50
--- /dev/null
+++ tools/src/main/bin/ooziedb.sh
@@ -0,0 +1,80 @@
+#!/bin/bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# resolve links - $0 may be a softlink
+PRG="${0}"
+
+while [ -h "${PRG}" ]; do
+  ls=`ls -ld "${PRG}"`
+  link=`expr "$ls" : '.*-> \(.*\)$'`
+  if expr "$link" : '/.*' > /dev/null; then
+    PRG="$link"
+  else
+    PRG=`dirname "${PRG}"`/"$link"
+  fi
+done
+
+BASEDIR=`dirname ${PRG}`
+BASEDIR=`cd ${BASEDIR}/..;pwd`
+
+OOZIE_HOME=${BASEDIR}
+OOZIE_CONFIG=${OOZIE_HOME}/conf
+OOZIE_LOG=${OOZIE_HOME}/logs
+OOZIE_DATA=${OOZIE_HOME}/data
+
+if [ -f ${OOZIE_HOME}/bin/oozie-env.sh ]
+then
+  source ${OOZIE_HOME}/bin/oozie-env.sh
+fi
+
+if [ ! -d ${OOZIE_CONFIG} ]
+then
+  echo
+  echo "ERROR: Oozie configuration directory could not be found at ${OOZIE_CONFIG}"
+  echo
+  exit 1
+fi
+
+OOZIEDB_OPTS="-Doozie.home.dir=${OOZIE_HOME}";
+OOZIEDB_OPTS="${OOZIEDB_OPTS} -Doozie.config.dir=${OOZIE_CONFIG}";
+OOZIEDB_OPTS="${OOZIEDB_OPTS} -Doozie.log.dir=${OOZIE_LOG}";
+OOZIEDB_OPTS="${OOZIEDB_OPTS} -Doozie.data.dir=${OOZIE_DATA}";
+
+OOZIECPPATH=""
+for i in "${BASEDIR}/libtools/"*.jar; do
+  OOZIECPPATH="${OOZIECPPATH}:$i"
+done
+for i in "${BASEDIR}/libext/"*.jar; do
+  OOZIECPPATH="${OOZIECPPATH}:$i"
+done
+
+
+if test -z ${JAVA_HOME}
+then
+    JAVA_BIN=java
+else
+    JAVA_BIN=${JAVA_HOME}/bin/java
+fi
+
+while [[ ${1} =~ ^\-D ]]; do
+  JAVA_PROPERTIES="${JAVA_PROPERTIES} ${1}"
+  shift
+done
+
+${JAVA_BIN} ${OOZIEDB_OPTS} ${JAVA_PROPERTIES} -cp ${OOZIECPPATH} org.apache.oozie.tools.OozieDBCLI "${@}"
diff --git tools/src/main/java/org/apache/oozie/tools/OozieDBCLI.java tools/src/main/java/org/apache/oozie/tools/OozieDBCLI.java
new file mode 100644
index 0000000..20e0ef8
--- /dev/null
+++ tools/src/main/java/org/apache/oozie/tools/OozieDBCLI.java
@@ -0,0 +1,488 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.oozie.tools;
+
+import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.Option;
+import org.apache.commons.cli.Options;
+import org.apache.commons.cli.ParseException;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.oozie.BuildInfo;
+import org.apache.oozie.cli.CLIParser;
+import org.apache.oozie.service.JPAService;
+import org.apache.oozie.service.Services;
+
+import java.io.FileWriter;
+import java.io.PrintWriter;
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.ResultSet;
+import java.sql.Statement;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Command line tool to create/upgrade Oozie Database
+ */
+public class OozieDBCLI {
+    public static final String HELP_CMD = "help";
+    public static final String VERSION_CMD = "version";
+    public static final String CREATE_CMD = "create";
+    public static final String UPGRADE_CMD = "upgrade";
+    public static final String SQL_FILE_OPT = "sqlfile";
+    public static final String RUN_OPT = "run";
+
+    public static final String[] HELP_INFO = {
+        "",
+        "IMPORTANT: If using an Oracle or MySQL Database, before running this",
+        "tool copy the corresponding JDBC driver to the tools libext/ directory"
+    };
+
+    private boolean used;
+
+    public static void main(String[] args) {
+        System.exit(new OozieDBCLI().run(args));
+    }
+
+    public OozieDBCLI() {
+        used = false;
+    }
+
+    protected Options createUpgradeOptions() {
+        Option sqlfile = new Option(SQL_FILE_OPT, true, "Generate SQL script instead creating/upgrading the DB schema");
+        Option run = new Option(RUN_OPT, false, "Confirm the DB schema creation/upgrade");
+        Options options = new Options();
+        options.addOption(sqlfile);
+        options.addOption(run);
+        return options;
+    }
+
+    public synchronized int run(String[] args) {
+        if (used) {
+            throw new IllegalStateException("CLI instance already used");
+        }
+        used = true;
+
+        CLIParser parser = new CLIParser("ooziedb.sh", HELP_INFO);
+        parser.addCommand(HELP_CMD, "", "display usage", new Options(), false);
+        parser.addCommand(VERSION_CMD, "", "show Oozie DB version information", new Options(), false);
+        parser.addCommand(CREATE_CMD, "", "create Oozie DB schema", createUpgradeOptions(), false);
+        parser.addCommand(UPGRADE_CMD, "", "upgrade Oozie DB schema", createUpgradeOptions(), false);
+
+        try {
+            System.out.println();
+            CLIParser.Command command = parser.parse(args);
+            if (command.getName().equals(HELP_CMD)) {
+                parser.showHelp();
+            }
+            else if (command.getName().equals(VERSION_CMD)) {
+                showVersion();
+            }
+            else {
+                if (!command.getCommandLine().hasOption(SQL_FILE_OPT)) {
+                    throw new Exception("'-sqlfile <FILE>' option must be specified");
+                }
+                if (command.getName().equals(CREATE_CMD)) {
+                    createDB(command.getCommandLine());
+                }
+                else {
+                    upgradeDB(command.getCommandLine());
+                }
+            }
+            return 0;
+        }
+        catch (ParseException ex) {
+            System.err.println("Invalid sub-command: " + ex.getMessage());
+            System.err.println();
+            System.err.println(parser.shortHelp());
+            return 1;
+        }
+        catch (Exception ex) {
+            System.err.println();
+            System.err.println("Error: " + ex.getMessage());
+            System.err.println();
+            System.err.println("--------------------------------------");
+            ex.printStackTrace(System.err);
+            System.err.println("--------------------------------------");
+            System.err.println();
+            return 1;
+        }
+    }
+
+    private Map<String, String> getJdbcConf() throws Exception {
+        Services services = new Services();
+        Configuration conf = services.getConf();
+        Map<String, String> jdbcConf = new HashMap<String, String>();
+        jdbcConf.put("driver", conf.get(JPAService.CONF_DRIVER));
+        String url = conf.get(JPAService.CONF_URL);
+        jdbcConf.put("url", url);
+        jdbcConf.put("user", conf.get(JPAService.CONF_USERNAME));
+        jdbcConf.put("password", conf.get(JPAService.CONF_PASSWORD));
+        String dbType = url.substring("jdbc:".length());
+        if (dbType.indexOf(":") <= 0) {
+            throw new RuntimeException("Invalid JDBC URL, missing vendor 'jdbc:[VENDOR]:...'");
+        }
+        dbType = dbType.substring(0, dbType.indexOf(":"));
+        jdbcConf.put("dbtype", dbType);
+        return jdbcConf;
+    }
+
+    private void createDB(CommandLine commandLine) throws Exception {
+        validateConnection();
+        validateDBSchema(false);
+        verifyOozieSysTable(false);
+        String sqlFile = (commandLine.hasOption(SQL_FILE_OPT)) ? commandLine.getOptionValue(SQL_FILE_OPT) : null;
+        boolean run = commandLine.hasOption(RUN_OPT);
+        createUpgradeDB(sqlFile, run);
+        createOozieSysTable(sqlFile, run);
+        System.out.println();
+        if (run) {
+            System.out.println("Oozie DB has been created for Oozie version '" +
+                               BuildInfo.getBuildInfo().getProperty(BuildInfo.BUILD_VERSION) + "'");
+        }
+        else {
+            System.out.println("The SQL commands have NOT been executed, you must use the '-run' option");
+        }
+        System.out.println();
+    }
+
+    private void upgradeDB(CommandLine commandLine) throws Exception {
+        validateConnection();
+        validateDBSchema(true);
+        verifyOozieSysTable(false);
+        verifyDBState();
+        String sqlFile = (commandLine.hasOption(SQL_FILE_OPT)) ? commandLine.getOptionValue(SQL_FILE_OPT) : null;
+        boolean run = commandLine.hasOption(RUN_OPT);
+        createUpgradeDB(sqlFile, run);
+        createOozieSysTable(sqlFile, run);
+        postUpgradeTasks(sqlFile, run);
+        manualStepsNotice();
+        if (run) {
+            System.out.println();
+            System.out.println("Oozie DB has been upgraded to Oozie version '" +
+                               BuildInfo.getBuildInfo().getProperty(BuildInfo.BUILD_VERSION) + "'");
+        }
+        else {
+            System.out.println();
+            System.out.println("The SQL commands have NOT been executed, you must use the '-run' option");
+        }
+        System.out.println();
+    }
+
+    private static final String COORD_JOBS_THROTTLING_DEFAULT =
+        "update COORD_JOBS set mat_throttling = 12";
+
+    private static final String COORD_JOBS_ADD_APP_NAMESPACE =
+        "update COORD_JOBS set app_namespace = 'uri:oozie:coordinator:0.1'";
+
+    private static final String COORD_JOBS_STATUS_1 =
+        "update COORD_JOBS set status = 'RUNNING', PENDING = 1 " +
+        "where id in ( " +
+        "select job_id from COORD_ACTIONS where job_id in ( " +
+        "select id from COORD_JOBS where status = 'SUCCEEDED') and(status != 'FAILED' and " +
+        "status != 'SUCCEEDED' and status != 'KILLED' and status != 'TIMEDOUT') )";
+
+    private static final String COORD_JOBS_STATUS_2 =
+        "update COORD_JOBS set status = 'RUNNING' where status = 'PREMATER'";
+
+    private static final String COORD_ACTIONS_STATUS =
+        "update COORD_ACTIONS set status = 'SUSPENDED' " +
+        "where id in( " +
+        "select A.id from COORD_ACTIONS A, WF_JOBS B where A.external_id = B.id " +
+        "and B.status = 'SUSPENDED' and A.status = 'RUNNING' )";
+
+    private void postUpgradeTasks(String sqlFile, boolean run) throws Exception {
+        PrintWriter writer = new PrintWriter(new FileWriter(sqlFile, true));
+        writer.println();
+        boolean skipUpdates = getJdbcConf().get("url").contains("mysql");
+        Connection conn = (run) ? createConnection() : null;
+        try {
+            System.out.println("Upgrade COORD_JOBS new columns default values");
+            writer.println(COORD_JOBS_THROTTLING_DEFAULT + ";");
+            if (run) {
+                conn.setAutoCommit(true);
+                Statement st = conn.createStatement();
+                st.executeUpdate(COORD_JOBS_THROTTLING_DEFAULT);
+                st.close();
+            }
+            writer.println(COORD_JOBS_ADD_APP_NAMESPACE + ";");
+            if (run) {
+                Statement st = conn.createStatement();
+                st.executeUpdate(COORD_JOBS_ADD_APP_NAMESPACE);
+                st.close();
+            }
+            System.out.println("DONE");
+            if (!skipUpdates) {
+                System.out.println("Upgrade COORD_JOBS & COORD_ACTIONS status values");
+                writer.println(COORD_JOBS_STATUS_1 + ";");
+                writer.println(COORD_JOBS_STATUS_2 + ";");
+                writer.println(COORD_ACTIONS_STATUS + ";");
+                if (run) {
+                    Statement st = conn.createStatement();
+                    st.executeUpdate(COORD_JOBS_STATUS_1);
+                    st.close();
+                    st = conn.createStatement();
+                    st.executeUpdate(COORD_JOBS_STATUS_2);
+                    st.close();
+                    st = conn.createStatement();
+                    st.executeUpdate(COORD_ACTIONS_STATUS);
+                    st.close();
+                }
+                System.out.println("DONE");
+            }
+            else {
+                System.out.println("SKIPPING Upgrade of COORD_JOBS & COORD_ACTIONS status values,");
+                System.out.println("         MySQL 5.0 does not support the update queries");
+            }
+            writer.close();
+        }
+        finally {
+            if (run) {
+                conn.close();
+            }
+        }
+    }
+
+    private void manualStepsNotice() {
+        System.out.println();
+        System.out.println("IMPORTANT: the following manual changes may have to be done in the Oozie DB");
+        System.out.println();
+        System.out.println(
+            "  The 'execution_path' column in the 'WF_ACTIONS' table should be modified to be a VARCHAR2(1024)");
+        System.out.println();
+    }
+
+    private Connection createConnection() throws Exception {
+        Map<String, String> conf = getJdbcConf();
+        Class.forName(conf.get("driver")).newInstance();
+        return DriverManager.getConnection(conf.get("url"), conf.get("user"), conf.get("password"));
+    }
+
+    private void validateConnection() throws Exception {
+        System.out.println("Validate DB Connection");
+        try {
+            createConnection().close();
+            System.out.println("DONE");
+        }
+        catch (Exception ex) {
+            throw new Exception("Could not connect to the database: " + ex.toString(), ex);
+        }
+    }
+
+    private static final String WORKFLOW_STATUS_QUERY =
+        "select count(*) from WF_JOBS where status IN ('RUNNING', 'SUSPENDED')";
+
+    private void validateDBSchema(boolean exists) throws Exception {
+        System.out.println((exists) ? "Check DB schema exists" : "Check DB schema does not exist");
+        boolean schemaExists;
+        Connection conn = createConnection();
+        try {
+            Statement st = conn.createStatement();
+            ResultSet rs = st.executeQuery(WORKFLOW_STATUS_QUERY);
+            rs.next();
+            rs.close();
+            st.close();
+            schemaExists = true;
+        }
+        catch (Exception ex) {
+            schemaExists = false;
+        }
+        finally {
+            conn.close();
+        }
+        if (schemaExists != exists) {
+            throw new Exception("DB schema " + ((exists) ? "does not exist" : "exists"));
+        }
+        System.out.println("DONE");
+    }
+
+    private final static String OOZIE_SYS_EXISTS = "select count(*) from OOZIE_SYS";
+
+    private void verifyOozieSysTable(boolean exists) throws Exception {
+        System.out.println((exists) ? "Check OOZIE_SYS table exists" : "Check OOZIE_SYS table does not exist");
+        boolean tableExists;
+        Connection conn = createConnection();
+        try {
+            Statement st = conn.createStatement();
+            ResultSet rs = st.executeQuery(OOZIE_SYS_EXISTS);
+            rs.next();
+            rs.close();
+            st.close();
+            tableExists = true;
+        }
+        catch (Exception ex) {
+            tableExists = false;
+        }
+        finally {
+            conn.close();
+        }
+        if (tableExists != exists) {
+            throw new Exception("OOZIE SYS table " + ((exists) ? "does not exist" : "exists"));
+        }
+        System.out.println("DONE");
+    }
+
+    private final static String CREATE_OOZIE_SYS =
+        "create table OOZIE_SYS (name varchar(100), data varchar(100))";
+
+    private final static String SET_DB_VERSION =
+        "insert into OOZIE_SYS (name, data) values ('db.version', '1')";
+
+    private final static String SET_OOZIE_VERSION =
+        "insert into OOZIE_SYS (name, data) values ('oozie.version', '" +
+        BuildInfo.getBuildInfo().getProperty(BuildInfo.BUILD_VERSION) + "')";
+
+    private void createOozieSysTable(String sqlFile, boolean run) throws Exception {
+        PrintWriter writer = new PrintWriter(new FileWriter(sqlFile, true));
+        writer.println();
+        writer.println(CREATE_OOZIE_SYS);
+        writer.println(SET_DB_VERSION);
+        writer.println(SET_OOZIE_VERSION);
+        writer.close();
+        System.out.println("Create OOZIE_SYS table");
+        if (run) {
+            Connection conn = createConnection();
+            try {
+                conn.setAutoCommit(true);
+                Statement st = conn.createStatement();
+                st.executeUpdate(CREATE_OOZIE_SYS);
+                st.executeUpdate(SET_DB_VERSION);
+                st.executeUpdate(SET_OOZIE_VERSION);
+                st.close();
+            }
+            catch (Exception ex) {
+                throw new Exception("Could not create OOZIE_SYS table: " + ex.toString(), ex);
+            }
+            finally {
+                conn.close();
+            }
+        }
+        System.out.println("DONE");
+    }
+
+    private final static String GET_OOZIE_SYS_INFO = "select name, data from OOZIE_SYS order by name";
+
+    private void showOozieSysInfo() throws Exception {
+        Connection conn = createConnection();
+        try {
+            System.out.println();
+            System.out.println("Oozie DB Version Information");
+            System.out.println("--------------------------------------");
+            Statement st = conn.createStatement();
+            ResultSet rs = st.executeQuery(GET_OOZIE_SYS_INFO);
+            while (rs.next()) {
+                System.out.println(rs.getString(1) + ": " + rs.getString(2));
+            }
+            System.out.println("--------------------------------------");
+            System.out.println();
+            rs.close();
+            st.close();
+        }
+        catch (Exception ex) {
+            throw new Exception("ERROR querying OOZIE_SYS table: " + ex.toString(), ex);
+        }
+        finally {
+            conn.close();
+        }
+    }
+
+    private void verifyDBState() throws Exception {
+        System.out.println("Verify there are not active Workflow Jobs");
+        Connection conn = createConnection();
+        try {
+            Statement st = conn.createStatement();
+            ResultSet rs = st.executeQuery(WORKFLOW_STATUS_QUERY);
+            rs.next();
+            long activeWorkflows = rs.getLong(1);
+            rs.close();
+            st.close();
+            if (activeWorkflows > 0) {
+                throw new Exception("There are [" + activeWorkflows +
+                                    "] workflows in RUNNING/SUSPENDED state, they must complete or be killed");
+            }
+            System.out.println("DONE");
+        }
+        finally {
+            conn.close();
+        }
+    }
+
+    private String[] createMappingToolArguments(String sqlFile) throws Exception {
+        Map<String, String> conf = getJdbcConf();
+        List<String> args = new ArrayList<String>();
+        args.add("-schemaAction");
+        args.add("add");
+        args.add("-p");
+        args.add("persistence.xml#oozie-" + conf.get("dbtype"));
+        args.add("-connectionDriverName");
+        args.add(conf.get("driver"));
+        args.add("-connectionURL");
+        args.add(conf.get("url"));
+        args.add("-connectionUserName");
+        args.add(conf.get("user"));
+        args.add("-connectionPassword");
+        args.add(conf.get("password"));
+        if (sqlFile != null) {
+            args.add("-sqlFile");
+            args.add(sqlFile);
+        }
+        args.add("org.apache.oozie.client.rest.JsonWorkflowJob");
+        args.add("org.apache.oozie.WorkflowJobBean");
+        args.add("org.apache.oozie.client.rest.JsonWorkflowAction");
+        args.add("org.apache.oozie.WorkflowActionBean");
+        args.add("org.apache.oozie.client.rest.JsonCoordinatorJob");
+        args.add("org.apache.oozie.CoordinatorJobBean");
+        args.add("org.apache.oozie.client.rest.JsonCoordinatorAction");
+        args.add("org.apache.oozie.CoordinatorActionBean");
+        args.add("org.apache.oozie.client.rest.JsonSLAEvent");
+        args.add("org.apache.oozie.client.rest.JsonBundleJob");
+        args.add("org.apache.oozie.BundleJobBean");
+        args.add("org.apache.oozie.BundleActionBean");
+        return args.toArray(new String[args.size()]);
+    }
+
+    private void createUpgradeDB(String sqlFile, boolean run) throws Exception {
+        System.out.println("Create SQL schema");
+        String[] args = createMappingToolArguments(sqlFile);
+        org.apache.openjpa.jdbc.meta.MappingTool.main(args);
+        if (run) {
+            args = createMappingToolArguments(null);
+            org.apache.openjpa.jdbc.meta.MappingTool.main(args);
+            System.out.println("DONE");
+        }
+        System.out.println("DONE");
+    }
+
+    private void showVersion() throws Exception {
+        System.out.println("Oozie DB tool version: "
+                           + BuildInfo.getBuildInfo().getProperty(BuildInfo.BUILD_VERSION));
+        System.out.println();
+        validateConnection();
+        validateDBSchema(true);
+        try {
+            verifyOozieSysTable(true);
+        }
+        catch (Exception ex) {
+            throw new Exception("ERROR: It seems this Oozie DB was never upgraded with the 'ooziedb' tool");
+        }
+        showOozieSysInfo();
+    }
+
+}
diff --git webapp/pom.xml webapp/pom.xml
index a5d1ac1..4185f95 100644
--- webapp/pom.xml
+++ webapp/pom.xml
@@ -16,16 +16,19 @@
   See the License for the specific language governing permissions and
   limitations under the License.
 -->
-<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
     <modelVersion>4.0.0</modelVersion>
     <parent>
         <groupId>org.apache.oozie</groupId>
         <artifactId>oozie-main</artifactId>
         <version>3.1.3-incubating</version>
     </parent>
+    <groupId>org.apache.oozie</groupId>
     <artifactId>oozie-webapp</artifactId>
-    <description>Oozie WebApp</description>
-    <name>Oozie WebApp</name>
+    <version>3.1.3-incubating</version>
+    <description>Apache Oozie WebApp</description>
+    <name>Apache Oozie WebApp</name>
     <packaging>war</packaging>
 
     <dependencies>
@@ -55,18 +58,10 @@
                     <artifactId>servlet-api</artifactId>
                 </exclusion>
                 <exclusion>
-                    <groupId>javax.servlet</groupId>
+                    <groupId>javax.servlet.jsp</groupId>
                     <artifactId>jsp-api</artifactId>
                 </exclusion>
                 <exclusion>
-                    <groupId>org.slf4j</groupId>
-                    <artifactId>slf4j-api</artifactId>
-                </exclusion>
-                <exclusion>
-                    <groupId>org.slf4j</groupId>
-                    <artifactId>slf4j-log4j12</artifactId>
-                </exclusion>
-                <exclusion>
                     <groupId>commons-logging</groupId>
                     <artifactId>commons-logging-api</artifactId>
                 </exclusion>
@@ -104,6 +99,13 @@
         <plugins>
             <plugin>
                 <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-deploy-plugin</artifactId>
+                <configuration>
+                    <skip>true</skip>
+                </configuration>
+            </plugin>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
                 <artifactId>maven-assembly-plugin</artifactId>
                 <configuration>
                     <descriptors>
@@ -112,93 +114,19 @@
                 </configuration>
             </plugin>
             <plugin>
-                 <groupId>org.apache.rat</groupId>
-                 <artifactId>apache-rat-plugin</artifactId>
-                 <configuration>
-                     <excludes>
+                <groupId>org.apache.rat</groupId>
+                <artifactId>apache-rat-plugin</artifactId>
+                <configuration>
+                    <excludes>
                         <exclude>src/main/resources/.gitignore</exclude>
-                     </excludes>
-                  </configuration>
+                    </excludes>
+                </configuration>
             </plugin>
         </plugins>
     </build>
 
     <profiles>
         <profile>
-            <id>includeHadoopJars</id>
-            <activation>
-                <activeByDefault>false</activeByDefault>
-                <property>
-                    <name>includeHadoopJars</name>
-                </property>
-            </activation>
-            <dependencies>
-                <dependency>
-                    <groupId>org.apache.hadoop</groupId>
-                    <artifactId>hadoop-core</artifactId>
-                    <scope>compile</scope>
-                    <exclusions>
-                        <exclusion>
-                            <groupId>commons-cli</groupId>
-                            <artifactId>commons-cli</artifactId>
-                        </exclusion>
-                        <exclusion>
-                            <groupId>commons-httpclient</groupId>
-                            <artifactId>commons-httpclient</artifactId>
-                        </exclusion>
-                        <exclusion>
-                            <groupId>tomcat</groupId>
-                            <artifactId>jasper-compiler</artifactId>
-                        </exclusion>
-                        <exclusion>
-                            <groupId>tomcat</groupId>
-                            <artifactId>jasper-runtime</artifactId>
-                        </exclusion>
-                        <exclusion>
-                            <groupId>javax.servlet</groupId>
-                            <artifactId>servlet-api</artifactId>
-                        </exclusion>
-                        <exclusion>
-                            <groupId>javax.servlet</groupId>
-                            <artifactId>jsp-api</artifactId>
-                        </exclusion>
-                        <exclusion>
-                            <groupId>org.slf4j</groupId>
-                            <artifactId>slf4j-api</artifactId>
-                        </exclusion>
-                        <exclusion>
-                            <groupId>org.slf4j</groupId>
-                            <artifactId>slf4j-log4j12</artifactId>
-                        </exclusion>
-                        <exclusion>
-                            <groupId>commons-logging</groupId>
-                            <artifactId>commons-logging-api</artifactId>
-                        </exclusion>
-                        <exclusion>
-                            <groupId>jetty</groupId>
-                            <artifactId>org.mortbay.jetty</artifactId>
-                        </exclusion>
-                        <exclusion>
-                            <groupId>org.mortbay.jetty</groupId>
-                            <artifactId>jetty</artifactId>
-                        </exclusion>
-                        <exclusion>
-                            <groupId>org.mortbay.jetty</groupId>
-                            <artifactId>jetty-util</artifactId>
-                        </exclusion>
-                        <exclusion>
-                            <groupId>org.mortbay.jetty</groupId>
-                            <artifactId>jsp-api-2.1</artifactId>
-                        </exclusion>
-                        <exclusion>
-                            <groupId>org.mortbay.jetty</groupId>
-                            <artifactId>servlet-api-2.5</artifactId>
-                        </exclusion>
-                    </exclusions>
-                </dependency>
-            </dependencies>
-        </profile>
-        <profile>
             <id>generateDocs</id>
             <activation>
                 <activeByDefault>false</activeByDefault>
diff --git webapp/src/main/webapp/WEB-INF/web.xml webapp/src/main/webapp/WEB-INF/web.xml
index 15606db..79352d8 100644
--- webapp/src/main/webapp/WEB-INF/web.xml
+++ webapp/src/main/webapp/WEB-INF/web.xml
@@ -146,4 +146,74 @@
         <welcome-file>index.html</welcome-file>
     </welcome-file-list>
 
+    <filter>
+        <filter-name>hostnameFilter</filter-name>
+        <filter-class>org.apache.oozie.servlet.HostnameFilter</filter-class>
+    </filter>
+
+    <filter>
+        <filter-name>authenticationfilter</filter-name>
+        <filter-class>org.apache.oozie.servlet.AuthFilter</filter-class>
+    </filter>
+
+    <filter-mapping>
+        <filter-name>hostnameFilter</filter-name>
+        <url-pattern>*</url-pattern>
+    </filter-mapping>
+
+    <filter-mapping>
+        <filter-name>authenticationfilter</filter-name>
+        <url-pattern>/versions/*</url-pattern>
+    </filter-mapping>
+
+    <filter-mapping>
+        <filter-name>authenticationfilter</filter-name>
+        <url-pattern>/v0/admin/*</url-pattern>
+    </filter-mapping>
+
+    <filter-mapping>
+        <filter-name>authenticationfilter</filter-name>
+        <url-pattern>/v1/admin/*</url-pattern>
+    </filter-mapping>
+
+    <filter-mapping>
+        <filter-name>authenticationfilter</filter-name>
+        <url-pattern>/v0/jobs</url-pattern>
+    </filter-mapping>
+
+    <filter-mapping>
+        <filter-name>authenticationfilter</filter-name>
+        <url-pattern>/v1/jobs</url-pattern>
+    </filter-mapping>
+
+    <filter-mapping>
+        <filter-name>authenticationfilter</filter-name>
+        <url-pattern>/v0/job/*</url-pattern>
+    </filter-mapping>
+
+    <filter-mapping>
+        <filter-name>authenticationfilter</filter-name>
+        <url-pattern>/v1/job/*</url-pattern>
+    </filter-mapping>
+
+    <filter-mapping>
+        <filter-name>authenticationfilter</filter-name>
+        <url-pattern>/index.html</url-pattern>
+    </filter-mapping>
+
+    <filter-mapping>
+        <filter-name>authenticationfilter</filter-name>
+        <url-pattern>*.js</url-pattern>
+    </filter-mapping>
+
+    <filter-mapping>
+        <filter-name>authenticationfilter</filter-name>
+        <url-pattern>/ext-2.2/*</url-pattern>
+    </filter-mapping>
+
+    <filter-mapping>
+        <filter-name>authenticationfilter</filter-name>
+        <url-pattern>/docs/*</url-pattern>
+    </filter-mapping>
+
 </web-app>
diff --git distro/src/main/bin/oozied.sh distro/src/main/bin/oozied.sh 
index 15606db..79352d8 100644
--- distro/src/main/bin/oozied.sh	(revision 1300625)
+++ distro/src/main/bin/oozied.sh	(working copy)
@@ -43,7 +43,7 @@
 
 source ${BASEDIR}/bin/oozie-sys.sh
 
-CATALINA=${BASEDIR}/oozie-server/bin/catalina.sh
+CATALINA=${OOZIE_CATALINA_HOME:-${BASEDIR}/oozie-server}/bin/catalina.sh
 
 setup_catalina_opts() {
   # The Java System property 'oozie.http.port' it is not used by Oozie,
